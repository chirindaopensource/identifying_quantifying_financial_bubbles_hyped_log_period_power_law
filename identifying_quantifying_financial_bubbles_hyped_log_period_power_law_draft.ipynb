{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7a1NZCkIj1D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.10878-b31b1b.svg)](https://arxiv.org/abs/2510.10878)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Quantitative%20Finance%20%7C%20NLP%20%7C%20Econophysics-00529B)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-CRSP%20%7C%20Compustat%20%7C%20News%20API-lightgrey)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![Core Method](https://img.shields.io/badge/Method-LPPL%20%7C%20Transformer-orange)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![NLP Models](https://img.shields.io/badge/NLP-FinBERT%20%7C%20BERTopic-red)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![Deep Learning](https://img.shields.io/badge/Deep%20Learning-Dual--Stream%20Transformer-blueviolet)](https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![Scipy](https://img.shields.io/badge/SciPy-%238CAAE6.svg?style=flat&logo=SciPy&logoColor=white)](https://scipy.org/)\n",
        "[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?style=flat)](https://huggingface.co/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![YAML](https://img.shields.io/badge/config-YAML-ffdd00.svg)](https://yaml.org/)\n",
        "---\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model\"** by:\n",
        "\n",
        "*   Zheng Cao\n",
        "*   Xingran Shao\n",
        "*   Yuheng Yan\n",
        "*   Helyette Geman\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and NLP feature engineering to LPPL model fitting, deep learning, and backtesting.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_full_study`](#key-callable-execute_full_study)\n",
        "- [Workflow Diagram](#workflow-diagram)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model.\" The core of this repository is the iPython Notebook `identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper proposes a novel framework (HLPPL) that fuses three distinct domains—econophysics, natural language processing, and deep learning—to create a superior, real-time indicator of financial asset mispricing. This codebase operationalizes that framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a `config.yaml` file.\n",
        "-   Process raw market data and news text through a multi-stage feature engineering pipeline.\n",
        "-   Fit the Log-Periodic Power Law (LPPL) model at scale using a robust, multi-start optimization strategy.\n",
        "-   Construct the novel `BubbleScore` by fusing technical and behavioral signals.\n",
        "-   Train a state-of-the-art Dual-Stream Transformer model to forecast the `BubbleScore`.\n",
        "-   Run a complete, event-driven backtest to evaluate the trading performance of the generated signals.\n",
        "-   Automatically conduct ablation and sensitivity studies to validate the model's robustness.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in econophysics, behavioral finance, and deep learning.\n",
        "\n",
        "**1. Log-Periodic Power Law (LPPL) Model:**\n",
        "Originating from the physics of critical phenomena, the LPPL model describes the super-exponential growth of an asset price leading up to a crash (a critical point). The implementation fits the 7-parameter model defined in Equation (1):\n",
        "$$\n",
        "\\ln p(t) = A + B(t_c - t)^m + C(t_c - t)^m \\cos(\\omega \\ln(t_c - t) + \\phi)\n",
        "$$\n",
        "The normalized residual from this fit, $\\epsilon_{\\text{norm}}(t)$, serves as the primary technical indicator of deviation from the theoretical bubble path.\n",
        "\n",
        "**2. Behavioral Finance Signals (NLP):**\n",
        "Two NLP-derived features are constructed to capture market psychology:\n",
        "-   **Hype Index ($H_{i,t}$):** The share of media attention a stock receives on a given day, measuring intensity. (Equation 11)\n",
        "-   **Sentiment Score ($S_{i,t}$):** The confidence-weighted average sentiment (positive, neutral, negative) of news articles, measuring tone. (Equation 9)\n",
        "\n",
        "**3. Hyped LPPL (HLPPL) `BubbleScore`:**\n",
        "The paper's core innovation is the fusion of the technical and behavioral signals into a single `BubbleScore`. The formula is regime-dependent, with the Hype Index acting as an amplifier in both positive and negative deviations. (Equation 14)\n",
        "$$\n",
        "\\text{BubbleScore}_{i}(t) =\n",
        "\\begin{cases}\n",
        "\\epsilon_{\\text{norm}}(t) + \\alpha_1 H_{i,t} + \\alpha_2 S_{i,t}, & \\text{if } \\epsilon_{\\text{norm}}(t) > 0 \\\\\n",
        "\\epsilon_{\\text{norm}}(t) - \\alpha_1 H_{i,t} + \\alpha_2 S_{i,t}, & \\text{if } \\epsilon_{\\text{norm}}(t) \\le 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**4. Dual-Stream Transformer:**\n",
        "A deep learning model is trained to forecast the `BubbleScore`. Its architecture is designed to process stock-specific features and market-wide features in parallel, allowing them to interact via a bi-directional cross-attention mechanism before making a final prediction.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 33 distinct, modular tasks, each with its own orchestrator function for maximum clarity and testability.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file, allowing for easy customization and replication.\n",
        "-   **Idempotent & Resumable Pipeline:** Computationally expensive steps (e.g., NLP processing, LPPL fitting, model training) create checkpoint files, allowing the pipeline to be resumed efficiently.\n",
        "-   **Robust LPPL Fitting:** Implements a multi-start, constrained non-linear least squares optimization to robustly fit the 7-parameter LPPL model across thousands of rolling windows.\n",
        "-   **State-of-the-Art Deep Learning:** Implements a Dual-Stream Transformer in PyTorch with modern training techniques (`AdamW`, `OneCycleLR`, gradient clipping, early stopping) and a custom multi-component loss function.\n",
        "-   **Realistic Event-Driven Backtester:** Simulates trading performance with daily stop-loss checks and transaction costs.\n",
        "-   **Automated Ablation & Sensitivity Analysis:** Includes a top-level orchestrator to automatically re-run the entire pipeline under different configurations to test the contribution of each model component.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The notebook is a direct, sequential implementation of the paper's methodology:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-6):** Ingests and validates the `config.yaml` and raw data, cleanses the data, adjusts for corporate actions, and engineers primary features.\n",
        "2.  **NLP Feature Engineering (Tasks 7-12):** Uses BERTopic and FinBERT to process news text and generate the `Sentiment_Score` and `Hype_Index`.\n",
        "3.  **LPPL Signal Generation (Tasks 13-18):** Defines rolling windows, fits the LPPL model, computes normalized residuals, fuses them into the `BubbleScore`, and labels discrete episodes.\n",
        "4.  **ML Data Preparation (Tasks 19-22):** Normalizes features (with leakage protection), constructs fixed-length sequences for the stock and market streams, creates multi-horizon targets, and performs a strict chronological split.\n",
        "5.  **Deep Learning (Tasks 23-28):** Defines the `DualStreamTransformer` architecture and `CompositeLoss`, trains the model with early stopping, persists the final artifact, and evaluates its out-of-sample predictive performance.\n",
        "6.  **Backtesting (Tasks 29-31):** Converts predictions into discrete trading signals, runs the event-driven backtest, and computes a full suite of performance metrics.\n",
        "7.  **Final Orchestration (Tasks 32-33):** Provides top-level functions to run the entire baseline pipeline and the full suite of ablation studies.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 33 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_full_study`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_full_study`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, from data validation to the final report.\n",
        "\n",
        "## Workflow Diagram\n",
        "\n",
        "The following diagram illustrates the high-level workflow orchestrated by the `run_hlppl_pipeline` function, which is the core engine called by `execute_full_study`.\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Start: Raw Data & Config] --> B(Phase I: Validation);\n",
        "    B --> C(Phase II: Data Cleansing & Feature Eng.);\n",
        "    C --> D(Phase III: NLP Signal Generation);\n",
        "    D --> E(Phase IV: LPPL Signal Generation);\n",
        "    E --> F(Phase V: ML Data Preparation);\n",
        "    F --> G(Phase VI: Model Training & Validation);\n",
        "    G --> H(Phase VII: Inference & Backtesting);\n",
        "    H --> I[End: Performance Report];\n",
        "\n",
        "    subgraph Phase III\n",
        "        D -- Hype Index & Sentiment --> E;\n",
        "    end\n",
        "\n",
        "    subgraph Phase IV\n",
        "        E -- BubbleScore --> F;\n",
        "    end\n",
        "\n",
        "    subgraph Phase VI\n",
        "        G -- Trained Model --> H;\n",
        "    end\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   A CUDA-enabled GPU is highly recommended for the deep learning and NLP components.\n",
        "-   Core dependencies: `pandas`, `numpy`, `torch`, `transformers`, `sentence-transformers`, `bertopic`, `umap-learn`, `hdbscan`, `scipy`, `pyyaml`, `matplotlib`, `seaborn`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law.git\n",
        "    cd identifying_quantifying_financial_bubbles_hyped_log_period_power_law\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a `pandas.DataFrame` (`df_raw`) with a `MultiIndex` of `('Date', 'TICKER')` and the following columns and dtypes:\n",
        "-   `PERMNO`: `int64`\n",
        "-   `SIC_Code`: `int64`\n",
        "-   `Close_Price_Raw`: `float64`\n",
        "-   `Volume_Raw`: `int64`\n",
        "-   `CFACSHR`: `float64`\n",
        "-   `PE_Ratio`: `float64`\n",
        "-   `PB_Ratio`: `float64`\n",
        "-   `VIX_Close`: `float64`\n",
        "-   `News_Articles`: `object` (containing `list` of `str`)\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which contains the main execution block:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This function generates a sample DataFrame for demonstration.\n",
        "# In a real run, you would load your own data here.\n",
        "df_raw = create_sample_dataframe()\n",
        "\n",
        "# Load the master configuration from the YAML file.\n",
        "with open(\"config.yaml\", 'r') as f:\n",
        "    base_config = yaml.safe_load(f)\n",
        "\n",
        "# --- Execute the entire study ---\n",
        "# To run only the baseline model (faster):\n",
        "final_results = execute_full_study(\n",
        "    df_raw=df_raw,\n",
        "    base_config=base_config,\n",
        "    run_ablation=False\n",
        ")\n",
        "\n",
        "# To run the baseline AND all ablation/sensitivity studies (very slow):\n",
        "# final_results = execute_full_study(\n",
        "#     df_raw=df_raw,\n",
        "#     base_config=base_config,\n",
        "#     run_ablation=True\n",
        "# )\n",
        "\n",
        "# The `final_results` dictionary will contain the key outputs.\n",
        "print(final_results['baseline_performance'])\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_full_study` function creates a `study_results/` directory with the following structure:\n",
        "\n",
        "```\n",
        "study_results/\n",
        "│\n",
        "├── baseline/\n",
        "│   ├── data_intermediate/\n",
        "│   ├── logs/\n",
        "│   ├── models/\n",
        "│   └── reports/\n",
        "│       └── performance_summary.csv\n",
        "│\n",
        "└── ablation_studies/\n",
        "    ├── ablation_no_hype/\n",
        "    │   ├── data_intermediate/\n",
        "    │   ├── logs/\n",
        "    │   ├── models/\n",
        "    │   └── reports/\n",
        "    ├── ... (other experiments)\n",
        "    │\n",
        "    ├── ablation_comparison_summary.csv\n",
        "    └── ablation_core_performance.png\n",
        "```\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "identifying_quantifying_financial_bubbles_hyped_log_period_power_law/\n",
        "│\n",
        "├── identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb # Main implementation notebook\n",
        "├── config.yaml                                                                      # Master configuration file\n",
        "├── requirements.txt                                                                 # Python package dependencies\n",
        "├── LICENSE                                                                          # MIT license file\n",
        "└── README.md                                                                        # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all study parameters, including LPPL window size, `BubbleScore` weights, Transformer architecture, and backtesting thresholds, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Architectures:** Replacing the Transformer with other sequence models like LSTMs or state-space models (e.g., Mamba).\n",
        "-   **Dynamic Alpha Weights:** Making the `alpha_1` and `alpha_2` weights in the `BubbleScore` dynamic, perhaps dependent on market volatility.\n",
        "-   **Advanced Backtesting:** Integrating a more sophisticated backtesting engine that handles portfolio-level constraints, realistic order execution, and market impact.\n",
        "-   **Cross-Asset Analysis:** Applying the HLPPL framework to other asset classes like cryptocurrencies, commodities, or fixed income.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{cao2025identifying,\n",
        "  title   = {Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model},\n",
        "  author  = {Cao, Zheng and Shao, Xingran and Yan, Yuheng and Geman, Helyette},\n",
        "  journal = {arXiv preprint arXiv:2510.10878},\n",
        "  year    = {2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the \"Hyped Log-Periodic Power Law Model\" Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/identifying_quantifying_financial_bubbles_hyped_log_period_power_law\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Zheng Cao, Xingran Shao, Yuheng Yan, and Helyette Geman** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Scikit-learn, PyTorch, Hugging Face, SciPy, and Jupyter**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `identifying_quantifying_financial_bubbles_hyped_log_period_power_law_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "tYIUizJAlnSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model*\"\n",
        "\n",
        "Authors: Zheng Cao, Xingran Shao, Yuheng Yan, Helyette Geman\n",
        "\n",
        "E-Journal Submission Date: 13 October 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2510.10878\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We propose a novel model, the Hyped Log-Periodic Power Law Model (HLPPL), to the problem of quantifying and detecting financial bubbles, an ever-fascinating one for academics and practitioners alike. Bubble labels are generated using a Log-Periodic Power Law (LPPL) model, sentiment scores, and a hype index we introduced in previous research on NLP forecasting of stock return volatility. Using these tools, a dual-stream transformer model is trained with market data and machine learning methods, resulting in a time series of confidence scores as a Bubble Score. A distinctive feature of our framework is that it captures phases of extreme overpricing and underpricing within a unified structure.\n",
        "\n",
        "We achieve an average yield of 34.13 percentage annualized return when backtesting U.S. equities during the period 2018 to 2024, while the approach exhibits a remarkable generalization ability across industry sectors. Its conservative bias in predicting bubble periods minimizes false positives, a feature which is especially beneficial for market signaling and decision-making. Overall, this approach utilizes both theoretical and empirical advances for real-time positive and negative bubble identification and measurement with HLPPL signals."
      ],
      "metadata": {
        "id": "CwPGYha2JofX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **The Core Problem and Proposed Contribution**\n",
        "\n",
        "The paper addresses a well-known, difficult problem in quantitative finance: the timely detection and quantification of financial bubbles. The authors correctly identify two primary limitations of existing models, particularly the classic Log-Periodic Power Law (LPPL) model:\n",
        "\n",
        "1.  **Asymmetry:** Traditional models focus almost exclusively on positive bubbles (speculative manias) and largely ignore \"negative bubbles\" or anti-bubbles (protracted periods of undervaluation and oversold conditions).\n",
        "2.  **Technical Isolation:** The LPPL model is a purely technical, price-based framework. It describes the *what* (super-exponential price acceleration) but not the *why*. It is agnostic to the underlying behavioral drivers like investor sentiment and media hype, which are central to the formation of bubbles.\n",
        "\n",
        "The paper's core contribution is to create a unified framework, the **Hyped Log-Periodic Power Law (HLPPL) Model**, that addresses both limitations by integrating behavioral signals directly into the technical model.\n",
        "\n",
        "### **Deconstructing the \"Bubble Score\" - The HLPPL Foundation**\n",
        "\n",
        "The first major innovation is the creation of a descriptive indicator they call the **Bubble Score**. This is not a machine learning output; rather, it's a composite score engineered from three components.\n",
        "\n",
        "1.  **LPPL Residuals (The Technical Base):** Instead of just using the LPPL model's crash prediction, the authors cleverly focus on the *residuals*—the difference between the actual log-price and the LPPL-fitted price trajectory.\n",
        "    *   A positive residual (`ε(t) > 0`) signifies that the price is running *ahead* of its super-exponential trend, indicating a \"bubble behavior.\"\n",
        "    *   A negative residual (`ε(t) < 0`) signifies the price is lagging, indicating a \"negative behavior\" or oversold state.\n",
        "    *   These residuals are normalized to lie within `[-1, 1]` for comparability.\n",
        "\n",
        "2.  **Hype Index (The Attention Metric):** Drawing on their previous work, they incorporate a novel NLP-derived metric. The Hype Index measures the *volume* of media attention a stock receives relative to its peers and its economic size (market capitalization). It quantifies disproportionate attention, which is a key catalyst for herding behavior.\n",
        "\n",
        "3.  **Sentiment Score (The Polarity Metric):** This is a more standard NLP metric, derived using FinBERT to classify financial news as positive, neutral, or negative. It captures the *tone* of the media coverage.\n",
        "\n",
        "These three components are combined into a single **Bubble Score** (Equation 14). The formula is intuitive: the normalized LPPL residual forms the base, which is then amplified or dampened by the Hype and Sentiment scores. Crucially, the Hype Index is designed to always amplify the extremity (i.e., it makes positive scores more positive and negative scores more negative), reflecting the idea that high attention exacerbates any market move.\n",
        "\n",
        "### **The Predictive Engine - The Dual-Stream Transformer**\n",
        "\n",
        "Having created a rich, descriptive `Bubble Score`, the authors pivot to prediction. They frame the problem as a supervised learning task: can we train a model to forecast the `Bubble Score` over the next five trading days?\n",
        "\n",
        "This is where their computer science expertise comes into play. They design a sophisticated **Dual-Stream Transformer Architecture**:\n",
        "\n",
        "*   **Stream 1 (Stock-Level):** This stream processes features specific to an individual asset, such as its historical prices, volume, and valuation ratios (P/E, P/B).\n",
        "*   **Stream 2 (Market-Level):** This stream processes market-wide and behavioral features, including the VIX, aggregate sentiment scores, and the Hype Index.\n",
        "\n",
        "The architecture uses self-attention within each stream to capture temporal dynamics and then employs **cross-attention** between the streams. This is a powerful design choice, as it allows the model to learn complex interactions, such as how a shift in market-wide sentiment (Stream 2) might influence the price trajectory of a specific stock (Stream 1).\n",
        "\n",
        "The model is trained to predict the `Bubble Score` at horizons of 1 to 5 days, using the score calculated in Step 2 as the \"ground truth\" label.\n",
        "\n",
        "### **Empirical Validation and Backtesting**\n",
        "\n",
        "The authors conduct a two-stage backtest on U.S. real estate stocks from 2018 to 2024.\n",
        "\n",
        "1.  **\"Traditional\" Strategy:** A simple rules-based strategy trading directly on the calculated `Bubble Score`. (e.g., Go short if Score > 0.7, go long if Score < -0.7). This strategy yielded an average annualized return of **16.64%** with a Sharpe ratio of **0.72**.\n",
        "\n",
        "2.  **\"ML-Enhanced\" Strategy:** This strategy trades on the *forecasted* `Bubble Score` from the Transformer model. This is the main test of their predictive framework. The results are substantially stronger:\n",
        "    *   Average Annualized Return: **34.13%**\n",
        "    *   Average Sharpe Ratio: **1.19**\n",
        "    *   Average Win Rate: **72.30%**\n",
        "\n",
        "They highlight an exceptional case study, HOUS, where the ML-enhanced strategy achieved an annualized return of **85.80%** with a Sharpe ratio of **2.49**, demonstrating the model's potential when a stock's dynamics align perfectly with the HLPPL framework.\n",
        "\n",
        "--\n",
        "\n",
        "### **Critical Analysis and Discussion**\n",
        "\n",
        "This is a highly commendable and methodologically sophisticated paper. The synthesis of ideas is novel and the empirical results are, on the surface, extremely impressive.\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "*   **Conceptual Elegance:** The idea of using LPPL residuals as a \"technical anchor\" for behavioral metrics is elegant and powerful. It grounds the abstract concepts of hype and sentiment in the mathematical reality of price dynamics.\n",
        "*   **Bidirectional Analysis:** The explicit modeling of both positive and negative bubbles is a significant and practical contribution, opening up a richer set of trading opportunities.\n",
        "*   **Sophisticated ML Architecture:** The choice of a dual-stream transformer with cross-attention is well-justified and perfectly suited to the problem of disentangling idiosyncratic and systematic drivers.\n",
        "\n",
        "**Potential Weaknesses and Questions for the Authors:**\n",
        "\n",
        "1.  **The Self-Referential Labeling Problem:** This is the most significant concern. The Transformer model is trained to predict a `Bubble Score` that is, itself, a model output. The \"ground truth\" is not an objective market fact but a construct of the authors' HLPPL formula. While the strategy's profitability provides external validation, it's possible the Transformer is simply becoming very good at approximating the complex HLPPL function, rather than predicting an objective, underlying economic phenomenon.\n",
        "2.  **LPPL Model Instability:** The fitting of the LPPL model is notoriously sensitive to the choice of the time window and initial parameters. The paper does not detail how this fitting was performed (e.g., rolling window size, parameter constraints). The stability of the LPPL fit is paramount, as unstable fits would lead to noisy residuals and, consequently, noisy training labels for the Transformer.\n",
        "3.  **Risk of Overfitting and Data Snooping:** The model is complex, and the backtest is confined to a single sector (real estate) during a specific, highly volatile period (2018-2024, which includes the COVID-19 crash and recovery). The stellar performance could be regime-dependent. The model must be tested on different asset classes (e.g., tech stocks, cryptocurrencies, commodities) and across different market regimes (e.g., the low-volatility bull market of 2013-2017) to prove its robustness.\n",
        "4.  **Practical Implementation Costs:** While a 0.1% transaction cost is included, the model relies on signals that can appear and disappear quickly. The backtest does not account for slippage or market impact, which could be significant when trying to execute trades based on rapidly changing news sentiment.\n",
        "\n",
        "### **Conclusion and Verdict**\n",
        "\n",
        "This paper presents a state-of-the-art framework for bubble detection that pushes the research frontier forward. The authors have successfully built a bridge between econophysics, behavioral finance, and deep learning. The HLPPL model and its resulting `Bubble Score` are a significant conceptual advance.\n",
        "\n",
        "The empirical results are compelling, suggesting that this integrated approach can generate substantial alpha. However, due to concerns about the self-referential nature of the training labels and the need for more extensive out-of-sample and cross-asset validation, the findings should be considered promising but preliminary.\n",
        "\n",
        "Hence, one would encourage the authors to next perform rigorous sensitivity analysis on the LPPL fitting process and to backtest their framework across a much wider universe of assets and time periods. If the results hold, this methodology could become a valuable tool for both asset managers and financial regulators.\n"
      ],
      "metadata": {
        "id": "9IdfmL5MPUNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "lV_plSRlplhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic\n",
        "#  Power Law (HLPPL) Model\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Identifying and Quantifying Financial\n",
        "#  Bubbles with the Hyped Log-Periodic Power Law Model\" by Cao, Shao, Yan, and\n",
        "#  Geman (2025). It delivers a real-time, data-driven system for the dynamic\n",
        "#  assessment of asset mispricing by fusing econophysics, natural language\n",
        "#  processing, and deep learning into a single, actionable metric for\n",
        "#  optimizing investment timing and managing risk exposure.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Log-Periodic Power Law (LPPL) model fitting via constrained non-linear\n",
        "#    optimization to generate a technical measure of deviation from a theoretical\n",
        "#    bubble trajectory.\n",
        "#  • NLP-driven behavioral feature engineering, including a Hype Index (attention\n",
        "#    share) and a confidence-weighted sentiment score derived from FinBERT.\n",
        "#  • A novel \"Bubble Score\" signal that fuses the technical LPPL residual with\n",
        "#    the behavioral NLP signals in a regime-dependent manner.\n",
        "#  • A Dual-Stream Transformer architecture that processes stock-specific and\n",
        "#    market-level features in parallel, with bi-directional cross-attention\n",
        "#    to learn their interactions.\n",
        "#  • Multi-horizon forecasting of the Bubble Score using independent prediction heads.\n",
        "#  • A composite loss function combining Huber, correlation, R-squared, and\n",
        "#    temporal consistency objectives for robust training.\n",
        "#  • An event-driven backtesting engine with realistic risk controls (stop-loss,\n",
        "#    transaction costs) to evaluate strategy performance.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Idempotent, checkpointed pipeline for all computationally intensive tasks.\n",
        "#  • Rigorous, leakage-proof data validation, cleansing, and normalization.\n",
        "#  • BERTopic for thematic filtering of large news corpora.\n",
        "#  • Multi-start optimization with a seeded random number generator for LPPL fits.\n",
        "#  • Modern deep learning training regimen including AdamW, OneCycleLR, gradient\n",
        "#    clipping, and early stopping.\n",
        "#  • Automated ablation and sensitivity analysis framework.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Cao, Z., Shao, X., Yan, Y., & Geman, H. (2025). Identifying and Quantifying\n",
        "#  Financial Bubbles with the Hyped Log-Periodic Power Law Model. arXiv\n",
        "#  preprint arXiv:2510.10878.\n",
        "#  https://arxiv.org/abs/2510.10878\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# Consolidated Imports for the HLPPL End-to-End Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Standard Library ---\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import pickle\n",
        "import subprocess\n",
        "import tarfile\n",
        "from collections.abc import Mapping\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from typing import (Any, Dict, List, NamedTuple, Optional, Set, Tuple, Union)\n",
        "\n",
        "# --- Core Scientific Computing ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Machine Learning & Deep Learning (PyTorch) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- NLP Libraries ---\n",
        "# Note: These are substantial dependencies.\n",
        "# pip install transformers sentence-transformers bertopic umap-learn hdbscan\n",
        "import hdbscan\n",
        "import umap\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
        "                          pipeline)\n",
        "\n",
        "# --- Scientific & Numerical Optimization ---\n",
        "# pip install scipy\n",
        "from scipy.optimize import least_squares, OptimizeResult\n",
        "\n",
        "# --- Visualization & Progress ---\n",
        "# pip install matplotlib seaborn tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Configure a basic logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n"
      ],
      "metadata": {
        "id": "_EHuuERappXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "i4yGKAm4pqo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional and Methodological Breakdown of Pipeline Orchestrators**\n",
        "\n",
        "#### **Task 1: `validate_and_parse_config`**\n",
        "\n",
        "*   **Inputs:** A Python dictionary (`study_parameters`) containing the nested configuration for the entire study.\n",
        "*   **Processes:**\n",
        "    1.  Recursively traverses the input dictionary to verify that its structure matches a predefined schema, ensuring all required keys and sections are present.\n",
        "    2.  Performs a series of specific validation checks on key numerical and string parameters to ensure they are within valid, sensible ranges and match expected values (e.g., model names).\n",
        "    3.  Creates a timestamped JSON snapshot of the validated configuration.\n",
        "*   **Outputs:** The original, validated configuration dictionary. A `KeyError`, `ValueError`, or `TypeError` is raised if any check fails.\n",
        "*   **Data Transformation:** The primary transformation is one of state validation and persistence. The dictionary is validated, and a serializable copy is written to disk.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **Gatekeeper** of the entire pipeline. It implements the foundational step of ensuring that the experiment is configured correctly and reproducibly before any computation begins. It enforces methodological consistency by validating parameters (e.g., LPPL constraints, backtesting thresholds) that are specified in the paper's text and are critical for a faithful replication.\n",
        "\n",
        "#### **Task 2: `validate_input_dataframe`**\n",
        "\n",
        "*   **Inputs:** A raw `pandas.DataFrame` (`df_raw`) with a `MultiIndex`.\n",
        "*   **Processes:**\n",
        "    1.  Validates that the index is a `pd.MultiIndex` with the correct names (`'Date'`, `'TICKER'`), dtypes (`datetime64[ns]`, `object`), and is chronologically sorted.\n",
        "    2.  Verifies the presence and correct dtypes of all required columns (e.g., `Close_Price_Raw`, `CFACSHR`, `News_Articles`).\n",
        "    3.  Checks that the data coverage meets minimum requirements for the study period (2018-2024), number of trading days, and number of unique tickers.\n",
        "*   **Outputs:** A validated and sorted copy of the input DataFrame.\n",
        "*   **Data Transformation:** The DataFrame is transformed by sorting its index if it was not already sorted. Otherwise, it is a validation step.\n",
        "*   **Role in Research Pipeline:** This callable is the **Data Schema Enforcer**. It ensures that the raw input data conforms to the precise structure and content requirements of the pipeline, preventing a vast category of potential downstream errors related to incorrect data types, missing columns, or unsorted time series.\n",
        "\n",
        "#### **Task 3: `cleanse_raw_data`**\n",
        "\n",
        "*   **Inputs:** A validated `pandas.DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  Removes rows with missing or non-positive `Close_Price_Raw`.\n",
        "    2.  Conditionally forward-fills `NaN`s in `Volume_Raw` if the missing percentage is below a threshold.\n",
        "    3.  Asserts completeness of `CFACSHR` and forward-fills `VIX_Close`.\n",
        "    4.  Logs extreme single-day log returns ($|r_t| > 0.5$) as potential outliers.\n",
        "    5.  Filters the entire DataFrame to retain only stocks within the specified real estate `SIC_Code` universe.\n",
        "*   **Outputs:** A cleansed and filtered `pandas.DataFrame`.\n",
        "*   **Data Transformation:** The DataFrame is transformed by removing rows (price cleaning, SIC filtering) and imputing a limited set of missing values (volume, VIX).\n",
        "*   **Role in Research Pipeline:** This callable is the **Data Purifier and Universe Selector**. It implements the initial data cleaning and universe selection steps, ensuring the data is free of critical errors and represents the specific sector (real estate) analyzed in the paper.\n",
        "\n",
        "#### **Task 4: `adjust_for_corporate_actions`**\n",
        "\n",
        "*   **Inputs:** A cleansed `pandas.DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  Computes a new `Close_Price_Adj` column using the formula:\n",
        "        $$\n",
        "        \\text{Close\\_Price\\_Adj} = \\text{Close\\_Price\\_Raw} \\times \\text{CFACSHR}\n",
        "        $$\n",
        "    2.  Computes a new `Volume_Adj` column using the formula:\n",
        "        $$\n",
        "        \\text{Volume\\_Adj} = \\frac{\\text{Volume\\_Raw}}{\\text{CFACSHR}}\n",
        "        $$\n",
        "    3.  Performs a programmatic spot-check to verify adjustment consistency.\n",
        "    4.  Drops the `CFACSHR` column to prevent data leakage.\n",
        "*   **Outputs:** A `pandas.DataFrame` with adjusted price and volume columns.\n",
        "*   **Data Transformation:** The DataFrame is transformed by adding two new columns (`Close_Price_Adj`, `Volume_Adj`) and removing one (`CFACSHR`).\n",
        "*   **Role in Research Pipeline:** This callable is the **Time Series Normalizer**. Its role is to create a continuous and comparable time series for price and volume by removing the artificial jumps caused by corporate actions like splits and dividends. This is a non-negotiable prerequisite for any meaningful time-series analysis, including the LPPL fitting and return calculations.\n",
        "\n",
        "#### **Task 5: `derive_engineered_features`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame` with adjusted price and volume.\n",
        "*   **Processes:**\n",
        "    1.  Computes log-transformed price and volume:\n",
        "        $$\n",
        "        \\text{Log\\_Price} = \\ln(\\text{Close\\_Price\\_Adj})\n",
        "        $$\n",
        "        $$\n",
        "        \\text{Log\\_Volume} = \\ln(\\text{Volume\\_Adj} + 1)\n",
        "        $$\n",
        "    2.  Computes daily log returns on a per-ticker basis:\n",
        "        $$\n",
        "        r_t = \\text{Log\\_Price}_t - \\text{Log\\_Price}_{t-1}\n",
        "        $$\n",
        "    3.  Extracts integer `Month` and `Day` features from the `Date` index.\n",
        "*   **Outputs:** A `pandas.DataFrame` enriched with the new feature columns.\n",
        "*   **Data Transformation:** The DataFrame is transformed by adding four new feature columns.\n",
        "*   **Role in Research Pipeline:** This callable is the **Primary Feature Engineer**. It derives the foundational technical and calendar features that will be used as inputs for both the LPPL model (via `Log_Price`) and the Transformer model.\n",
        "\n",
        "#### **Task 6: `align_and_validate_calendar`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame` with engineered features.\n",
        "*   **Processes:**\n",
        "    1.  Constructs a master `DatetimeIndex` of all unique trading days and checks it for large gaps.\n",
        "    2.  Analyzes the number of tickers per day to assess if the panel is balanced or unbalanced.\n",
        "    3.  Enforces consistency on market-wide features (e.g., `VIX_Close`) to ensure every ticker has the identical value for a given day.\n",
        "*   **Outputs:** A `pandas.DataFrame` with a validated temporal structure.\n",
        "*   **Data Transformation:** The `VIX_Close` column is potentially modified to enforce consistency.\n",
        "*   **Role in Research Pipeline:** This callable is the **Temporal Integrity Validator**. It ensures the time dimension of the panel data is coherent and complete before any time-sensitive operations (like NLP aggregation or model fitting) are performed.\n",
        "\n",
        "#### **Task 7: `setup_topic_model`**\n",
        "\n",
        "*   **Inputs:** The main `pandas.DataFrame` and the study configuration.\n",
        "*   **Processes:**\n",
        "    1.  Extracts all unique news articles from the `News_Articles` column into a single corpus.\n",
        "    2.  Uses a pre-trained `SentenceTransformer` model (e.g., `\"all-MiniLM-L6-v2\"`) to convert each unique article into a high-dimensional vector embedding.\n",
        "    3.  Initializes and fits a `BERTopic` model on these embeddings, using the specified `UMAP` and `HDBSCAN` hyperparameters for dimensionality reduction and clustering.\n",
        "*   **Outputs:** A tuple containing the unique corpus (list of strings), the embeddings (numpy array), and the fitted `BERTopic` model object.\n",
        "*   **Data Transformation:** A list of lists of strings is transformed into a flat list of unique strings, a dense numerical matrix of embeddings, and a trained topic model object.\n",
        "*   **Role in Research Pipeline:** This callable implements the first major step of the NLP pipeline described in Section 4.2.1. It is the **Thematic Structure Extractor**, responsible for discovering the underlying topics within the entire news corpus.\n",
        "\n",
        "#### **Task 8: `apply_topic_filter`**\n",
        "\n",
        "*   **Inputs:** The main `DataFrame`, the unique corpus, and the fitted `BERTopic` model.\n",
        "*   **Processes:**\n",
        "    1.  Extracts the keyword representations for each topic discovered by the model.\n",
        "    2.  Identifies a subset of topics as \"real estate-relevant\" by matching their keywords against a predefined set (e.g., `\"housing\"`, `\"REIT\"`).\n",
        "    3.  Filters the `News_Articles` column in the main DataFrame, removing any article that does not belong to one of the identified relevant topics.\n",
        "*   **Outputs:** A `pandas.DataFrame` with the filtered `News_Articles` column and a `set` of the retained article texts.\n",
        "*   **Data Transformation:** The lists within the `News_Articles` column are modified (shortened or emptied).\n",
        "*   **Role in Research Pipeline:** This callable is the **Domain-Specific Information Filter**. It implements the second step of the NLP pipeline in Section 4.2.1, ensuring that the subsequent sentiment analysis is performed only on text that is thematically relevant to the study's universe (real estate), thereby reducing noise.\n",
        "\n",
        "#### **Task 9: `classify_article_sentiment`**\n",
        "\n",
        "*   **Inputs:** The set of filtered, relevant articles and the study configuration.\n",
        "*   **Processes:**\n",
        "    1.  Loads the pre-trained `ProsusAI/finbert` model and tokenizer.\n",
        "    2.  Runs batched inference on the entire corpus of relevant articles to classify each as 'positive', 'neutral', or 'negative' and obtain a confidence score.\n",
        "    3.  Maps the string labels to numerical polarity scores (`+1.0`, `0.0`, `-1.0`).\n",
        "*   **Outputs:** A dictionary mapping each unique article text to its sentiment class, confidence, and numerical polarity.\n",
        "*   **Data Transformation:** A set of strings is transformed into a dictionary mapping strings to structured sentiment data.\n",
        "*   **Role in Research Pipeline:** This callable is the **Affective Classifier**. It implements the core sentiment analysis step from Section 4.2.1, quantifying the emotional tone of each relevant news article.\n",
        "\n",
        "#### **Task 10: `aggregate_stock_day_sentiment`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` with filtered `News_Articles` and the dictionary of sentiment results.\n",
        "*   **Processes:** For each row (i.e., for each stock `i` on day `t`), it calculates the confidence-weighted average of the polarity scores of all articles in its `News_Articles` list. The formula implemented is from Section 3.3, Equation (9):\n",
        "    $$\n",
        "    S_{i,t} = \\frac{\\sum_{k=1}^{N_{i,t}} w_{i,t,k} s_{i,t,k}}{\\sum_{k=1}^{N_{i,t}} w_{i,t,k}}\n",
        "    $$\n",
        "    where the weight $w_{i,t,k}$ is the FinBERT confidence score.\n",
        "*   **Outputs:** A `pandas.DataFrame` with a new `Sentiment_Score` column.\n",
        "*   **Data Transformation:** The `News_Articles` column (list of strings) is transformed into a new `Sentiment_Score` column (float).\n",
        "*   **Role in Research Pipeline:** This callable is the **Stock-Level Sentiment Aggregator**. It bridges the gap from individual article sentiments to a structured, time-series feature, $S_{i,t}$, representing the daily sentiment for a specific stock.\n",
        "\n",
        "#### **Task 11: `aggregate_market_level_sentiment`**\n",
        "\n",
        "*   **Inputs:** The main `DataFrame` and the dictionary of sentiment results.\n",
        "*   **Processes:** For each day `t`, it aggregates the confidence-weighted one-hot vectors of *all* articles published on that day across *all* tickers. It then normalizes these sums to get the daily market-wide share of positive, neutral, and negative sentiment. The formula implemented is from Section 4.2.1, Equation (17):\n",
        "    $$\n",
        "    S_t^{c} = \\frac{\\sum_{i,k \\in D_t} s_{i,t,k}^{c}}{\\sum_{i,k \\in D_t} p_{i,t,k}}\n",
        "    $$\n",
        "*   **Outputs:** A `pandas.DataFrame` with three new columns: `Market_Sentiment_Neg`, `Market_Sentiment_Neu`, `Market_Sentiment_Pos`.\n",
        "*   **Data Transformation:** The `News_Articles` column is transformed into three new market-wide feature columns.\n",
        "*   **Role in Research Pipeline:** This callable is the **Market-Level Sentiment Aggregator**. It creates the macro-behavioral features required for the market stream of the Dual-Stream Transformer.\n",
        "\n",
        "#### **Task 12: `construct_hype_index`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` with the filtered `News_Articles` column.\n",
        "*   **Processes:**\n",
        "    1.  Counts the number of articles for each stock-day, $N_{i,t}$.\n",
        "    2.  Sums these counts across all tickers for each day to get the total market-wide article count, $N_{\\text{mkt},t}$.\n",
        "    3.  Calculates the Hype Index as the ratio, as defined in Section 3.4, Equation (11):\n",
        "        $$\n",
        "        H_{i,t} = \\frac{N_{i,t}}{N_{\\text{mkt},t}}\n",
        "        $$\n",
        "*   **Outputs:** A `pandas.DataFrame` with a new `Hype_Index` column.\n",
        "*   **Data Transformation:** The `News_Articles` column is transformed into a new `Hype_Index` column (float).\n",
        "*   **Role in Research Pipeline:** This callable is the **Attention Share Calculator**. It implements the Hype Index, a key behavioral feature that measures the intensity of media attention, distinct from its tone.\n",
        "\n",
        "#### **Task 13: `define_lppl_calibration_windows`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` with the `Log_Price` column and the study configuration.\n",
        "*   **Processes:** For each ticker, it generates a list of all possible overlapping windows of `Log_Price` data with a fixed length `W` (e.g., 250 days). It validates that each window is complete (no `NaN`s) and prepares it with an integer time index from 1 to `W`.\n",
        "*   **Outputs:** A list of `LPPLWindow` objects, where each object contains the data and metadata for one window.\n",
        "*   **Data Transformation:** The continuous `Log_Price` time series for each stock is transformed into a discrete list of fixed-length segments.\n",
        "*   **Role in Research Pipeline:** This callable is the **LPPL Problem Definer**. It prepares the individual, well-defined data segments that will be fed into the LPPL optimization algorithm.\n",
        "\n",
        "#### **Task 14: `initialize_lppl_fitter`**\n",
        "\n",
        "*   **Inputs:** The study configuration.\n",
        "*   **Processes:**\n",
        "    1.  Defines the strict mathematical bounds for each of the 7 LPPL parameters (e.g., $0 < m < 1$, $B < 0$).\n",
        "    2.  Defines the strategy for generating multiple random starting points (seeds) for the optimization.\n",
        "    3.  Documents this entire initialization strategy in a metadata file.\n",
        "*   **Outputs:** A tuple containing the parameter bounds and the configuration.\n",
        "*   **Data Transformation:** The configuration parameters are transformed into a structured set of bounds and a documented strategy for the optimizer.\n",
        "*   **Role in Research Pipeline:** This callable is the **Optimizer Initializer**. It sets up the constrained, multi-start optimization framework required for robustly fitting the notoriously difficult LPPL model, as described in Section 2.1.1.\n",
        "\n",
        "#### **Task 15: `fit_lppl_model_to_windows`**\n",
        "\n",
        "*   **Inputs:** The list of `LPPLWindow` objects and the parameter bounds.\n",
        "*   **Processes:** For each window, it runs a constrained non-linear least squares optimization (`scipy.optimize.least_squares`) multiple times, starting from each of the random seeds. It seeks to find the parameter vector $\\theta = (A, B, C, m, \\omega, \\phi, t_c)$ that minimizes the sum of squared errors:\n",
        "    $$\n",
        "    J(\\theta) = \\sum_{i=1}^{W} \\big[\\ln p_i - \\ln \\hat{p}(t_i; \\theta)\\big]^2\n",
        "    $$\n",
        "    It selects the best valid fit (lowest error) from the multiple starts.\n",
        "*   **Outputs:** A `pandas.DataFrame` where each row contains the best-fit 7 parameters for a single window.\n",
        "*   **Data Transformation:** The list of data windows is transformed into a table of fitted model parameters.\n",
        "*   **Role in Research Pipeline:** This callable is the **LPPL Calibration Engine**. It is the computational core of the econophysics analysis, performing the thousands of optimizations required to fit the LPPL model across the entire dataset.\n",
        "\n",
        "#### **Task 16: `compute_and_merge_lppl_residuals`**\n",
        "\n",
        "*   **Inputs:** The main `DataFrame` and the `lppl_fits` DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  For each fitted window, it calculates the raw residuals: $\\epsilon(t) = \\ln p(t) - \\ln \\hat{p}(t)$, as defined in Section 3.2, Equation (5).\n",
        "    2.  It combines the residuals from all overlapping windows.\n",
        "    3.  It then computes the normalized residual $\\epsilon_{\\text{norm}}(t)$ on a per-ticker basis using a running maximum, as defined in Equation (8):\n",
        "        $$\n",
        "        \\epsilon_{\\text{norm}}(t) = \\frac{\\epsilon(t)}{\\max_{s \\le t} |\\epsilon(s)|}\n",
        "        $$\n",
        "    4.  Merges the final `Residual_Norm` series back into the main DataFrame.\n",
        "*   **Outputs:** A `pandas.DataFrame` with a new `Residual_Norm` column.\n",
        "*   **Data Transformation:** The table of fitted parameters is transformed into a single, normalized time-series feature.\n",
        "*   **Role in Research Pipeline:** This callable is the **Technical Signal Extractor**. It translates the results of the LPPL fits into the primary technical indicator, $\\epsilon_{\\text{norm}}(t)$, which quantifies the price deviation from the theoretical bubble path.\n",
        "\n",
        "#### **Task 17: `construct_bubblescore`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` containing `Residual_Norm`, `Hype_Index`, and `Sentiment_Score`.\n",
        "*   **Processes:** It implements the core HLPPL fusion logic from Section 3.5, Equation (14), a piecewise formula that depends on the sign of the residual:\n",
        "    $$\n",
        "    \\text{BubbleScore}_{i}(t) =\n",
        "    \\begin{cases}\n",
        "    \\epsilon_{\\text{norm}}(t) + \\alpha_1 H_{i,t} + \\alpha_2 S_{i,t}, & \\text{if } \\epsilon_{\\text{norm}}(t) > 0 \\\\\n",
        "    \\epsilon_{\\text{norm}}(t) - \\alpha_1 H_{i,t} + \\alpha_2 S_{i,t}, & \\text{if } \\epsilon_{\\text{norm}}(t) \\le 0\n",
        "    \\end{cases}\n",
        "    $$\n",
        "*   **Outputs:** A `pandas.DataFrame` with a new `BubbleScore` column.\n",
        "*   **Data Transformation:** Three feature columns are fused into a single, more powerful feature column.\n",
        "*   **Role in Research Pipeline:** This callable is the **Signal Fusion Engine**. It implements the central hypothesis of the paper: that combining a technical signal with behavioral signals in a regime-dependent way creates a superior measure of mispricing. The output, `BubbleScore`, is the paper's key methodological innovation.\n",
        "\n",
        "#### **Task 18: `label_bubble_episodes`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` with the `BubbleScore` column.\n",
        "*   **Processes:** It implements the thresholding and persistence algorithm from Section 4.2.2. It identifies contiguous periods where $|\\text{BubbleScore}(t)| > \\tau$ and the duration is at least $d_{\\min}$ days.\n",
        "*   **Outputs:** A `pandas.DataFrame` with two new binary indicator columns (`In_Bubble_Episode`, `In_Negative_Episode`).\n",
        "*   **Data Transformation:** The continuous `BubbleScore` series is transformed into discrete event labels.\n",
        "*   **Role in Research Pipeline:** This callable is the **Event Labeler**. While the `BubbleScore` itself is the target for the regression model, these discrete labels are useful for descriptive analysis and for understanding the model's output, as shown in the paper's case study figures.\n",
        "\n",
        "#### **Task 19: `engineer_stock_level_sequences`**\n",
        "\n",
        "*   **Inputs:** The fully featured `DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  Selects the stock-specific features (`Log_Price`, `PE_Ratio`, etc.).\n",
        "    2.  Performs Z-score normalization, crucially fitting the scaler *only* on the training data to prevent data leakage.\n",
        "    3.  Handles missing fundamental data by dropping rows.\n",
        "    4.  For each ticker, it constructs a set of overlapping sequences of length `L` (e.g., 60 days).\n",
        "*   **Outputs:** A list of numpy arrays (the sequences), a `MultiIndex` of their corresponding anchor points, and the fitted scaler objects.\n",
        "*   **Data Transformation:** The 2D panel DataFrame is transformed into a 3D-like dataset of sequences suitable for a Transformer model.\n",
        "*   **Role in Research Pipeline:** This callable is the **Stock-Stream Data Preparer**. It prepares the first of the two input streams for the Dual-Stream Transformer, as described in Section 4.1.\n",
        "\n",
        "#### **Task 20: `engineer_market_level_sequences`**\n",
        "\n",
        "*   **Inputs:** The fully featured `DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  Selects the market-wide features (`VIX_Close`, `Market_Sentiment_*`, etc.).\n",
        "    2.  Performs leakage-proof normalization on the continuous features.\n",
        "    3.  Constructs a set of overlapping sequences of length `L` for the market-wide data.\n",
        "*   **Outputs:** A dictionary mapping each anchor date to its corresponding market sequence array.\n",
        "*   **Data Transformation:** The market-level columns of the panel DataFrame are transformed into a lookup map of sequence arrays.\n",
        "*   **Role in Research Pipeline:** This callable is the **Market-Stream Data Preparer**. It prepares the second input stream for the Dual-Stream Transformer, providing the market context for each prediction.\n",
        "\n",
        "#### **Task 21: `construct_and_align_targets`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` with the `BubbleScore` column and the anchor indices from the sequence generation.\n",
        "*   **Processes:** For each anchor point at time `t`, it looks up the future values of the `BubbleScore` at times $t+1, t+2, \\ldots, t+5$. It uses a leakage-proof `groupby().shift()` operation. It drops any samples for which a complete set of future targets is not available.\n",
        "*   **Outputs:** A numpy array of target vectors and the final, filtered list of valid anchor indices.\n",
        "*   **Data Transformation:** The `BubbleScore` time series is transformed into a matrix of multi-horizon targets, perfectly aligned with the input sequences.\n",
        "*   **Role in Research Pipeline:** This callable is the **Supervised Learning Target Constructor**. It creates the ground-truth labels (`y`) that the Transformer model will be trained to predict.\n",
        "\n",
        "#### **Task 22: `split_dataset_chronologically`**\n",
        "\n",
        "*   **Inputs:** All the prepared data components (stock sequences, market sequences, targets, anchor indices).\n",
        "*   **Processes:** It determines date-based boundaries based on the specified ratios (e.g., 80-10-10) and partitions all data components into training, validation, and test sets. The split is strictly chronological to prevent look-ahead bias.\n",
        "*   **Outputs:** A dictionary containing the three partitioned `ModelDataset` objects.\n",
        "*   **Data Transformation:** The single, unified dataset is split into three disjoint subsets.\n",
        "*   **Role in Research Pipeline:** This callable is the **Cross-Validation Manager**. It implements the essential step of splitting data for model training and evaluation in a way that is valid for time-series forecasting.\n",
        "\n",
        "#### **Task 23: `DualStreamTransformer` (Class Definition)**\n",
        "\n",
        "*   **Inputs:** A configuration dictionary and the number of input features for each stream.\n",
        "*   **Processes:** This is not a data processing function but a class that *defines* the neural network architecture. It constructs the PyTorch modules for the input projectors, positional encodings, parallel Transformer encoders, the bi-directional cross-attention layer, the fusion layer, and the multi-horizon prediction heads, as described in Section 4.1.\n",
        "*   **Outputs:** An instantiated `torch.nn.Module` object representing the model.\n",
        "*   **Data Transformation:** Not applicable.\n",
        "*   **Role in Research Pipeline:** This is the **Model Architect**. It provides the blueprint for the predictive engine of the entire system.\n",
        "\n",
        "#### **Task 24: `CompositeLoss` (Class Definition)**\n",
        "\n",
        "*   **Inputs:** A configuration dictionary with loss weights.\n",
        "*   **Processes:** This class defines the custom, multi-component loss function. Its `forward` method takes model predictions and targets and computes the weighted sum of the five loss components, as specified in Section \"Combined Loss Function\", Equation (15):\n",
        "    $$\n",
        "    \\mathcal{L} = \\lambda_1 \\mathcal{L}_{\\text{Huber}} + \\lambda_2 \\mathcal{L}_{\\text{Corr}} + \\lambda_3 \\mathcal{L}_{R^2} + \\lambda_4 \\mathcal{L}_{\\text{Cons}} + \\lambda_5 \\mathcal{L}_{\\text{Smooth}}\n",
        "    $$\n",
        "*   **Outputs:** An instantiated `torch.nn.Module` object that can be called to compute a scalar loss.\n",
        "*   **Data Transformation:** Not applicable.\n",
        "*   **Role in Research Pipeline:** This is the **Objective Function Definer**. It specifies the exact mathematical objective that the model training process will seek to minimize, tailoring the optimization towards producing robust, stable, and accurate financial forecasts.\n",
        "\n",
        "#### **Task 25-6: `train_and_validate_model`**\n",
        "\n",
        "*   **Inputs:** The instantiated model, the partitioned datasets, the loss function, and the configuration.\n",
        "*   **Processes:** This function orchestrates the entire model training process. It sets up the `AdamW` optimizer and `OneCycleLR` scheduler. It then runs the main training loop, which for each epoch, iterates through the training data, performs the forward/backward pass, and updates the model weights. It incorporates the specific regularization techniques mentioned in the paper: gradient clipping and dropout (handled by the model architecture). Crucially, it also runs a validation loop at the end of each epoch and implements early stopping to prevent overfitting and save the best-performing model.\n",
        "*   **Outputs:** The trained model loaded with its best weights, and a `DataFrame` of the training history.\n",
        "*   **Data Transformation:** The training data is used to transform the model from a randomly initialized state to a trained state.\n",
        "*   **Role in Research Pipeline:** This callable is the **Model Training Engine**. It is the computational core of the deep learning phase, responsible for optimizing the millions of parameters in the Transformer model.\n",
        "\n",
        "#### **Task 27: `persist_model_and_metadata`**\n",
        "\n",
        "*   **Inputs:** The study configuration and the training history.\n",
        "*   **Processes:**\n",
        "    1.  Validates the `best_model.pth` checkpoint saved by the training function.\n",
        "    2.  Creates a detailed `model_metadata.json` file, including hyperparameters and the Git commit hash of the code.\n",
        "    3.  Bundles all critical artifacts into a single `.tar.gz` archive.\n",
        "*   **Outputs:** A compressed archive file.\n",
        "*   **Data Transformation:** A collection of model artifacts and logs are transformed into a single, portable file.\n",
        "*   **Role in Research Pipeline:** This callable is the **Reproducibility Manager**. It ensures that the result of the computationally expensive training process is saved in a complete, auditable, and reproducible format.\n",
        "\n",
        "#### **Task 28: `run_inference_pipeline`**\n",
        "\n",
        "*   **Inputs:** The partitioned datasets, configuration, and model artifacts.\n",
        "*   **Processes:**\n",
        "    1.  Loads the best trained model from the saved checkpoint.\n",
        "    2.  Runs the model in evaluation mode on the held-out test set to generate multi-horizon predictions.\n",
        "    3.  Computes a suite of performance metrics (Correlation, MSE, RMSE, MAE) by comparing the predictions to the ground-truth targets.\n",
        "*   **Outputs:** A `DataFrame` of predictions and a `DataFrame` of performance metrics.\n",
        "*   **Data Transformation:** The test set input sequences are transformed into a `DataFrame` of predictions and then into a summary table of metrics.\n",
        "*   **Role in Research Pipeline:** This callable is the **Out-of-Sample Evaluator**. It provides the final, unbiased assessment of the model's predictive performance, generating the key results reported in the paper's Table 3 and Figure 11.\n",
        "\n",
        "#### **Task 29: `generate_trading_signals`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` of model predictions and the configuration.\n",
        "*   **Processes:** It translates the continuous numerical predictions into discrete trading signals (`LONG_ENTRY`, `SHORT_EXIT`, etc.) by applying the threshold-based rules and the prediction reversal rule described in Section 6.1.\n",
        "*   **Outputs:** A long-form `DataFrame` where each row is a specific trading signal for a given ticker, date, and horizon.\n",
        "*   **Data Transformation:** A `DataFrame` of continuous predictions is transformed into a `DataFrame` of discrete events.\n",
        "*   **Role in Research Pipeline:** This callable is the **Strategy Logic Interpreter**. It bridges the gap between the model's forecasts and an actionable trading strategy.\n",
        "\n",
        "#### **Task 30: `simulate_all_strategies`**\n",
        "\n",
        "*   **Inputs:** The `DataFrame` of signals, a `DataFrame` of prices, and the configuration.\n",
        "*   **Processes:** It runs an event-driven backtest for each unique strategy (ticker-horizon pair). It iterates day-by-day, manages position state, applies transaction costs, and enforces the stop-loss rule specified in Section 6.1.\n",
        "*   **Outputs:** Dictionaries containing the daily equity curves and detailed trade logs for every strategy.\n",
        "*   **Data Transformation:** The discrete signals and price series are transformed into a continuous equity curve and a log of realized trades.\n",
        "*   **Role in Research Pipeline:** This callable is the **Backtesting Engine**. It simulates the performance of the trading strategy in a realistic historical context, generating the raw PnL data required for performance evaluation.\n",
        "\n",
        "#### **Task 31: `report_backtest_performance`**\n",
        "\n",
        "*   **Inputs:** The dictionaries of equity curves and trade logs.\n",
        "*   **Processes:** For each simulated strategy, it calculates a comprehensive set of performance metrics (Annualized Return, Sharpe Ratio, Max Drawdown, Win Rate). It then aggregates these results into a summary table and computes the high-level findings reported in the paper, such as the top-5 performers (Table 4), overall success rate (Table 5), and the distribution of optimal horizons (Table 7).\n",
        "*   **Outputs:** A final summary `DataFrame` of performance metrics.\n",
        "*   **Data Transformation:** The raw simulation outputs (equity curves, trade logs) are transformed into a final, interpretable table of performance statistics.\n",
        "*   **Role in Research Pipeline:** This callable is the **Performance Analyst**. It distills the results of the backtest into the key metrics that determine the success or failure of the research project.\n",
        "\n",
        "#### **Task 32: `run_hlppl_pipeline`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  A raw `pandas.DataFrame` (`df_raw`) containing the complete, unprocessed dataset.\n",
        "    2.  A Python dictionary (`study_parameters`) containing the configuration for a single experimental run.\n",
        "    3.  A set of `pathlib.Path` objects (`intermediate_dir`, `model_dir`, `log_dir`, `report_dir`) specifying the isolated output directories for this specific run.\n",
        "*   **Processes:** This function is the **Master Orchestrator** for a single, end-to-end execution of the entire research methodology. It does not implement any novel algorithms itself but is responsible for the critical task of sequencing the calls to all previously defined task-level orchestrators (from Task 1 to Task 31). Its internal process is a direct, linear sequence:\n",
        "    1.  It begins by calling `validate_and_parse_config` (Task 1) and `validate_input_dataframe` (Task 2).\n",
        "    2.  It then proceeds through the data cleansing, feature engineering, and NLP/LPPL signal generation phases by calling the orchestrators from Tasks 3 through 18.\n",
        "    3.  It passes the fully featured DataFrame to the machine learning data preparation functions (Tasks 19-22) to create the final `partitioned_datasets`.\n",
        "    4.  It orchestrates the deep learning phase by defining the model and loss function (Tasks 23-24) and then calling `train_and_validate_model` (Task 26).\n",
        "    5.  It finalizes the modeling phase by calling `persist_model_and_metadata` (Task 27).\n",
        "    6.  Finally, it orchestrates the evaluation phase by calling the inference, signal generation, backtesting, and reporting functions (Tasks 28-31).\n",
        "    7.  The entire sequence is wrapped in a top-level `try...except` block to catch and log any failure at any stage of the pipeline.\n",
        "*   **Outputs:** The primary output is the final `performance_summary` `pandas.DataFrame` generated by the last step (Task 31). As side effects, it populates the specified output directories with all intermediate artifacts, logs, models, and reports generated during the run.\n",
        "*   **Data Transformation:** This function orchestrates the grand transformation of the entire project: converting the raw input `df_raw` and `study_parameters` into the final `performance_summary` DataFrame. It manages the flow of data through dozens of intermediate states.\n",
        "*   **Role in Research Pipeline:** This callable is the **Execution Engine**. While other functions define *what* to do, this function defines *in what order* to do it. Its role is to ensure the logical and causal dependencies between tasks are respected (e.g., you cannot calculate residuals before fitting the model). The amended version, which accepts parameterized output directories, makes it a reusable and modular workflow, which is essential for the ablation studies in Task 33. It represents the complete, reproducible recipe for a single experimental run.\n",
        "\n",
        "#### **Task 33: `run_ablation_studies`**\n",
        "\n",
        "*   **Inputs:** The raw `DataFrame` and the base configuration.\n",
        "*   **Processes:**\n",
        "    1.  Systematically generates a list of modified configurations to test different hypotheses (e.g., model performance without the Hype Index).\n",
        "    2.  For each modified configuration, it calls the master `run_hlppl_pipeline` function, executing the entire research process in an isolated directory.\n",
        "    3.  It collects the final performance summary from each run and aggregates them into a final comparison report and a set of visualizations.\n",
        "*   **Outputs:** A collection of reports and plots saved to disk.\n",
        "*   **Data Transformation:** A base configuration is transformed into a comprehensive analysis of the model's robustness and component contributions.\n",
        "*   **Role in Research Pipeline:** This callable is the **Robustness and Sensitivity Analyst**. It moves beyond a single point estimate of performance to explore the model's behavior under different conditions, providing a deeper understanding of what drives its success and how sensitive it is to key parameters.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "### Example of End-to-End Pipeline Execution\n",
        "\n",
        "Below is a high fidelity example, of how to execute the End-to-End Pipeline using sythentic data:\n",
        "<br>\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Main Execution Script for the HLPPL Research Pipeline\n",
        "# ==============================================================================\n",
        "# This script serves as the main entry point for running the entire end-to-end\n",
        "# research pipeline developed to replicate the \"Identifying and Quantifying\n",
        "# Financial Bubbles with the Hyped Log-Periodic Power Law Model\" study.\n",
        "#\n",
        "# It demonstrates the three core steps of a professional quantitative project:\n",
        "#   1. Configuration Loading: Loading all study parameters from an external,\n",
        "#      human-readable YAML file.\n",
        "#   2. Data Loading: Preparing the raw input DataFrame that matches the exact\n",
        "#      schema required by the pipeline. In this example, we generate a small,\n",
        "#      synthetic but structurally correct dataset for demonstration purposes.\n",
        "#   3. Pipeline Execution: Calling the top-level orchestrator function to run\n",
        "#      the baseline model and, optionally, the full suite of ablation studies.\n",
        "#\n",
        "# To run this script, ensure all previously defined functions (from Task 1 to 33)\n",
        "# are available in the execution scope (e.g., in preceding notebook cells or\n",
        "# imported from modules).\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml  # Requires PyYAML: pip install pyyaml\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Assume all orchestrator functions are defined in the current scope.\n",
        "# Assume all imports of the requisite Python modules have been made.\n",
        "# For example:\n",
        "# from .pipeline import execute_full_study\n",
        "\n",
        "def create_sample_dataframe() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a small, synthetic but structurally correct DataFrame for demonstration.\n",
        "    \n",
        "    In a real-world scenario, this function would be replaced with a data loader\n",
        "    that reads and merges data from actual sources like CRSP, Compustat, and news APIs.\n",
        "    \"\"\"\n",
        "    # Define the time range and tickers for the sample data.\n",
        "    dates = pd.to_datetime(pd.date_range(start='2018-01-01', end='2024-12-31', freq='B'))\n",
        "    tickers = ['HOUS', 'AMTX']\n",
        "    \n",
        "    # Create a MultiIndex from the product of dates and tickers.\n",
        "    index = pd.MultiIndex.from_product([dates, tickers], names=['Date', 'TICKER'])\n",
        "    \n",
        "    # Create the DataFrame with the correct index.\n",
        "    df = pd.DataFrame(index=index)\n",
        "    \n",
        "    # --- Generate Realistic Synthetic Data ---\n",
        "    # Generate random walks for the closing prices for each ticker.\n",
        "    log_returns = np.random.normal(loc=0.0005, scale=0.02, size=(len(dates), len(tickers)))\n",
        "    log_prices = np.log(100) + np.cumsum(log_returns, axis=0)\n",
        "    df['Close_Price_Raw'] = pd.DataFrame(np.exp(log_prices), index=dates, columns=tickers).stack()\n",
        "\n",
        "    # Generate other columns with the correct dtypes and realistic values.\n",
        "    df['PERMNO'] = df.index.get_level_values('TICKER').map({'HOUS': 10001, 'AMTX': 10002}).astype('int64')\n",
        "    df['SIC_Code'] = df.index.get_level_values('TICKER').map({'HOUS': 6531, 'AMTX': 2869}).astype('int64') # HOUS is in target universe\n",
        "    df['Volume_Raw'] = np.random.randint(100_000, 5_000_000, size=len(df)).astype('int64')\n",
        "    df['CFACSHR'] = 1.0 # Assume no corporate actions for simplicity in this sample.\n",
        "    df['PE_Ratio'] = np.random.uniform(10, 30, size=len(df))\n",
        "    df['PB_Ratio'] = np.random.uniform(1, 5, size=len(df))\n",
        "    \n",
        "    # VIX is a market-wide feature, so it should be the same for all tickers on a given day.\n",
        "    vix_series = pd.Series(np.random.uniform(12, 40, size=len(dates)), index=dates)\n",
        "    df['VIX_Close'] = df.index.get_level_values('Date').map(vix_series)\n",
        "    \n",
        "    # News articles: mostly empty lists, with some sample text.\n",
        "    news_list = [[] for _ in range(len(df))]\n",
        "    # Sprinkle in some sample articles.\n",
        "    for i in np.random.choice(len(df), size=50, replace=False):\n",
        "        news_list[i] = [\"This is a sample news article about real estate trends.\", \"Another article discusses market volatility.\"]\n",
        "    df['News_Articles'] = news_list\n",
        "    \n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main entry point to run the HLPPL study.\n",
        "    \"\"\"\n",
        "    # --- 1. Configuration Loading ---\n",
        "    # Define the path to the configuration file.\n",
        "    config_path = Path(\"config.yaml\")\n",
        "    \n",
        "    # Check if the configuration file exists.\n",
        "    if not config_path.exists():\n",
        "        logging.error(f\"Configuration file not found at '{config_path}'. Please create it.\")\n",
        "        return\n",
        "\n",
        "    # Load the study parameters from the YAML file.\n",
        "    logging.info(f\"Loading base configuration from '{config_path}'...\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        try:\n",
        "            # Use safe_load to prevent execution of arbitrary code.\n",
        "            base_config: Dict[str, Any] = yaml.safe_load(f)\n",
        "            logging.info(\"Base configuration loaded successfully.\")\n",
        "        except yaml.YAMLError as e:\n",
        "            logging.error(f\"Error parsing YAML configuration file: {e}\")\n",
        "            return\n",
        "\n",
        "    # --- 2. Data Loading / Generation ---\n",
        "    # In a real project, you would load your data from CSV, Parquet, or a database here.\n",
        "    # For this example, we generate a structurally correct synthetic DataFrame.\n",
        "    logging.info(\"Generating synthetic raw data for demonstration...\")\n",
        "    df_raw = create_sample_dataframe()\n",
        "    logging.info(f\"Generated sample DataFrame with {len(df_raw):,} rows.\")\n",
        "    \n",
        "    # --- 3. Pipeline Execution ---\n",
        "    # This is the main call to the top-level orchestrator.\n",
        "    # It will run the entire end-to-end pipeline.\n",
        "    \n",
        "    # For a full run including sensitivity analysis (takes a very long time):\n",
        "    # execute_full_study(df_raw=df_raw, base_config=base_config, run_ablation=True)\n",
        "    \n",
        "    # For a faster, baseline-only run:\n",
        "    results = execute_full_study(\n",
        "        df_raw=df_raw,\n",
        "        base_config=base_config,\n",
        "        run_ablation=False  # Set to False to skip the expensive ablation studies\n",
        "    )\n",
        "    \n",
        "    # --- 4. Display Final Results ---\n",
        "    # Print the key results returned by the orchestrator.\n",
        "    if results.get('status') == 'SUCCESS':\n",
        "        logging.info(\"\\n\\n--- FINAL BASELINE PERFORMANCE SUMMARY ---\")\n",
        "        # The baseline_performance DataFrame is the final output of the main run.\n",
        "        print(results.get('baseline_performance'))\n",
        "        logging.info(f\"All baseline outputs saved in: {results.get('baseline_output_directory')}\")\n",
        "    else:\n",
        "        logging.error(f\"Pipeline execution failed. Error: {results.get('error')}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block ensures the main function is called only when the script is executed directly.\n",
        "    main()\n",
        "```"
      ],
      "metadata": {
        "id": "lt4xHD-NPMF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Load the `study_parameters` dictionary and verify structural\n",
        "#                  completeness.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_config_structure(\n",
        "    config: Dict[str, Any],\n",
        "    schema: Dict[str, Any],\n",
        "    path: str = \"\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Recursively validates the nested structure of a configuration dictionary\n",
        "    against a schema.\n",
        "\n",
        "    This function ensures that all keys defined in the schema exist in the\n",
        "    configuration at the correct level of nesting. It raises a detailed\n",
        "    KeyError if a structural mismatch is found.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "        schema (Dict[str, Any]): A dictionary representing the expected\n",
        "                                 structure. Nested dictionaries in the schema\n",
        "                                 indicate expected sub-sections.\n",
        "        path (str): The current nested path, used for generating precise\n",
        "                    error messages. Should not be set by the user.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a key from the schema is missing in the configuration.\n",
        "        TypeError: If the configuration object is not a dictionary.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure the config object is a dictionary.\n",
        "    if not isinstance(config, Mapping):\n",
        "        # Raise a TypeError if the object at the current path is not a dict-like\n",
        "        # object where a nested structure is expected.\n",
        "        raise TypeError(f\"Configuration error at path '{path}': Expected a dictionary, but found type {type(config)}.\")\n",
        "\n",
        "    # Iterate through all keys defined in the schema for the current level.\n",
        "    for key in schema:\n",
        "        # Construct the full path for the current key for clear error reporting.\n",
        "        current_path = f\"{path}.{key}\" if path else key\n",
        "\n",
        "        # Check if the key exists in the configuration dictionary.\n",
        "        if key not in config:\n",
        "            # If a required key is missing, raise a KeyError with the exact path.\n",
        "            raise KeyError(f\"Missing required configuration key at path: '{current_path}'\")\n",
        "\n",
        "        # If the schema expects a nested dictionary for this key, recurse.\n",
        "        if isinstance(schema[key], Mapping):\n",
        "            # Recursively call the validation function for the nested sub-dictionary.\n",
        "            _validate_config_structure(config[key], schema[key], path=current_path)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate numerical parameter ranges and types.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_numerical_params(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the types and ranges of critical numerical parameters within the\n",
        "    configuration dictionary.\n",
        "\n",
        "    This function performs specific checks on key parameters to ensure they are\n",
        "    within sensible and theoretically sound bounds. It raises a ValueError or\n",
        "    TypeError if any parameter fails validation.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a numerical parameter is outside its allowed range.\n",
        "        TypeError: If a parameter has an incorrect data type.\n",
        "    \"\"\"\n",
        "    # A list to aggregate all validation errors found.\n",
        "    errors = []\n",
        "\n",
        "    # Define a validation map: path -> (validation_function, error_message)\n",
        "    # This declarative approach makes it easy to add or modify checks.\n",
        "    validation_map = {\n",
        "        (\"descriptive_model\", \"lppl_fitting\", \"rolling_window_size\"):\n",
        "            (lambda x: isinstance(x, int) and 100 <= x <= 500, \"must be an integer between 100 and 500.\"),\n",
        "        (\"descriptive_model\", \"lppl_fitting\", \"parameter_constraints\", \"m\", \"min\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x, \"for 'm' min bound must be a float > 0.\"),\n",
        "        (\"descriptive_model\", \"lppl_fitting\", \"parameter_constraints\", \"m\", \"max\"):\n",
        "            (lambda x: isinstance(x, float) and x < 1, \"for 'm' max bound must be a float < 1.\"),\n",
        "        (\"descriptive_model\", \"lppl_fitting\", \"parameter_constraints\", \"omega\", \"min\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"for 'omega' min bound must be a float > 0.\"),\n",
        "        (\"descriptive_model\", \"bubble_score_synthesis\", \"alpha_1_hype_weight\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"must be a positive float.\"),\n",
        "        (\"descriptive_model\", \"bubble_score_synthesis\", \"alpha_2_sentiment_weight\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"must be a positive float.\"),\n",
        "        (\"descriptive_model\", \"episode_labeling\", \"significance_threshold_tau\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"descriptive_model\", \"episode_labeling\", \"min_duration_d_min\"):\n",
        "            (lambda x: isinstance(x, int) and x >= 1, \"must be an integer >= 1.\"),\n",
        "        (\"predictive_model\", \"data_preparation\", \"sequence_length\"):\n",
        "            (lambda x: isinstance(x, int) and 20 <= x <= 252, \"must be an integer between 20 and 252.\"),\n",
        "        (\"predictive_model\", \"training\", \"dropout_rate\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"predictive_model\", \"optimizer\", \"learning_rate\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"must be a positive float.\"),\n",
        "        (\"predictive_model\", \"optimizer\", \"weight_decay\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"must be a positive float.\"),\n",
        "        (\"predictive_model\", \"optimizer\", \"gradient_clipping_threshold\"):\n",
        "            (lambda x: isinstance(x, float) and x > 0, \"must be a positive float.\"),\n",
        "        (\"backtesting\", \"strategy_rules\", \"entry_threshold_theta_1\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"backtesting\", \"strategy_rules\", \"exit_threshold_theta_2\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"backtesting\", \"risk_management\", \"stop_loss_percentage\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"backtesting\", \"risk_management\", \"max_position_size_percentage\"):\n",
        "            (lambda x: isinstance(x, float) and 0 < x < 1, \"must be a float between 0 and 1.\"),\n",
        "        (\"backtesting\", \"market_assumptions\", \"transaction_cost_per_trade\"):\n",
        "            (lambda x: isinstance(x, float) and 0 <= x <= 0.01, \"must be a float between 0 and 0.01.\"),\n",
        "        (\"backtesting\", \"market_assumptions\", \"risk_free_rate_annual\"):\n",
        "            (lambda x: isinstance(x, float) and 0 <= x <= 0.10, \"must be a float between 0 and 0.10.\"),\n",
        "    }\n",
        "\n",
        "    # Iterate through the validation map to perform checks.\n",
        "    for path_tuple, (validator, msg) in validation_map.items():\n",
        "        try:\n",
        "            # Traverse the dictionary to get the value.\n",
        "            value = config\n",
        "            for key in path_tuple:\n",
        "                value = value[key]\n",
        "            # Apply the validator function.\n",
        "            if not validator(value):\n",
        "                # If validation fails, append a detailed error message.\n",
        "                errors.append(f\"Parameter '{'.'.join(path_tuple)}' (value: {value}) is invalid: {msg}\")\n",
        "        except KeyError:\n",
        "            # This should be caught by the structure validator, but is here for safety.\n",
        "            errors.append(f\"Parameter '{'.'.join(path_tuple)}' is missing.\")\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected errors during validation.\n",
        "            errors.append(f\"Error validating parameter '{'.'.join(path_tuple)}': {e}\")\n",
        "\n",
        "    # Additional, more complex relational checks.\n",
        "    try:\n",
        "        # Check m_min < m_max for LPPL constraints.\n",
        "        m_min = config[\"descriptive_model\"][\"lppl_fitting\"][\"parameter_constraints\"][\"m\"][\"min\"]\n",
        "        m_max = config[\"descriptive_model\"][\"lppl_fitting\"][\"parameter_constraints\"][\"m\"][\"max\"]\n",
        "        if not m_min < m_max:\n",
        "            errors.append(\"LPPL constraint error: 'm' min bound must be less than max bound.\")\n",
        "\n",
        "        # Check theta_2 < theta_1 for backtesting thresholds.\n",
        "        theta_1 = config[\"backtesting\"][\"strategy_rules\"][\"entry_threshold_theta_1\"]\n",
        "        theta_2 = config[\"backtesting\"][\"strategy_rules\"][\"exit_threshold_theta_2\"]\n",
        "        if not theta_2 < theta_1:\n",
        "            errors.append(\"Backtesting threshold error: 'exit_threshold_theta_2' must be less than 'entry_threshold_theta_1'.\")\n",
        "\n",
        "        # Check all loss function weights are non-negative.\n",
        "        loss_weights = config[\"predictive_model\"][\"training\"][\"loss_function_weights\"]\n",
        "        for name, weight in loss_weights.items():\n",
        "            if not (isinstance(weight, (int, float)) and weight >= 0):\n",
        "                errors.append(f\"Loss weight '{name}' must be a non-negative number.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing key for relational check: {e}\")\n",
        "\n",
        "    # If any errors were collected, raise a single, comprehensive ValueError.\n",
        "    if errors:\n",
        "        raise ValueError(\"Configuration validation failed with the following errors:\\n\" + \"\\n\".join(errors))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate string-based model identifiers and create a\n",
        "#                 configuration snapshot.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _make_json_serializable(obj: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Recursively converts non-JSON-serializable types (like numpy types) in a\n",
        "    nested structure to their Python native equivalents.\n",
        "\n",
        "    Args:\n",
        "        obj (Any): The object (e.g., dict, list, value) to process.\n",
        "\n",
        "    Returns:\n",
        "        Any: A version of the object with all values converted to be\n",
        "             JSON-serializable.\n",
        "    \"\"\"\n",
        "    # If the object is a dictionary, recurse on its values.\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: _make_json_serializable(v) for k, v in obj.items()}\n",
        "    # If the object is a list or tuple, recurse on its elements.\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return [_make_json_serializable(i) for i in obj]\n",
        "    # Convert numpy infinity to a string representation.\n",
        "    elif obj == np.inf:\n",
        "        return \"Infinity\"\n",
        "    elif obj == -np.inf:\n",
        "        return \"-Infinity\"\n",
        "    # Convert other numpy number types to Python native types.\n",
        "    elif isinstance(obj, np.generic):\n",
        "        return obj.item()\n",
        "    # Return the object as is if it's already serializable.\n",
        "    return obj\n",
        "\n",
        "\n",
        "def _validate_identifiers_and_snapshot(\n",
        "    config: Dict[str, Any],\n",
        "    log_dir: Union[str, Path] = \"logs\"\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Validates string-based identifiers and saves a timestamped JSON snapshot\n",
        "    of the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The validated configuration dictionary.\n",
        "        log_dir (Union[str, Path]): The directory to save the snapshot in.\n",
        "                                    Defaults to \"logs\".\n",
        "\n",
        "    Returns:\n",
        "        Path: The path to the saved configuration snapshot file.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a string identifier does not match its expected value.\n",
        "    \"\"\"\n",
        "    # A list to aggregate all validation errors found.\n",
        "    errors = []\n",
        "\n",
        "    # Define a map for exact string value checks.\n",
        "    identifier_map = {\n",
        "        (\"nlp_settings\", \"sentiment_model\", \"huggingface_model_name\"): \"ProsusAI/finbert\",\n",
        "        (\"nlp_settings\", \"topic_model\", \"embedding_model\"): \"all-MiniLM-L6-v2\",\n",
        "        (\"predictive_model\", \"optimizer\", \"scheduler\"): \"OneCycleLR\",\n",
        "    }\n",
        "\n",
        "    # Perform the string identifier checks.\n",
        "    for path_tuple, expected_value in identifier_map.items():\n",
        "        try:\n",
        "            # Traverse the dictionary to get the value.\n",
        "            value = config\n",
        "            for key in path_tuple:\n",
        "                value = value[key]\n",
        "            # Compare the actual value with the expected value.\n",
        "            if value != expected_value:\n",
        "                errors.append(f\"Identifier '{'.'.join(path_tuple)}' is incorrect. Expected '{expected_value}', found '{value}'.\")\n",
        "        except KeyError:\n",
        "            errors.append(f\"Identifier '{'.'.join(path_tuple)}' is missing.\")\n",
        "\n",
        "    # If any errors were found, raise a comprehensive ValueError.\n",
        "    if errors:\n",
        "        raise ValueError(\"Identifier validation failed:\\n\" + \"\\n\".join(errors))\n",
        "\n",
        "    # --- Create Configuration Snapshot ---\n",
        "    # Ensure the log directory exists.\n",
        "    log_path = Path(log_dir)\n",
        "    log_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Generate a timestamp for the filename.\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    snapshot_filename = f\"config_snapshot_{timestamp}.json\"\n",
        "    snapshot_filepath = log_path / snapshot_filename\n",
        "\n",
        "    # Prepare the configuration for JSON serialization.\n",
        "    serializable_config = _make_json_serializable(config)\n",
        "\n",
        "    # Write the snapshot to a JSON file with indentation for readability.\n",
        "    try:\n",
        "        with open(snapshot_filepath, 'w') as f:\n",
        "            json.dump(serializable_config, f, indent=4)\n",
        "    except (IOError, TypeError) as e:\n",
        "        # Handle potential file writing or serialization errors.\n",
        "        logging.error(f\"Failed to write configuration snapshot to {snapshot_filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Return the path to the created snapshot file.\n",
        "    return snapshot_filepath\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_and_parse_config(\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the study configuration dictionary.\n",
        "\n",
        "    This function serves as the entry point for configuration validation. It\n",
        "    sequentially performs three critical validation steps:\n",
        "    1.  Structural Validation: Ensures all required keys and nested sections\n",
        "        are present.\n",
        "    2.  Numerical Validation: Checks that key numerical parameters are within\n",
        "        their valid and sensible ranges.\n",
        "    3.  Identifier Validation & Snapshotting: Verifies specific string\n",
        "        identifiers (e.g., model names) and creates a timestamped JSON\n",
        "        snapshot of the configuration for reproducibility.\n",
        "\n",
        "    The function operates on a \"fail-fast\" basis, raising a specific,\n",
        "    informative exception upon the first validation failure.\n",
        "\n",
        "    Args:\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary\n",
        "                                           for the entire study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The original, validated study_parameters dictionary,\n",
        "                        returned if all checks pass.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the configuration is structurally incomplete.\n",
        "        ValueError: If any numerical or string parameter is invalid.\n",
        "        TypeError: If a part of the configuration has an incorrect type.\n",
        "    \"\"\"\n",
        "    # Log the start of the validation process.\n",
        "    logging.info(\"Initiating validation of the study configuration dictionary...\")\n",
        "\n",
        "    # Define the expected schema for structural validation.\n",
        "    # An empty dict `{}` signifies a section with parameters to be validated later.\n",
        "    schema = {\n",
        "        \"descriptive_model\": {\n",
        "            \"lppl_fitting\": {},\n",
        "            \"bubble_score_synthesis\": {},\n",
        "            \"episode_labeling\": {}\n",
        "        },\n",
        "        \"predictive_model\": {\n",
        "            \"data_preparation\": {},\n",
        "            \"architecture\": {},\n",
        "            \"training\": {\"loss_function_weights\": {}},\n",
        "            \"optimizer\": {},\n",
        "            \"early_stopping\": {}\n",
        "        },\n",
        "        \"nlp_settings\": {\n",
        "            \"sentiment_model\": {},\n",
        "            \"topic_model\": {}\n",
        "        },\n",
        "        \"backtesting\": {\n",
        "            \"strategy_rules\": {},\n",
        "            \"risk_management\": {},\n",
        "            \"market_assumptions\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Validate the overall structure of the dictionary. ---\n",
        "    # This ensures all required sections and subsections are present before\n",
        "    # checking their contents.\n",
        "    _validate_config_structure(study_parameters, schema)\n",
        "    logging.info(\"Step 1/3: Configuration structure is complete and valid.\")\n",
        "\n",
        "    # --- Step 2: Validate numerical parameter types and ranges. ---\n",
        "    # This checks the actual values of key parameters for correctness.\n",
        "    _validate_numerical_params(study_parameters)\n",
        "    logging.info(\"Step 2/3: Numerical parameters are within specified ranges.\")\n",
        "\n",
        "    # --- Step 3: Validate string identifiers and create a snapshot. ---\n",
        "    # This verifies model names and creates a reproducible record of the config.\n",
        "    snapshot_path = _validate_identifiers_and_snapshot(study_parameters)\n",
        "    logging.info(f\"Step 3/3: String identifiers are correct. Configuration snapshot saved to '{snapshot_path}'.\")\n",
        "\n",
        "    # Log the successful completion of all validation steps.\n",
        "    logging.info(\"Configuration validation successful.\")\n",
        "\n",
        "    # Return the validated configuration dictionary for use in the pipeline.\n",
        "    return study_parameters\n"
      ],
      "metadata": {
        "id": "zUGyG_7ZpuGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate the input DataFrame structure and schema\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate the input DataFrame structure and schema\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1:  Validate the MultiIndex structure.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_index(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates the MultiIndex structure of the input DataFrame.\n",
        "\n",
        "    This function ensures the DataFrame's index is a two-level MultiIndex\n",
        "    named ('Date', 'TICKER') with correct dtypes (datetime64[ns], object)\n",
        "    and is monotonically increasing. If the index is not sorted, it sorts\n",
        "    the DataFrame and returns a sorted copy.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A validated and sorted copy of the input DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the index is not a pandas MultiIndex.\n",
        "        ValueError: If the index does not meet the structural, naming, or\n",
        "                    dtype requirements.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df_validated = df.copy()\n",
        "\n",
        "    # --- MultiIndex Type Check ---\n",
        "    # Verify that the index is indeed a MultiIndex.\n",
        "    if not isinstance(df_validated.index, pd.MultiIndex):\n",
        "        raise TypeError(\"Input DataFrame index is not a pandas MultiIndex.\")\n",
        "\n",
        "    # --- MultiIndex Level Count Check ---\n",
        "    # Verify that the MultiIndex has exactly two levels.\n",
        "    if df_validated.index.nlevels != 2:\n",
        "        raise ValueError(f\"Index must have 2 levels, but found {df_validated.index.nlevels}.\")\n",
        "\n",
        "    # --- MultiIndex Level Names Check ---\n",
        "    # Verify the names of the index levels are 'Date' and 'TICKER' in order.\n",
        "    expected_names = ['Date', 'TICKER']\n",
        "    if list(df_validated.index.names) != expected_names:\n",
        "        raise ValueError(f\"Index names must be {expected_names}, but found {list(df_validated.index.names)}.\")\n",
        "\n",
        "    # --- MultiIndex Level Dtypes Check ---\n",
        "    # Verify the dtype of the first level ('Date') is datetime.\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_validated.index.levels[0]):\n",
        "        raise ValueError(f\"Index level 'Date' must be datetime64, but found {df_validated.index.levels[0].dtype}.\")\n",
        "\n",
        "    # Verify the dtype of the second level ('TICKER') is string/object.\n",
        "    if not pd.api.types.is_string_dtype(df_validated.index.levels[1]):\n",
        "        raise ValueError(f\"Index level 'TICKER' must be object/string, but found {df_validated.index.levels[1].dtype}.\")\n",
        "\n",
        "    # --- Index Sorting Check ---\n",
        "    # Verify that the index is sorted for efficient time-series operations.\n",
        "    if not df_validated.index.is_monotonic_increasing:\n",
        "        # If not sorted, log a warning and sort it. This can be a costly operation.\n",
        "        logging.warning(\"DataFrame index is not sorted. Sorting index... For better performance, provide pre-sorted data.\")\n",
        "        # Sort the index in place on the copied DataFrame.\n",
        "        df_validated.sort_index(inplace=True)\n",
        "\n",
        "    # Return the validated and sorted DataFrame copy.\n",
        "    return df_validated\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate required columns and their data types.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_columns(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the presence and data types of required columns in the DataFrame.\n",
        "\n",
        "    This function checks against a predefined schema for column names and\n",
        "    their expected dtypes. It performs a special validation on the\n",
        "    'News_Articles' column by sampling to ensure its contents are lists of\n",
        "    strings.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a validated index.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If columns are missing or have incorrect dtypes.\n",
        "        TypeError: If the 'News_Articles' column contains invalid data types.\n",
        "    \"\"\"\n",
        "    # Define the required schema: column name -> expected dtype.\n",
        "    REQUIRED_SCHEMA = {\n",
        "        \"PERMNO\": \"int64\",\n",
        "        \"SIC_Code\": \"int64\",\n",
        "        \"Close_Price_Raw\": \"float64\",\n",
        "        \"Volume_Raw\": \"int64\", # Target dtype, may be float if NaNs exist\n",
        "        \"CFACSHR\": \"float64\",\n",
        "        \"PE_Ratio\": \"float64\",\n",
        "        \"PB_Ratio\": \"float64\",\n",
        "        \"VIX_Close\": \"float64\",\n",
        "        \"News_Articles\": \"object\",\n",
        "    }\n",
        "\n",
        "    # --- Column Presence Check ---\n",
        "    # Get the set of actual columns and required columns.\n",
        "    actual_columns = set(df.columns)\n",
        "    required_columns = set(REQUIRED_SCHEMA.keys())\n",
        "\n",
        "    # Check if all required columns are present in the DataFrame.\n",
        "    if not required_columns.issubset(actual_columns):\n",
        "        # Identify and report missing columns.\n",
        "        missing_cols = required_columns - actual_columns\n",
        "        raise ValueError(f\"DataFrame is missing required columns: {sorted(list(missing_cols))}\")\n",
        "\n",
        "    # --- Column Dtype Check ---\n",
        "    # Iterate through the schema to validate each column's dtype.\n",
        "    for col, expected_dtype in REQUIRED_SCHEMA.items():\n",
        "        actual_dtype = str(df[col].dtype)\n",
        "\n",
        "        # Special handling for Volume_Raw, which might be float due to NaNs.\n",
        "        if col == \"Volume_Raw\" and actual_dtype == 'float64':\n",
        "            # If it's float but contains no NaNs, it's a problem. It should be int.\n",
        "            if df[col].isnull().sum() == 0:\n",
        "                 raise TypeError(f\"Column '{col}' is float64 but contains no NaNs. It should be int64.\")\n",
        "            # If it contains NaNs, we accept float64 for now; Task 3 will handle cleansing.\n",
        "            continue\n",
        "\n",
        "        # For all other columns, perform a strict dtype check.\n",
        "        if actual_dtype != expected_dtype:\n",
        "            raise TypeError(f\"Column '{col}' has incorrect dtype. Expected '{expected_dtype}', found '{actual_dtype}'.\")\n",
        "\n",
        "    # --- Special Validation for 'News_Articles' Column ---\n",
        "    # Sample up to 100 non-null rows for efficient validation.\n",
        "    news_col_non_null = df['News_Articles'].dropna()\n",
        "    sample_size = min(100, len(news_col_non_null))\n",
        "    if sample_size > 0:\n",
        "        # Use a fixed random_state for reproducible sampling.\n",
        "        sample = news_col_non_null.sample(n=sample_size, random_state=42)\n",
        "        # Iterate through the sampled cells to check their content.\n",
        "        for item in sample:\n",
        "            # Each cell must be a list.\n",
        "            if not isinstance(item, list):\n",
        "                raise TypeError(f\"Column 'News_Articles' contains a non-list element of type {type(item)}.\")\n",
        "            # All elements within a non-empty list must be strings.\n",
        "            if item and not all(isinstance(article, str) for article in item):\n",
        "                raise TypeError(\"A list in 'News_Articles' contains non-string elements.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate temporal and cross-sectional coverage.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_coverage(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the temporal and cross-sectional coverage of the DataFrame.\n",
        "\n",
        "    This function checks if the data spans the required study period and\n",
        "    contains a minimum number of trading days and unique tickers for a\n",
        "    meaningful analysis.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with validated index and columns.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the data coverage does not meet the minimum requirements.\n",
        "    \"\"\"\n",
        "    # --- Temporal Coverage Validation ---\n",
        "    # Extract unique dates from the index.\n",
        "    unique_dates = df.index.get_level_values('Date').unique()\n",
        "\n",
        "    # Define the required start and end dates for the study.\n",
        "    required_start_date = pd.Timestamp('2018-01-01')\n",
        "    required_end_date = pd.Timestamp('2024-12-31')\n",
        "\n",
        "    # Check if the data's date range covers the required study period.\n",
        "    if unique_dates.min() > required_start_date or unique_dates.max() < required_end_date:\n",
        "        raise ValueError(f\"Data does not cover the required study period from {required_start_date.date()} to {required_end_date.date()}. \"\n",
        "                         f\"Actual range: {unique_dates.min().date()} to {unique_dates.max().date()}.\")\n",
        "\n",
        "    # Check for the minimum number of trading days.\n",
        "    n_days = len(unique_dates)\n",
        "    min_days = 1700\n",
        "    if n_days < min_days:\n",
        "        raise ValueError(f\"Insufficient trading days. Found {n_days}, but require at least {min_days}.\")\n",
        "\n",
        "    # --- Cross-Sectional Coverage Validation ---\n",
        "    # Extract unique tickers from the index.\n",
        "    unique_tickers = df.index.get_level_values('TICKER').unique()\n",
        "\n",
        "    # Check for the minimum number of unique tickers.\n",
        "    n_tickers = len(unique_tickers)\n",
        "    min_tickers = 10\n",
        "    if n_tickers < min_tickers:\n",
        "        raise ValueError(f\"Insufficient unique tickers. Found {n_tickers}, but require at least {min_tickers}.\")\n",
        "\n",
        "    # Log summary statistics upon successful validation.\n",
        "    logging.info(f\"DataFrame coverage validated: {len(df)} rows, {n_days} trading days, {n_tickers} unique tickers.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_input_dataframe(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the input DataFrame's schema.\n",
        "\n",
        "    This function serves as the entry point for data validation. It executes\n",
        "    a sequence of checks to ensure the input DataFrame is correctly structured\n",
        "    and has adequate data coverage for the study. The steps are:\n",
        "    1.  Index Validation: Verifies the MultiIndex structure, names, dtypes,\n",
        "        and sortedness. Returns a sorted copy if necessary.\n",
        "    2.  Column Validation: Checks for the presence and correct dtypes of all\n",
        "        required columns.\n",
        "    3.  Coverage Validation: Ensures the data spans the required time period\n",
        "        and includes a minimum number of assets and trading days.\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame): The raw input DataFrame for the study.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A validated, sorted copy of the input DataFrame, ready\n",
        "                      for the next processing step.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame or if dtypes are\n",
        "                   incorrect.\n",
        "        ValueError: If the structure, columns, or data coverage are invalid.\n",
        "    \"\"\"\n",
        "    # --- Input Type Check ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(df_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    # Log the start of the validation process.\n",
        "    logging.info(\"Initiating validation of the input DataFrame...\")\n",
        "\n",
        "    # --- Step 1: Validate the DataFrame's MultiIndex. ---\n",
        "    # This step returns a sorted copy of the DataFrame.\n",
        "    df_validated = _validate_dataframe_index(df_raw)\n",
        "    logging.info(\"Step 1/3: DataFrame index structure is valid and sorted.\")\n",
        "\n",
        "    # --- Step 2: Validate the required columns and their dtypes. ---\n",
        "    # This step operates on the validated (and possibly sorted) DataFrame.\n",
        "    _validate_dataframe_columns(df_validated)\n",
        "    logging.info(\"Step 2/3: DataFrame columns and dtypes are valid.\")\n",
        "\n",
        "    # --- Step 3: Validate the temporal and cross-sectional coverage. ---\n",
        "    _validate_dataframe_coverage(df_validated)\n",
        "    logging.info(\"Step 3/3: DataFrame temporal and cross-sectional coverage is sufficient.\")\n",
        "\n",
        "    # Log the successful completion of all validation steps.\n",
        "    logging.info(\"DataFrame validation successful.\")\n",
        "\n",
        "    # Return the fully validated and sorted DataFrame.\n",
        "    return df_validated\n"
      ],
      "metadata": {
        "id": "YeKOWFiAq6Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Cleanse the raw data for missing values, outliers, and inconsistencies\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse the raw data for missing values, outliers, and\n",
        "#         inconsistencies\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Handle missing values in price and volume fields.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _cleanse_price_volume_fields(\n",
        "    df: pd.DataFrame,\n",
        "    volume_ffill_threshold: float = 0.01\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleanses missing and invalid data in price and volume columns.\n",
        "\n",
        "    This function performs the following actions:\n",
        "    1. Removes rows with missing or non-positive 'Close_Price_Raw'.\n",
        "    2. Forward-fills missing 'Volume_Raw' within each ticker group if the\n",
        "       proportion of missing data is below a specified threshold.\n",
        "    3. Asserts that the 'CFACSHR' column has no missing values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        volume_ffill_threshold (float): The maximum proportion of missing\n",
        "                                        'Volume_Raw' data to allow for\n",
        "                                        forward-filling. Defaults to 0.01 (1%).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed copy of the input DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If 'CFACSHR' contains missing values.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects.\n",
        "    df_cleansed = df.copy()\n",
        "    initial_rows = len(df_cleansed)\n",
        "\n",
        "    # --- Cleanse 'Close_Price_Raw' ---\n",
        "    # Identify rows with missing or non-positive closing prices.\n",
        "    invalid_price_mask = df_cleansed['Close_Price_Raw'].isna() | (df_cleansed['Close_Price_Raw'] <= 0)\n",
        "    num_invalid_prices = invalid_price_mask.sum()\n",
        "\n",
        "    # If invalid prices are found, log and remove them.\n",
        "    if num_invalid_prices > 0:\n",
        "        logging.info(f\"Found {num_invalid_prices} rows ({num_invalid_prices / initial_rows:.2%}) with missing or non-positive 'Close_Price_Raw'. Removing them.\")\n",
        "        # Keep only the rows that do not match the invalid mask.\n",
        "        df_cleansed = df_cleansed[~invalid_price_mask]\n",
        "\n",
        "    # --- Cleanse 'Volume_Raw' ---\n",
        "    # Calculate the number and proportion of missing volume data.\n",
        "    missing_volume_count = df_cleansed['Volume_Raw'].isna().sum()\n",
        "    if missing_volume_count > 0:\n",
        "        missing_volume_pct = missing_volume_count / len(df_cleansed)\n",
        "        logging.info(f\"Found {missing_volume_count} missing 'Volume_Raw' values ({missing_volume_pct:.2%}).\")\n",
        "\n",
        "        # Conditionally forward-fill based on the threshold.\n",
        "        if missing_volume_pct < volume_ffill_threshold:\n",
        "            logging.info(f\"Missing volume percentage is below threshold of {volume_ffill_threshold:.2%}. Forward-filling within each ticker group.\")\n",
        "            # Group by ticker to prevent filling across different securities.\n",
        "            df_cleansed['Volume_Raw'] = df_cleansed.groupby(level='TICKER')['Volume_Raw'].ffill()\n",
        "            # Log remaining NaNs, which could exist at the start of a series.\n",
        "            remaining_nans = df_cleansed['Volume_Raw'].isna().sum()\n",
        "            if remaining_nans > 0:\n",
        "                logging.warning(f\"{remaining_nans} 'Volume_Raw' NaNs remain at the beginning of time series after forward-filling.\")\n",
        "        else:\n",
        "            # If above threshold, retain NaNs and warn the user.\n",
        "            logging.warning(f\"Missing volume percentage exceeds threshold. Retaining NaNs in 'Volume_Raw' for later treatment.\")\n",
        "\n",
        "    # --- Validate 'CFACSHR' ---\n",
        "    # This is a critical field for price adjustments; it cannot have missing data.\n",
        "    if df_cleansed['CFACSHR'].isna().any():\n",
        "        raise ValueError(\"'CFACSHR' column contains missing values, which is not permissible for price/volume adjustments.\")\n",
        "\n",
        "    # Log the final count of rows removed.\n",
        "    final_rows = len(df_cleansed)\n",
        "    logging.info(f\"Price/volume cleansing complete. Total rows removed: {initial_rows - final_rows}.\")\n",
        "\n",
        "    return df_cleansed\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Handle missing values in fundamental and macro fields.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _cleanse_fundamental_macro_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleanses or validates missing data in fundamental and macro columns.\n",
        "\n",
        "    This function applies distinct rules:\n",
        "    1. 'PE_Ratio', 'PB_Ratio': Missing values are permitted and counted.\n",
        "    2. 'VIX_Close': Missing values are not permitted and are forward-filled.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed copy of the input DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If 'VIX_Close' has a missing value at the very start of\n",
        "                    the series that cannot be forward-filled.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_cleansed = df.copy()\n",
        "\n",
        "    # --- Handle Fundamental Ratios ('PE_Ratio', 'PB_Ratio') ---\n",
        "    # These are allowed to be NaN. We just log their presence.\n",
        "    for col in ['PE_Ratio', 'PB_Ratio']:\n",
        "        missing_count = df_cleansed[col].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            missing_pct = missing_count / len(df_cleansed)\n",
        "            logging.info(f\"Column '{col}' contains {missing_count} NaNs ({missing_pct:.2%}). These are permitted and will be retained.\")\n",
        "\n",
        "    # --- Handle Macro Indicator ('VIX_Close') ---\n",
        "    # VIX should be complete. We forward-fill any gaps.\n",
        "    if df_cleansed['VIX_Close'].isna().any():\n",
        "        # Check for the critical edge case: NaN at the beginning of the series.\n",
        "        if pd.isna(df_cleansed['VIX_Close'].iloc[0]):\n",
        "            raise ValueError(\"Missing 'VIX_Close' value at the start of the dataset. Cannot forward-fill.\")\n",
        "\n",
        "        # Log and perform the forward-fill.\n",
        "        logging.info(\"Found missing values in 'VIX_Close'. Forward-filling...\")\n",
        "        df_cleansed['VIX_Close'].ffill(inplace=True)\n",
        "\n",
        "        # Final assertion to guarantee completeness.\n",
        "        if df_cleansed['VIX_Close'].isna().any():\n",
        "             # This should not be reached if the initial check passes, but is a safeguard.\n",
        "             raise RuntimeError(\"Forward-filling 'VIX_Close' failed unexpectedly.\")\n",
        "\n",
        "    return df_cleansed\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Detect and handle outliers and data quality issues.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _filter_and_screen_data(\n",
        "    df: pd.DataFrame,\n",
        "    target_sic_codes: Set[int],\n",
        "    return_outlier_threshold: float = 0.5\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to the target universe and screens for outliers.\n",
        "\n",
        "    This function performs two main actions:\n",
        "    1. Calculates 1-day log returns on raw prices to detect extreme single-day\n",
        "       movements, logging them as potential data errors.\n",
        "    2. Filters the DataFrame to include only stocks belonging to a specified\n",
        "       set of SIC codes (the real estate sector).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        target_sic_codes (Set[int]): A set of SIC codes for the target universe.\n",
        "        return_outlier_threshold (float): The absolute log return value to\n",
        "                                          flag as an outlier. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered copy of the DataFrame containing only the\n",
        "                      target universe.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If filtering by SIC code results in an empty DataFrame.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_screened = df.copy()\n",
        "\n",
        "    # --- Outlier Detection based on Log Returns ---\n",
        "    # Calculate log returns on raw prices within each ticker group.\n",
        "    # Equation: r_t = ln(P_t / P_{t-1})\n",
        "    log_returns = np.log(\n",
        "        df_screened.groupby(level='TICKER')['Close_Price_Raw'].pct_change() + 1\n",
        "    )\n",
        "\n",
        "    # Identify outliers where the absolute log return exceeds the threshold.\n",
        "    outlier_mask = log_returns.abs() > return_outlier_threshold\n",
        "    num_outliers = outlier_mask.sum()\n",
        "\n",
        "    # If outliers are found, log them for manual review.\n",
        "    if num_outliers > 0:\n",
        "        logging.warning(f\"Found {num_outliers} potential outliers with absolute 1-day log return > {return_outlier_threshold:.0%}.\")\n",
        "        # Log the details of the first few outliers for quick inspection.\n",
        "        outlier_details = df_screened[outlier_mask].copy()\n",
        "        outlier_details['Log_Return'] = log_returns[outlier_mask]\n",
        "        logging.warning(\"Outlier examples:\\n\" + outlier_details[['Log_Return']].head().to_string())\n",
        "        # Note: No automatic removal/winsorization is performed to preserve data integrity.\n",
        "\n",
        "    # --- Filter by Target SIC Codes ---\n",
        "    initial_rows = len(df_screened)\n",
        "    logging.info(f\"Filtering DataFrame to target SIC codes: {target_sic_codes}.\")\n",
        "    # Create a boolean mask for rows with a target SIC code.\n",
        "    sic_filter_mask = df_screened['SIC_Code'].isin(target_sic_codes)\n",
        "    # Apply the filter.\n",
        "    df_screened = df_screened[sic_filter_mask]\n",
        "    final_rows = len(df_screened)\n",
        "\n",
        "    # Check if the filtering resulted in an empty DataFrame.\n",
        "    if df_screened.empty:\n",
        "        raise ValueError(\"Filtering by SIC codes resulted in an empty DataFrame. Check input data and target SIC codes.\")\n",
        "\n",
        "    # Log the result of the filtering operation.\n",
        "    logging.info(f\"Retained {final_rows} rows ({final_rows / initial_rows:.2%}) after SIC code filtering.\")\n",
        "\n",
        "    return df_screened\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_raw_data(\n",
        "    df_validated: pd.DataFrame,\n",
        "    target_sic_codes: Set[int] = {6500, 6512, 6513, 6519, 6531, 6541, 6552, 6798}\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete data cleansing pipeline.\n",
        "\n",
        "    This function applies a series of cleansing and filtering steps to the\n",
        "    validated input DataFrame to prepare it for feature engineering. The\n",
        "    pipeline includes:\n",
        "    1.  Handling missing values in core price and volume fields.\n",
        "    2.  Handling missing values in fundamental and macroeconomic indicators.\n",
        "    3.  Screening for extreme return outliers and filtering the dataset to the\n",
        "        specified target universe based on SIC codes.\n",
        "\n",
        "    Args:\n",
        "        df_validated (pd.DataFrame): The DataFrame that has passed the schema\n",
        "                                     validation checks from Task 2.\n",
        "        target_sic_codes (Set[int]): A set of SIC codes defining the study's\n",
        "                                     investment universe. Defaults to the real\n",
        "                                     estate sector codes from the paper.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed and filtered DataFrame ready for the next\n",
        "                      stage of processing (corporate action adjustments).\n",
        "    \"\"\"\n",
        "    # Log the start of the cleansing process.\n",
        "    logging.info(\"Initiating data cleansing and filtering pipeline...\")\n",
        "\n",
        "    # --- Step 1: Cleanse price and volume fields. ---\n",
        "    df_step1 = _cleanse_price_volume_fields(df_validated)\n",
        "    logging.info(\"Step 1/3: Cleansing of price and volume fields complete.\")\n",
        "\n",
        "    # --- Step 2: Cleanse fundamental and macro fields. ---\n",
        "    df_step2 = _cleanse_fundamental_macro_fields(df_step1)\n",
        "    logging.info(\"Step 2/3: Cleansing of fundamental and macro fields complete.\")\n",
        "\n",
        "    # --- Step 3: Filter to target universe and screen for outliers. ---\n",
        "    df_final = _filter_and_screen_data(df_step2, target_sic_codes)\n",
        "    logging.info(\"Step 3/3: Outlier screening and SIC code filtering complete.\")\n",
        "\n",
        "    # Log the successful completion of the cleansing pipeline.\n",
        "    logging.info(\"Data cleansing pipeline finished successfully.\")\n",
        "\n",
        "    # Return the fully cleansed and filtered DataFrame.\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "vtLjTusssMML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Adjust prices and volumes for corporate actions using CFACSHR\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Compute split- and dividend-adjusted prices.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_adjusted_prices(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes split- and dividend-adjusted closing prices.\n",
        "\n",
        "    This function applies the Cumulative Factor to Adjust Shares (CFACSHR) from\n",
        "    CRSP to the raw closing prices to create a continuous, comparable time\n",
        "    series. It performs rigorous pre- and post-computation validation.\n",
        "\n",
        "    Equation: Close_Price_Adj = Close_Price_Raw * CFACSHR\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The cleansed DataFrame containing 'Close_Price_Raw'\n",
        "                           and 'CFACSHR' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new\n",
        "                      'Close_Price_Adj' column.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If 'CFACSHR' contains non-positive values.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid modifying the original DataFrame.\n",
        "    df_adj = df.copy()\n",
        "\n",
        "    # --- Pre-computation Validation ---\n",
        "    # The adjustment factor must be strictly positive.\n",
        "    if not (df_adj['CFACSHR'] > 0).all():\n",
        "        raise ValueError(\"'CFACSHR' column contains non-positive values, which is invalid for price adjustment.\")\n",
        "\n",
        "    # --- Price Adjustment Calculation ---\n",
        "    # Apply the vectorized multiplication to compute the adjusted price.\n",
        "    df_adj['Close_Price_Adj'] = df_adj['Close_Price_Raw'] * df_adj['CFACSHR']\n",
        "\n",
        "    # --- Post-computation Validation ---\n",
        "    # Adjusted prices, like raw prices, must be positive.\n",
        "    if not (df_adj['Close_Price_Adj'] > 0).all():\n",
        "        # This would indicate an issue with raw prices that was missed in cleansing.\n",
        "        logging.warning(\"Post-adjustment check found non-positive adjusted prices. Review raw price and CFACSHR data.\")\n",
        "\n",
        "    return df_adj\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Compute adjusted volumes.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_adjusted_volumes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes split- and dividend-adjusted trading volumes.\n",
        "\n",
        "    This function adjusts the raw trading volume by dividing by the CFACSHR.\n",
        "    This is the inverse operation to the price adjustment, ensuring that the\n",
        "    total value traded (price * volume) remains consistent before and after\n",
        "    adjustment.\n",
        "\n",
        "    Equation: Volume_Adj = Volume_Raw / CFACSHR\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame from the price adjustment step.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new\n",
        "                      'Volume_Adj' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_adj = df.copy()\n",
        "\n",
        "    # --- Volume Adjustment Calculation ---\n",
        "    # Apply the vectorized division to compute the adjusted volume.\n",
        "    # Since we already validated CFACSHR > 0, division by zero is not a risk.\n",
        "    df_adj['Volume_Adj'] = df_adj['Volume_Raw'] / df_adj['CFACSHR']\n",
        "\n",
        "    # --- Post-computation Validation ---\n",
        "    # Adjusted volume must be a non-negative quantity.\n",
        "    if not (df_adj['Volume_Adj'] >= 0).all():\n",
        "        logging.warning(\"Post-adjustment check found negative adjusted volumes. Review raw volume and CFACSHR data.\")\n",
        "\n",
        "    return df_adj\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Verify adjustment consistency and persistence.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _verify_adjustments_and_finalize(\n",
        "    df: pd.DataFrame,\n",
        "    num_tickers_to_spot_check: int = 5,\n",
        "    random_state: int = 42\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a programmatic spot-check of the adjustments and removes the\n",
        "    CFACSHR column.\n",
        "\n",
        "    The verification logic identifies corporate action dates (where CFACSHR\n",
        "    changes) for a sample of tickers. It checks if the adjusted return on\n",
        "    these dates is significantly smaller in magnitude than the raw return,\n",
        "    which indicates a successful adjustment. Finally, it drops the CFACSHR\n",
        "    column to prevent data leakage.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with both raw and adjusted columns.\n",
        "        num_tickers_to_spot_check (int): The number of random tickers to verify.\n",
        "        random_state (int): Seed for the random sampler for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final adjusted DataFrame with the 'CFACSHR' column\n",
        "                      removed.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_final = df.copy()\n",
        "\n",
        "    # --- Programmatic Spot-Check ---\n",
        "    # Get a list of all unique tickers in the DataFrame.\n",
        "    all_tickers = df_final.index.get_level_values('TICKER').unique()\n",
        "\n",
        "    # Ensure the number of tickers to check is not more than available tickers.\n",
        "    num_to_check = min(num_tickers_to_spot_check, len(all_tickers))\n",
        "\n",
        "    # Select a random sample of tickers for verification.\n",
        "    if num_to_check > 0:\n",
        "        spot_check_tickers = np.random.RandomState(random_state).choice(\n",
        "            all_tickers, size=num_to_check, replace=False\n",
        "        )\n",
        "        logging.info(f\"Performing programmatic spot-check on {num_to_check} tickers: {list(spot_check_tickers)}\")\n",
        "\n",
        "        # Group by ticker to perform time-series operations.\n",
        "        grouped = df_final.groupby(level='TICKER')\n",
        "\n",
        "        for ticker in spot_check_tickers:\n",
        "            # Get the data for the specific ticker.\n",
        "            ticker_df = grouped.get_group(ticker)\n",
        "\n",
        "            # Identify corporate action dates by finding where CFACSHR changes.\n",
        "            cfacshr_change = ticker_df['CFACSHR'].diff().abs() > 1e-8 # Use tolerance for float comparison\n",
        "            event_dates = cfacshr_change[cfacshr_change].index\n",
        "\n",
        "            if not event_dates.empty:\n",
        "                # Calculate raw and adjusted returns for the entire series.\n",
        "                raw_returns = ticker_df['Close_Price_Raw'].pct_change()\n",
        "                adj_returns = ticker_df['Close_Price_Adj'].pct_change()\n",
        "\n",
        "                # Check the returns on the identified event dates.\n",
        "                raw_event_returns = raw_returns.loc[event_dates].abs()\n",
        "                adj_event_returns = adj_returns.loc[event_dates].abs()\n",
        "\n",
        "                # A successful adjustment should make the adjusted return much smaller than the raw return.\n",
        "                # Heuristic: Adjusted return magnitude should be less than 1/5th of raw return magnitude.\n",
        "                if (adj_event_returns > 0.2 * raw_event_returns).any():\n",
        "                    logging.warning(f\"Ticker {ticker}: Adjustment consistency check failed. \"\n",
        "                                    f\"Adjusted return is not significantly smaller than raw return on a corporate action date. \"\n",
        "                                    f\"Manual review recommended.\")\n",
        "\n",
        "    # --- Finalize by Dropping the CFACSHR Column ---\n",
        "    # This is a critical step to prevent data leakage in downstream models.\n",
        "    df_final.drop(columns=['CFACSHR'], inplace=True)\n",
        "    logging.info(\"'CFACSHR' column dropped to prevent data leakage.\")\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def adjust_for_corporate_actions(df_cleansed: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full pipeline for adjusting price and volume data.\n",
        "\n",
        "    This function takes a cleansed DataFrame and applies corporate action\n",
        "    adjustments using the CRSP CFACSHR factor. The process includes:\n",
        "    1.  Computing adjusted prices.\n",
        "    2.  Computing adjusted volumes.\n",
        "    3.  Performing a programmatic spot-check to verify the consistency of the\n",
        "        adjustments.\n",
        "    4.  Dropping the adjustment factor column ('CFACSHR') to finalize the\n",
        "        dataset for the next stage.\n",
        "\n",
        "    Args:\n",
        "        df_cleansed (pd.DataFrame): The DataFrame that has passed the cleansing\n",
        "                                    and filtering steps from Task 3.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with new 'Close_Price_Adj' and 'Volume_Adj'\n",
        "                      columns, and the 'CFACSHR' column removed.\n",
        "    \"\"\"\n",
        "    # Log the start of the adjustment process.\n",
        "    logging.info(\"Initiating corporate action adjustment pipeline...\")\n",
        "\n",
        "    # --- Step 1: Compute split- and dividend-adjusted prices. ---\n",
        "    df_adj_price = _compute_adjusted_prices(df_cleansed)\n",
        "    logging.info(\"Step 1/3: Adjusted prices computed successfully.\")\n",
        "\n",
        "    # --- Step 2: Compute adjusted volumes. ---\n",
        "    df_adj_volume = _compute_adjusted_volumes(df_adj_price)\n",
        "    logging.info(\"Step 2/3: Adjusted volumes computed successfully.\")\n",
        "\n",
        "    # --- Step 3: Verify adjustments and finalize the DataFrame. ---\n",
        "    df_final = _verify_adjustments_and_finalize(df_adj_volume)\n",
        "    logging.info(\"Step 3/3: Adjustments verified and 'CFACSHR' column removed.\")\n",
        "\n",
        "    # Log the successful completion of the pipeline.\n",
        "    logging.info(\"Corporate action adjustment pipeline finished successfully.\")\n",
        "\n",
        "    return df_final\n",
        "\n"
      ],
      "metadata": {
        "id": "POLY0QHWtOLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Derive engineered features from adjusted prices and volumes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Derive engineered features from adjusted prices and volumes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Compute log-transformed price and volume series.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_log_transforms(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the natural logarithm of adjusted prices and volumes.\n",
        "\n",
        "    Logarithmic transformation is a standard technique in finance to stabilize\n",
        "    variance and convert multiplicative relationships into additive ones.\n",
        "\n",
        "    Equations:\n",
        "    1. Log_Price = ln(Close_Price_Adj)\n",
        "    2. Log_Volume = ln(Volume_Adj + 1)\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with adjusted price and volume columns.\n",
        "                           Must contain 'Close_Price_Adj' and 'Volume_Adj'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with new 'Log_Price' and\n",
        "                      'Log_Volume' columns.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If 'Close_Price_Adj' contains non-positive values.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects on the original object.\n",
        "    df_featured = df.copy()\n",
        "\n",
        "    # --- Pre-computation Validation ---\n",
        "    # The argument to the logarithm for price must be strictly positive.\n",
        "    if not (df_featured['Close_Price_Adj'] > 0).all():\n",
        "        raise ValueError(\"Cannot compute log price: 'Close_Price_Adj' column contains non-positive values.\")\n",
        "\n",
        "    # --- Log Price Calculation ---\n",
        "    # Equation: Log_Price = ln(Close_Price_Adj)\n",
        "    # Compute the natural logarithm of the adjusted closing price.\n",
        "    df_featured['Log_Price'] = np.log(df_featured['Close_Price_Adj'])\n",
        "\n",
        "    # --- Log Volume Calculation ---\n",
        "    # Equation: Log_Volume = ln(Volume_Adj + 1)\n",
        "    # Add 1 to volume before taking the log to handle zero-volume days gracefully.\n",
        "    # This avoids log(0) = -inf and ensures the result is non-negative.\n",
        "    df_featured['Log_Volume'] = np.log(df_featured['Volume_Adj'] + 1)\n",
        "\n",
        "    return df_featured\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Compute daily log returns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_log_returns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes daily logarithmic returns from the log-transformed price series.\n",
        "\n",
        "    Log returns are the first difference of the log price series and are a\n",
        "    cornerstone of quantitative financial analysis due to their desirable\n",
        "    statistical properties (e.g., time-additivity).\n",
        "\n",
        "    Equation: r_t = Log_Price_t - Log_Price_{t-1}\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'Log_Price' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new 'Log_Return'\n",
        "                      column. The first entry for each ticker will be NaN.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_featured = df.copy()\n",
        "\n",
        "    # --- Log Return Calculation ---\n",
        "    # Group by ticker to ensure returns are calculated only within each security's\n",
        "    # time series. This is critical to prevent data leakage across tickers.\n",
        "    # The .diff() method calculates the difference from the previous row in the group.\n",
        "    df_featured['Log_Return'] = df_featured.groupby(level='TICKER')['Log_Price'].diff()\n",
        "\n",
        "    # --- Post-computation Logging ---\n",
        "    # Log the number of NaNs created, which should equal the number of unique tickers.\n",
        "    num_tickers = df_featured.index.get_level_values('TICKER').nunique()\n",
        "    num_nans = df_featured['Log_Return'].isna().sum()\n",
        "    if num_nans == num_tickers:\n",
        "        logging.info(f\"Successfully computed log returns. {num_nans} NaNs created for the first observation of each ticker, as expected.\")\n",
        "    else:\n",
        "        logging.warning(f\"Log return calculation resulted in {num_nans} NaNs, while there are {num_tickers} tickers. Review for potential data gaps.\")\n",
        "\n",
        "    return df_featured\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Extract calendar features from the Date index.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _extract_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts month and day-of-month features from the DataFrame's Date index.\n",
        "\n",
        "    These features can capture potential seasonal or calendar-based anomalies\n",
        "    in market behavior.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with a datetime index level named 'Date'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with new 'Month' and 'Day'\n",
        "                      columns.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_featured = df.copy()\n",
        "\n",
        "    # --- Feature Extraction ---\n",
        "    # Access the 'Date' level of the MultiIndex.\n",
        "    date_index = df_featured.index.get_level_values('Date')\n",
        "\n",
        "    # Extract the month (integer 1-12) using the .month accessor.\n",
        "    df_featured['Month'] = date_index.month\n",
        "\n",
        "    # Extract the day of the month (integer 1-31) using the .day accessor.\n",
        "    df_featured['Day'] = date_index.day\n",
        "\n",
        "    return df_featured\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def derive_engineered_features(df_adjusted: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the derivation of primary engineered features from adjusted data.\n",
        "\n",
        "    This function takes a DataFrame with adjusted prices and volumes and creates\n",
        "    several key features required for the downstream LPPL model and the\n",
        "    Transformer model. The pipeline includes:\n",
        "    1.  Computing log-transformed price and volume series.\n",
        "    2.  Calculating daily log returns.\n",
        "    3.  Extracting calendar features (month and day) from the index.\n",
        "\n",
        "    Args:\n",
        "        df_adjusted (pd.DataFrame): The DataFrame that has passed the corporate\n",
        "                                    action adjustment steps from Task 4.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame enriched with the new engineered features:\n",
        "                      'Log_Price', 'Log_Volume', 'Log_Return', 'Month', 'Day'.\n",
        "    \"\"\"\n",
        "    # Log the start of the feature engineering process.\n",
        "    logging.info(\"Initiating derivation of engineered features...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure required adjusted columns are present.\n",
        "    required_cols = ['Close_Price_Adj', 'Volume_Adj']\n",
        "    if not all(col in df_adjusted.columns for col in required_cols):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Compute log-transformed price and volume series. ---\n",
        "    df_step1 = _compute_log_transforms(df_adjusted)\n",
        "    logging.info(\"Step 1/3: Log-transformed price and volume features computed.\")\n",
        "\n",
        "    # --- Step 2: Compute daily log returns. ---\n",
        "    df_step2 = _compute_log_returns(df_step1)\n",
        "    logging.info(\"Step 2/3: Daily log returns computed.\")\n",
        "\n",
        "    # --- Step 3: Extract calendar features. ---\n",
        "    df_final = _extract_calendar_features(df_step2)\n",
        "    logging.info(\"Step 3/3: Calendar features (Month, Day) extracted.\")\n",
        "\n",
        "    # Log the successful completion of the pipeline.\n",
        "    logging.info(\"Engineered feature derivation pipeline finished successfully.\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "4zxyTIO4vt3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Align and validate the temporal calendar across all tickers\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Construct the master trading calendar.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_and_validate_master_calendar(\n",
        "    df: pd.DataFrame,\n",
        "    max_gap_days: int = 10\n",
        ") -> pd.DatetimeIndex:\n",
        "    \"\"\"\n",
        "    Constructs and validates a master trading calendar from the DataFrame index.\n",
        "\n",
        "    This function extracts all unique dates from the DataFrame's index, sorts\n",
        "    them, and performs validation checks for monotonicity and large gaps, which\n",
        "    might indicate data outages.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a 'Date' level in its index.\n",
        "        max_gap_days (int): The maximum number of calendar days allowed between\n",
        "                            consecutive dates before logging a warning.\n",
        "\n",
        "    Returns:\n",
        "        pd.DatetimeIndex: A sorted, unique DatetimeIndex of all trading days.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the constructed calendar is not monotonic.\n",
        "    \"\"\"\n",
        "    # --- Calendar Construction ---\n",
        "    # Extract all unique dates from the 'Date' index level.\n",
        "    master_calendar = df.index.get_level_values('Date').unique()\n",
        "    # Sort the dates to ensure chronological order.\n",
        "    master_calendar = master_calendar.sort_values()\n",
        "\n",
        "    # --- Monotonicity Validation ---\n",
        "    # A trading calendar must be strictly increasing.\n",
        "    if not master_calendar.is_monotonic_increasing:\n",
        "        raise ValueError(\"Master calendar is not monotonically increasing. Check for duplicate or out-of-order dates.\")\n",
        "\n",
        "    # --- Gap Analysis ---\n",
        "    # Calculate the difference in calendar days between consecutive trading days.\n",
        "    gaps = np.diff(master_calendar).astype('timedelta64[D]').astype(int)\n",
        "\n",
        "    # Check if the median gap is reasonable (e.g., <= 3 for weekends/holidays).\n",
        "    if pd.Series(gaps).median() > 3:\n",
        "        logging.warning(f\"Median gap between trading days is {pd.Series(gaps).median()} days, which is unusually high.\")\n",
        "\n",
        "    # Check for any single gap larger than the specified maximum.\n",
        "    if (gaps > max_gap_days).any():\n",
        "        # Find the locations of large gaps to provide informative warnings.\n",
        "        large_gap_indices = np.where(gaps > max_gap_days)[0]\n",
        "        for idx in large_gap_indices:\n",
        "            gap_start = master_calendar[idx].date()\n",
        "            gap_end = master_calendar[idx + 1].date()\n",
        "            logging.warning(f\"Large data gap detected: {gaps[idx]} calendar days between {gap_start} and {gap_end}.\")\n",
        "\n",
        "    return master_calendar\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Verify cross-sectional completeness (balanced vs unbalanced).\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _verify_cross_sectional_completeness(\n",
        "    df: pd.DataFrame,\n",
        "    min_obs_per_ticker: int = 100\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Analyzes the DataFrame to assess its cross-sectional completeness.\n",
        "\n",
        "    This function performs two checks:\n",
        "    1. Assesses if the panel is balanced or unbalanced by analyzing the number\n",
        "       of tickers present each day.\n",
        "    2. Identifies and logs tickers with very few observations.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        min_obs_per_ticker (int): The minimum number of observations a ticker\n",
        "                                  should have. Tickers below this are flagged.\n",
        "    \"\"\"\n",
        "    # --- Panel Balance Assessment ---\n",
        "    # Count the number of tickers with data for each day.\n",
        "    daily_ticker_counts = df.groupby(level='Date').size()\n",
        "    mean_count = daily_ticker_counts.mean()\n",
        "    std_count = daily_ticker_counts.std()\n",
        "\n",
        "    # Check if the standard deviation is large relative to the mean.\n",
        "    if std_count > 0.3 * mean_count:\n",
        "        logging.warning(f\"Unbalanced panel detected. Daily ticker count has mean={mean_count:.2f} and std={std_count:.2f}. \"\n",
        "                        \"Missing data will be handled per-ticker.\")\n",
        "    else:\n",
        "        logging.info(f\"Panel is relatively balanced. Daily ticker count: mean={mean_count:.2f}, std={std_count:.2f}.\")\n",
        "\n",
        "    # --- Per-Ticker Observation Count ---\n",
        "    # Count the total number of observations for each ticker.\n",
        "    obs_per_ticker = df.groupby(level='TICKER').size()\n",
        "    # Identify tickers with fewer observations than the minimum threshold.\n",
        "    sparse_tickers = obs_per_ticker[obs_per_ticker < min_obs_per_ticker]\n",
        "\n",
        "    if not sparse_tickers.empty:\n",
        "        logging.warning(f\"Found {len(sparse_tickers)} tickers with fewer than {min_obs_per_ticker} observations. \"\n",
        "                        f\"These may be unsuitable for models requiring long lookbacks. \"\n",
        "                        f\"Example sparse tickers: {sparse_tickers.head(5).index.tolist()}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Forward-fill market-wide features consistently.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _enforce_market_feature_consistency(\n",
        "    df: pd.DataFrame,\n",
        "    master_calendar: pd.DatetimeIndex\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensures market-wide features are consistent and complete across all tickers.\n",
        "\n",
        "    This function takes a market-wide feature (e.g., 'VIX_Close'), creates a\n",
        "    single authoritative time series for it on the master calendar, and then\n",
        "    maps this series back to the entire DataFrame. This corrects any potential\n",
        "    inconsistencies where the same feature might have different values for\n",
        "    different tickers on the same day.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        master_calendar (pd.DatetimeIndex): The master list of all trading days.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with the 'VIX_Close' column\n",
        "                      made consistent and complete.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_consistent = df.copy()\n",
        "    market_feature = 'VIX_Close'\n",
        "\n",
        "    # --- Create Authoritative Time Series ---\n",
        "    # Extract the series for the first ticker as a reference.\n",
        "    first_ticker = df_consistent.index.get_level_values('TICKER')[0]\n",
        "    # Use .droplevel to get a simple Series indexed by Date.\n",
        "    authoritative_series = df_consistent.loc[(slice(None), first_ticker), market_feature].droplevel('TICKER')\n",
        "\n",
        "    # Reindex to the master calendar to fill any missing dates with NaN.\n",
        "    # Then forward-fill to propagate the last known value.\n",
        "    complete_series = authoritative_series.reindex(master_calendar).ffill()\n",
        "\n",
        "    # --- Final Validation of the Complete Series ---\n",
        "    if complete_series.isna().any():\n",
        "        raise ValueError(f\"Could not create a complete series for '{market_feature}'. NaNs remain after reindexing and forward-filling, likely due to missing data at the start.\")\n",
        "\n",
        "    # --- Map Back to the Full DataFrame ---\n",
        "    # Create a mapping from the Date index to the complete VIX values.\n",
        "    date_to_vix_map = complete_series.to_dict()\n",
        "    # Use the .map() function on the Date index level for an efficient update.\n",
        "    df_consistent[market_feature] = df_consistent.index.get_level_values('Date').map(date_to_vix_map)\n",
        "\n",
        "    # --- Final Assertion ---\n",
        "    if df_consistent[market_feature].isna().any():\n",
        "        raise RuntimeError(f\"Enforcing consistency for '{market_feature}' failed unexpectedly. NaNs are still present.\")\n",
        "\n",
        "    return df_consistent\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def align_and_validate_calendar(df_featured: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation and alignment of the dataset's temporal structure.\n",
        "\n",
        "    This function ensures the time dimension of the panel data is coherent,\n",
        "    complete, and consistent before proceeding to modeling. The pipeline includes:\n",
        "    1.  Constructing a master trading calendar and checking for gaps.\n",
        "    2.  Assessing the cross-sectional completeness (panel balance).\n",
        "    3.  Enforcing consistency and completeness of market-wide features like VIX.\n",
        "\n",
        "    Args:\n",
        "        df_featured (pd.DataFrame): The DataFrame with engineered features from\n",
        "                                    Task 5.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a validated temporal structure and\n",
        "                      consistent market-wide features.\n",
        "    \"\"\"\n",
        "    # Log the start of the calendar validation process.\n",
        "    logging.info(\"Initiating temporal calendar alignment and validation...\")\n",
        "\n",
        "    # --- Step 1: Construct and validate the master trading calendar. ---\n",
        "    master_calendar = _construct_and_validate_master_calendar(df_featured)\n",
        "    logging.info(f\"Step 1/3: Master calendar constructed, containing {len(master_calendar)} unique trading days.\")\n",
        "\n",
        "    # --- Step 2: Verify cross-sectional completeness. ---\n",
        "    _verify_cross_sectional_completeness(df_featured)\n",
        "    logging.info(\"Step 2/3: Cross-sectional completeness verified.\")\n",
        "\n",
        "    # --- Step 3: Enforce consistency for market-wide features. ---\n",
        "    df_final = _enforce_market_feature_consistency(df_featured, master_calendar)\n",
        "    logging.info(\"Step 3/3: Consistency of market-wide features enforced.\")\n",
        "\n",
        "    # Log the successful completion of the pipeline.\n",
        "    logging.info(\"Temporal calendar alignment and validation finished successfully.\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "--nuK3FgxHVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Set up BERTopic and generate sentence embeddings for the news corpus\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Set up BERTopic and generate sentence embeddings for the news corpus\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Extract and deduplicate the full news corpus.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _extract_and_deduplicate_corpus(df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extracts all news articles from the DataFrame, normalizes them, and\n",
        "    returns a list of unique articles.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'News_Articles' column,\n",
        "                           where each cell is a list of strings.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A deduplicated list of all news articles.\n",
        "    \"\"\"\n",
        "    # --- Corpus Extraction ---\n",
        "    # Use dropna() to skip any rows that might have nulls in this column.\n",
        "    # chain.from_iterable is a highly efficient way to flatten a list of lists.\n",
        "    logging.info(\"Extracting all news articles from the DataFrame...\")\n",
        "    corpus_raw = list(chain.from_iterable(df['News_Articles'].dropna()))\n",
        "    logging.info(f\"Extracted a total of {len(corpus_raw):,} articles.\")\n",
        "\n",
        "    # --- Normalization and Deduplication ---\n",
        "    # Normalize by stripping whitespace and converting to lowercase to ensure\n",
        "    # that semantically identical articles are treated as unique.\n",
        "    # Using a set is the most efficient method for deduplication.\n",
        "    logging.info(\"Normalizing and deduplicating corpus...\")\n",
        "    unique_articles_set = {article.strip().lower() for article in corpus_raw if article.strip()}\n",
        "    corpus_unique = list(unique_articles_set)\n",
        "    logging.info(f\"Found {len(corpus_unique):,} unique articles after deduplication.\")\n",
        "\n",
        "    return corpus_unique\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Load the sentence-transformer model and generate embeddings.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_corpus_embeddings(\n",
        "    corpus: List[str],\n",
        "    config: Dict[str, Any],\n",
        "    output_path: Path,\n",
        "    batch_size: int = 64\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates sentence embeddings for a corpus of documents using a pre-trained\n",
        "    transformer model.\n",
        "\n",
        "    This is a computationally expensive step. The function is designed to be\n",
        "    idempotent by checking if the output file already exists.\n",
        "\n",
        "    Args:\n",
        "        corpus (List[str]): The list of unique documents to embed.\n",
        "        config (Dict[str, Any]): The study configuration dictionary, containing\n",
        "                                 the model name.\n",
        "        output_path (Path): The file path to save the embeddings to.\n",
        "        batch_size (int): The batch size to use for encoding, for memory\n",
        "                          efficiency.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D numpy array of shape (n_articles, embedding_dim).\n",
        "    \"\"\"\n",
        "    # --- Idempotency Check ---\n",
        "    # If the embeddings file already exists, load and return it to avoid re-computation.\n",
        "    if output_path.exists():\n",
        "        logging.info(f\"Found existing embeddings at '{output_path}'. Loading from file.\")\n",
        "        return np.load(output_path)\n",
        "\n",
        "    # --- Model Loading ---\n",
        "    # Determine the device to use (GPU if available, otherwise CPU).\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model_name = config['nlp_settings']['topic_model']['embedding_model']\n",
        "    logging.info(f\"Loading sentence-transformer model '{model_name}' onto device '{device}'.\")\n",
        "    # Load the specified pre-trained model.\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # --- Embedding Generation ---\n",
        "    logging.info(f\"Generating embeddings for {len(corpus):,} articles... (This may take a while)\")\n",
        "    # The .encode() method handles tokenization, inference, and pooling.\n",
        "    # Using a progress bar provides crucial feedback for this long-running task.\n",
        "    embeddings = model.encode(\n",
        "        corpus,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=batch_size,\n",
        "        normalize_embeddings=True # Often improves clustering performance\n",
        "    )\n",
        "\n",
        "    # --- Persistence ---\n",
        "    # Ensure the parent directory for the output file exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the numpy array to the specified path.\n",
        "    logging.info(f\"Saving embeddings to '{output_path}'.\")\n",
        "    np.save(output_path, embeddings)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Initialize BERTopic with the specified hyperparameters.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _fit_bertopic_model(\n",
        "    corpus: List[str],\n",
        "    embeddings: np.ndarray,\n",
        "    config: Dict[str, Any],\n",
        "    output_path: Path\n",
        ") -> BERTopic:\n",
        "    \"\"\"\n",
        "    Initializes and fits a BERTopic model using pre-computed embeddings.\n",
        "\n",
        "    This function configures the underlying UMAP and HDBSCAN models with\n",
        "    parameters from the configuration to ensure reproducibility. The fitted\n",
        "    model is saved to disk to avoid re-fitting.\n",
        "\n",
        "    Args:\n",
        "        corpus (List[str]): The list of unique documents.\n",
        "        embeddings (np.ndarray): The pre-computed embeddings for the corpus.\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "        output_path (Path): The file path to save the fitted BERTopic model.\n",
        "\n",
        "    Returns:\n",
        "        BERTopic: The fitted BERTopic model instance.\n",
        "    \"\"\"\n",
        "    # --- Idempotency Check ---\n",
        "    if output_path.exists():\n",
        "        logging.info(f\"Found existing BERTopic model at '{output_path}'. Loading from file.\")\n",
        "        return BERTopic.load(output_path)\n",
        "\n",
        "    # --- Component Configuration ---\n",
        "    # Extract hyperparameters from the configuration.\n",
        "    topic_config = config['nlp_settings']['topic_model']\n",
        "    umap_params = {'n_neighbors': topic_config['umap_n_neighbors'], 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'random_state': 42}\n",
        "    hdbscan_params = {'min_cluster_size': topic_config['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom'}\n",
        "\n",
        "    # Instantiate the components with the specified parameters for reproducibility.\n",
        "    umap_model = UMAP(**umap_params)\n",
        "    hdbscan_model = HDBSCAN(**hdbscan_params)\n",
        "\n",
        "    # --- BERTopic Initialization ---\n",
        "    # Initialize BERTopic with the custom components.\n",
        "    logging.info(\"Initializing BERTopic model with custom UMAP and HDBSCAN parameters.\")\n",
        "    topic_model = BERTopic(\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # --- Model Fitting ---\n",
        "    logging.info(f\"Fitting BERTopic model on {len(corpus):,} documents...\")\n",
        "    # Fit the model using the pre-computed embeddings.\n",
        "    # This is more efficient and ensures consistency with the embedding step.\n",
        "    topics, _ = topic_model.fit_transform(corpus, embeddings)\n",
        "\n",
        "    # --- Persistence ---\n",
        "    # Ensure the parent directory exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the entire fitted model object.\n",
        "    logging.info(f\"Saving fitted BERTopic model to '{output_path}'.\")\n",
        "    topic_model.save(output_path)\n",
        "\n",
        "    # Log a summary of the topic modeling results.\n",
        "    num_topics = len(topic_model.get_topic_info())\n",
        "    logging.info(f\"BERTopic fitting complete. Found {num_topics} topics.\")\n",
        "\n",
        "    return topic_model\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def setup_topic_model(\n",
        "    df_aligned: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    intermediate_data_dir: Union[str, Path] = \"data_intermediate\",\n",
        "    model_dir: Union[str, Path] = \"models\"\n",
        ") -> Tuple[List[str], np.ndarray, BERTopic]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end process of setting up the BERTopic model.\n",
        "\n",
        "    This function prepares the textual data for thematic analysis by:\n",
        "    1.  Extracting and deduplicating all news articles into a clean corpus.\n",
        "    2.  Generating high-quality sentence embeddings for the corpus.\n",
        "    3.  Initializing and fitting a BERTopic model with specified hyperparameters.\n",
        "\n",
        "    The function is designed to be idempotent, leveraging saved intermediate\n",
        "    artifacts (embeddings, fitted model) to avoid expensive re-computation.\n",
        "\n",
        "    Args:\n",
        "        df_aligned (pd.DataFrame): The DataFrame from Task 6 with a validated\n",
        "                                   temporal structure.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        intermediate_data_dir (Union[str, Path]): Directory to save/load\n",
        "                                                   intermediate data like embeddings.\n",
        "        model_dir (Union[str, Path]): Directory to save/load the fitted model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], np.ndarray, BERTopic]: A tuple containing:\n",
        "            - The unique, normalized corpus of articles.\n",
        "            - The corresponding sentence embeddings.\n",
        "            - The fitted BERTopic model instance.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating NLP setup for topic modeling...\")\n",
        "\n",
        "    # Define paths for intermediate artifacts.\n",
        "    data_path = Path(intermediate_data_dir)\n",
        "    model_path = Path(model_dir)\n",
        "    embeddings_filepath = data_path / \"corpus_embeddings.npy\"\n",
        "    bertopic_model_filepath = model_path / \"bertopic_model.pkl\"\n",
        "\n",
        "    # --- Step 1: Extract and deduplicate the news corpus. ---\n",
        "    corpus = _extract_and_deduplicate_corpus(df_aligned)\n",
        "    logging.info(\"Step 1/3: News corpus extracted and deduplicated successfully.\")\n",
        "\n",
        "    # --- Step 2: Generate sentence embeddings for the corpus. ---\n",
        "    embeddings = _generate_corpus_embeddings(corpus, study_parameters, embeddings_filepath)\n",
        "    logging.info(\"Step 2/3: Sentence embeddings generated successfully.\")\n",
        "\n",
        "    # --- Step 3: Initialize and fit the BERTopic model. ---\n",
        "    topic_model = _fit_bertopic_model(corpus, embeddings, study_parameters, bertopic_model_filepath)\n",
        "    logging.info(\"Step 3/3: BERTopic model fitted successfully.\")\n",
        "\n",
        "    logging.info(\"NLP setup for topic modeling finished successfully.\")\n",
        "\n",
        "    return corpus, embeddings, topic_model\n"
      ],
      "metadata": {
        "id": "Yco4jaQJyCLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Apply BERTopic clustering and filter to real-estate-relevant articles\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Apply BERTopic clustering and filter to real-estate-relevant articles\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Extract topic assignments and representations.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _extract_topic_assignments(\n",
        "    corpus: List[str],\n",
        "    topic_model: BERTopic\n",
        ") -> Tuple[Dict[str, int], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Extracts topic assignments for each document and retrieves topic representations.\n",
        "\n",
        "    Args:\n",
        "        corpus (List[str]): The unique, normalized corpus of articles that the\n",
        "                            model was fitted on.\n",
        "        topic_model (BERTopic): The fitted BERTopic model instance.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, int], pd.DataFrame]: A tuple containing:\n",
        "            - A dictionary mapping each article text to its assigned topic ID.\n",
        "            - A DataFrame with information about each topic (keywords, count).\n",
        "    \"\"\"\n",
        "    # --- Get Topic Information ---\n",
        "    # get_topic_info() returns a DataFrame with details for each topic,\n",
        "    # including the default c-TF-IDF keyword representation.\n",
        "    logging.info(\"Extracting topic representations from the BERTopic model.\")\n",
        "    topic_info_df = topic_model.get_topic_info()\n",
        "\n",
        "    # --- Create Article-to-Topic Mapping ---\n",
        "    # The `topic_model.topics_` attribute stores the topic assignment for each\n",
        "    # document in the same order as the input corpus.\n",
        "    if len(corpus) != len(topic_model.topics_):\n",
        "        raise ValueError(\"Mismatch between corpus size and topic assignment count. The model may have been fitted on a different corpus.\")\n",
        "\n",
        "    # Create a dictionary for efficient lookup of an article's topic.\n",
        "    logging.info(\"Creating a map from articles to their assigned topic IDs.\")\n",
        "    article_to_topic_map = dict(zip(corpus, topic_model.topics_))\n",
        "\n",
        "    return article_to_topic_map, topic_info_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Identify real-estate-relevant topics via keyword matching.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _identify_relevant_topics(\n",
        "    topic_info_df: pd.DataFrame,\n",
        "    keywords: Set[str],\n",
        "    metadata_path: Path\n",
        ") -> Set[int]:\n",
        "    \"\"\"\n",
        "    Identifies relevant topics based on a set of keywords and saves metadata.\n",
        "\n",
        "    This function implements a deterministic, keyword-based heuristic to flag\n",
        "    topics related to the real estate sector. It creates an audit trail by\n",
        "    saving the selected topics and their keywords to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        topic_info_df (pd.DataFrame): DataFrame containing topic information.\n",
        "        keywords (Set[str]): A set of keywords to identify relevant topics.\n",
        "        metadata_path (Path): Path to save the topic filter metadata JSON file.\n",
        "\n",
        "    Returns:\n",
        "        Set[int]: A set of topic IDs identified as relevant.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Identifying real estate-related topics using keywords: {keywords}\")\n",
        "    relevant_topic_ids = set()\n",
        "    # This dictionary will store the metadata for the audit trail.\n",
        "    selection_metadata = {}\n",
        "\n",
        "    # Iterate over each topic returned by the model.\n",
        "    # The outlier topic (-1) is always ignored.\n",
        "    for _, row in topic_info_df[topic_info_df.Topic != -1].iterrows():\n",
        "        topic_id = row['Topic']\n",
        "        # 'Representation' column contains the top keywords for the topic.\n",
        "        topic_keywords = row['Representation']\n",
        "\n",
        "        # Check if any of the target keywords appear in the topic's representation.\n",
        "        if any(keyword in topic_keywords for keyword in keywords):\n",
        "            # If a match is found, add the topic ID to our set.\n",
        "            relevant_topic_ids.add(topic_id)\n",
        "            # Record the reason for selection in the metadata.\n",
        "            selection_metadata[topic_id] = {\n",
        "                \"keywords\": topic_keywords,\n",
        "                \"reason\": \"Matched one or more target keywords.\"\n",
        "            }\n",
        "\n",
        "    logging.info(f\"Identified {len(relevant_topic_ids)} relevant topics: {sorted(list(relevant_topic_ids))}\")\n",
        "\n",
        "    # --- Persistence of Metadata ---\n",
        "    # Save the selection metadata for reproducibility and review.\n",
        "    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with metadata_path.open('w') as f:\n",
        "        json.dump(selection_metadata, f, indent=4)\n",
        "    logging.info(f\"Topic selection metadata saved to '{metadata_path}'.\")\n",
        "\n",
        "    return relevant_topic_ids\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Filter the corpus and update the DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _filter_dataframe_articles(\n",
        "    df: pd.DataFrame,\n",
        "    article_to_topic_map: Dict[str, int],\n",
        "    relevant_topic_ids: Set[int]\n",
        ") -> Tuple[pd.DataFrame, Set[str]]:\n",
        "    \"\"\"\n",
        "    Filters the 'News_Articles' column in the DataFrame to retain only\n",
        "    articles belonging to relevant topics.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be filtered.\n",
        "        article_to_topic_map (Dict[str, int]): Mapping from article text to topic ID.\n",
        "        relevant_topic_ids (Set[int]): Set of topic IDs to keep.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Set[str]]: A tuple containing:\n",
        "            - A copy of the DataFrame with the 'News_Articles' lists filtered.\n",
        "            - A set containing the texts of all articles that were kept.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects.\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # --- Build the Set of Relevant Articles ---\n",
        "    # This is more efficient than checking topics for every article in every row.\n",
        "    logging.info(\"Building the set of all relevant articles for efficient filtering.\")\n",
        "    corpus_filtered = {\n",
        "        article for article, topic_id in article_to_topic_map.items()\n",
        "        if topic_id in relevant_topic_ids\n",
        "    }\n",
        "\n",
        "    # --- Define the Filtering Function ---\n",
        "    # This function will be applied to each cell in the 'News_Articles' column.\n",
        "    def filter_article_list(articles: List[str]) -> List[str]:\n",
        "        # Ensure the input is a list before processing.\n",
        "        if not isinstance(articles, list):\n",
        "            return []\n",
        "        # Normalize articles in the same way (strip/lower) and check for membership\n",
        "        # in the pre-computed set of relevant articles.\n",
        "        return [\n",
        "            article for article in articles\n",
        "            if article.strip().lower() in corpus_filtered\n",
        "        ]\n",
        "\n",
        "    # --- Apply the Filter to the DataFrame ---\n",
        "    logging.info(\"Applying topic filter to the 'News_Articles' column in the DataFrame...\")\n",
        "    df_filtered['News_Articles'] = df_filtered['News_Articles'].apply(filter_article_list)\n",
        "\n",
        "    return df_filtered, corpus_filtered\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def apply_topic_filter(\n",
        "    df_aligned: pd.DataFrame,\n",
        "    corpus: List[str],\n",
        "    topic_model: BERTopic,\n",
        "    log_dir: Union[str, Path] = \"logs\"\n",
        ") -> Tuple[pd.DataFrame, Set[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the filtering of news articles based on thematic relevance.\n",
        "\n",
        "    This function uses a fitted BERTopic model to:\n",
        "    1.  Extract topic assignments for all unique articles.\n",
        "    2.  Identify topics relevant to a specific domain (real estate) using a\n",
        "        keyword-based heuristic.\n",
        "    3.  Filter the 'News_Articles' column in the main DataFrame, removing any\n",
        "        articles that do not belong to the identified relevant topics.\n",
        "\n",
        "    Args:\n",
        "        df_aligned (pd.DataFrame): The main DataFrame.\n",
        "        corpus (List[str]): The unique, normalized corpus of articles.\n",
        "        topic_model (BERTopic): The fitted BERTopic model from Task 7.\n",
        "        log_dir (Union[str, Path]): Directory to save the topic filter metadata.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Set[str]]: A tuple containing:\n",
        "            - The DataFrame with its 'News_Articles' column filtered.\n",
        "            - A set of the unique, normalized article texts that were retained.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating article filtering based on topic modeling...\")\n",
        "\n",
        "    # Define the keywords for identifying the real estate domain.\n",
        "    real_estate_keywords = {\n",
        "        \"real estate\", \"property\", \"housing\", \"reit\", \"mortgage\",\n",
        "        \"construction\", \"leasing\", \"landlord\", \"tenant\", \"zoning\"\n",
        "    }\n",
        "    metadata_filepath = Path(log_dir) / \"topic_filter_metadata.json\"\n",
        "\n",
        "    # --- Step 1: Extract topic assignments and representations. ---\n",
        "    article_to_topic_map, topic_info_df = _extract_topic_assignments(corpus, topic_model)\n",
        "    logging.info(\"Step 1/3: Extracted topic assignments and representations.\")\n",
        "\n",
        "    # --- Step 2: Identify real-estate-relevant topics. ---\n",
        "    relevant_topic_ids = _identify_relevant_topics(topic_info_df, real_estate_keywords, metadata_filepath)\n",
        "    logging.info(\"Step 2/3: Identified relevant topics using keyword matching.\")\n",
        "\n",
        "    # --- Step 3: Filter the corpus and update the DataFrame. ---\n",
        "    # Get the total number of articles before filtering for comparison.\n",
        "    total_articles_before = sum(df_aligned['News_Articles'].apply(lambda x: len(x) if isinstance(x, list) else 0))\n",
        "\n",
        "    df_filtered, corpus_retained = _filter_dataframe_articles(df_aligned, article_to_topic_map, relevant_topic_ids)\n",
        "\n",
        "    # Calculate and log retention statistics.\n",
        "    total_articles_after = sum(df_filtered['News_Articles'].apply(lambda x: len(x) if isinstance(x, list) else 0))\n",
        "    retention_pct = (total_articles_after / total_articles_before) * 100 if total_articles_before > 0 else 0\n",
        "    logging.info(f\"Step 3/3: DataFrame's 'News_Articles' column filtered successfully.\")\n",
        "    logging.info(f\"Article retention rate: {total_articles_after:,} / {total_articles_before:,} ({retention_pct:.2f}%).\")\n",
        "\n",
        "    logging.info(\"Article filtering based on topics finished successfully.\")\n",
        "\n",
        "    return df_filtered, corpus_retained\n"
      ],
      "metadata": {
        "id": "HDNWtilBzfHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Apply FinBERT sentiment classification to each filtered article\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Apply FinBERT sentiment classification to each filtered article\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Load the FinBERT model and tokenizer.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def _load_sentiment_pipeline(config: Dict[str, Any]) -> pipeline:\n",
        "    \"\"\"\n",
        "    Loads the pre-trained FinBERT model and tokenizer into a Hugging Face pipeline\n",
        "    with explicit evaluation mode setting for maximum rigor.\n",
        "\n",
        "    This function handles device detection (GPU/CPU), loads the model and\n",
        "    tokenizer, explicitly sets the model to evaluation mode to disable\n",
        "    training-specific layers like dropout, and then initializes the inference\n",
        "    pipeline. This explicit state management is a best practice for\n",
        "    production-grade, deterministic inference.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary, containing\n",
        "                                 the sentiment model name.\n",
        "\n",
        "    Returns:\n",
        "        transformers.pipeline: An initialized text-classification pipeline\n",
        "                               guaranteed to be in evaluation mode.\n",
        "\n",
        "    Raises:\n",
        "        IOError: If the model fails to download from the Hugging Face Hub.\n",
        "        Exception: For other unexpected errors during model loading.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Device Selection ---\n",
        "    # Determine the appropriate device for PyTorch operations. Use CUDA if a\n",
        "    # compatible GPU is available for significant performance gains, otherwise\n",
        "    # default to the CPU. The device_id is used by the pipeline constructor.\n",
        "    device_id = 0 if torch.cuda.is_available() else -1\n",
        "    device = torch.device(\"cuda:0\" if device_id == 0 else \"cpu\")\n",
        "\n",
        "    # --- Step 2: Model and Tokenizer Loading with Explicit Eval Mode ---\n",
        "    # Extract the model identifier from the configuration dictionary.\n",
        "    model_name = config['nlp_settings']['sentiment_model']['huggingface_model_name']\n",
        "    logging.info(f\"Loading sentiment model '{model_name}' and tokenizer for device '{device}'.\")\n",
        "\n",
        "    try:\n",
        "        # Load the tokenizer associated with the pre-trained model.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Load the pre-trained model weights and architecture.\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        # Explicitly set the model to evaluation mode.\n",
        "        # This is the critical step for ensuring deterministic inference by disabling\n",
        "        # layers like Dropout that behave differently during training.\n",
        "        model.eval()\n",
        "\n",
        "        # Move the model to the selected device (GPU or CPU).\n",
        "        model.to(device)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch potential network errors or other issues during download/loading.\n",
        "        logging.error(f\"Failed to load model '{model_name}' from Hugging Face. Check model name and internet connection.\")\n",
        "        # Re-raise the exception to halt execution, as this is a critical failure.\n",
        "        raise IOError(f\"Could not load model '{model_name}'.\") from e\n",
        "\n",
        "    # --- Step 3: Pipeline Initialization ---\n",
        "    # Instantiate the text-classification pipeline, passing the pre-loaded and\n",
        "    # pre-configured model and tokenizer objects. This removes any ambiguity\n",
        "    # about the model's state.\n",
        "    sentiment_pipeline = pipeline(\n",
        "        task=\"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device_id, # The pipeline API uses the integer ID.\n",
        "        framework=\"pt\"\n",
        "    )\n",
        "\n",
        "    logging.info(\"FinBERT sentiment analysis pipeline loaded successfully in evaluation mode.\")\n",
        "\n",
        "    # Return the fully configured pipeline object.\n",
        "    return sentiment_pipeline\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Run inference on each article in the filtered corpus.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_batch_sentiment_inference(\n",
        "    corpus: Union[List[str], Set[str]],\n",
        "    sentiment_pipeline: pipeline,\n",
        "    batch_size: int = 64\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs batched sentiment analysis inference on a corpus of articles.\n",
        "\n",
        "    Args:\n",
        "        corpus (Union[List[str], Set[str]]): A list or set of unique,\n",
        "                                             normalized article texts.\n",
        "        sentiment_pipeline (pipeline): The initialized Hugging Face pipeline.\n",
        "        batch_size (int): The number of articles to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary mapping each article text to\n",
        "                                   its predicted class ('label') and\n",
        "                                   confidence ('score').\n",
        "    \"\"\"\n",
        "    # Convert corpus to list if it's a set, as pipelines expect a sequence.\n",
        "    corpus_list = list(corpus)\n",
        "    logging.info(f\"Running sentiment inference on {len(corpus_list):,} articles with batch size {batch_size}...\")\n",
        "\n",
        "    # --- Batch Inference ---\n",
        "    # Passing the entire list to the pipeline with a batch_size enables highly\n",
        "    # optimized parallel processing on the GPU.\n",
        "    # We specify truncation to handle articles longer than the model's max length.\n",
        "    try:\n",
        "        results = sentiment_pipeline(\n",
        "            corpus_list,\n",
        "            batch_size=batch_size,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during batch sentiment inference: {e}\")\n",
        "        raise\n",
        "\n",
        "    # --- Result Structuring ---\n",
        "    # Combine the input articles with their corresponding results into a dictionary\n",
        "    # for efficient O(1) lookup by article text.\n",
        "    sentiment_results = {\n",
        "        text: {'class': result['label'].lower(), 'confidence': result['score']}\n",
        "        for text, result in zip(corpus_list, results)\n",
        "    }\n",
        "\n",
        "    logging.info(\"Batch sentiment inference completed.\")\n",
        "    return sentiment_results\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Map FinBERT classes to numerical polarity scores.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _map_sentiment_to_polarity(\n",
        "    sentiment_results: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Adds a numerical polarity score to the sentiment results dictionary.\n",
        "\n",
        "    Args:\n",
        "        sentiment_results (Dict[str, Dict[str, Any]]): The dictionary of\n",
        "                                                       inference results.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: The same dictionary, with each value dict\n",
        "                                   augmented with a 'polarity' key.\n",
        "    \"\"\"\n",
        "    # Define the canonical mapping from class label to numerical score.\n",
        "    POLARITY_MAP = {'positive': 1.0, 'neutral': 0.0, 'negative': -1.0}\n",
        "\n",
        "    logging.info(\"Mapping sentiment classes to numerical polarity scores...\")\n",
        "    # Iterate through the results and add the 'polarity' score.\n",
        "    for article_text, result in sentiment_results.items():\n",
        "        sentiment_class = result.get('class')\n",
        "        # Use .get() for safety; if an unexpected class appears, default to neutral (0.0).\n",
        "        polarity = POLARITY_MAP.get(sentiment_class)\n",
        "\n",
        "        if polarity is None:\n",
        "            logging.warning(f\"Unexpected sentiment class '{sentiment_class}' found for an article. Assigning neutral polarity (0.0).\")\n",
        "            polarity = 0.0\n",
        "\n",
        "        result['polarity'] = polarity\n",
        "\n",
        "    return sentiment_results\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def classify_article_sentiment(\n",
        "    corpus_filtered: Set[str],\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_path: Union[str, Path]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end sentiment classification of a news corpus.\n",
        "\n",
        "    This function is idempotent: it will load results from the output path if\n",
        "    the file already exists, avoiding re-computation. Otherwise, it performs:\n",
        "    1.  Loading the pre-trained FinBERT model into an efficient pipeline.\n",
        "    2.  Running batched inference on the entire filtered corpus.\n",
        "    3.  Mapping the resulting sentiment labels to numerical polarity scores.\n",
        "    4.  Persisting the final, comprehensive results to disk.\n",
        "\n",
        "    Args:\n",
        "        corpus_filtered (Set[str]): A set of unique, normalized, and\n",
        "                                    thematically relevant article texts.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_path (Union[str, Path]): The file path to save/load the final\n",
        "                                        sentiment results dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary mapping each article to its\n",
        "                                   sentiment class, confidence, and polarity.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating sentiment classification pipeline...\")\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # --- Idempotency Check ---\n",
        "    # If results already exist, load and return them immediately.\n",
        "    if output_path.exists():\n",
        "        logging.info(f\"Found existing sentiment results at '{output_path}'. Loading from file.\")\n",
        "        with open(output_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    # --- Step 1: Load the FinBERT model and tokenizer into a pipeline. ---\n",
        "    sentiment_pipeline = _load_sentiment_pipeline(study_parameters)\n",
        "    logging.info(\"Step 1/3: FinBERT sentiment pipeline loaded successfully.\")\n",
        "\n",
        "    # --- Step 2: Run batched inference on the filtered corpus. ---\n",
        "    # The pipeline is most efficient when processing many documents at once.\n",
        "    sentiment_results = _run_batch_sentiment_inference(corpus_filtered, sentiment_pipeline)\n",
        "    logging.info(\"Step 2/3: Batch inference completed for all articles.\")\n",
        "\n",
        "    # --- Step 3: Map sentiment classes to numerical polarity scores. ---\n",
        "    final_results = _map_sentiment_to_polarity(sentiment_results)\n",
        "    logging.info(\"Step 3/3: Polarity scores mapped successfully.\")\n",
        "\n",
        "    # --- Persistence ---\n",
        "    # Save the final dictionary to disk for future runs.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Saving final sentiment results to '{output_path}'.\")\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(final_results, f)\n",
        "\n",
        "    logging.info(\"Sentiment classification pipeline finished successfully.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "A5PMZjjS0-uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Aggregate article-level sentiment into stock-day sentiment scores\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Aggregate article-level sentiment into stock-day sentiment scores S_i,t\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Steps 1 & 2: Define weighting and compute the weighted sentiment\n",
        "#                       score per stock-day.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_weighted_sentiment(\n",
        "    articles: List[str],\n",
        "    sentiment_map: Dict[str, Dict[str, Any]]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the confidence-weighted average sentiment score for a list of articles.\n",
        "\n",
        "    This function implements the aggregation logic defined in the paper's\n",
        "    Equation 9, using the FinBERT model's confidence score as the weight.\n",
        "\n",
        "    Equation: S_i,t = (Σ p_k * s_k) / (Σ p_k)\n",
        "    where p_k is the confidence and s_k is the polarity of article k.\n",
        "\n",
        "    Args:\n",
        "        articles (List[str]): A list of raw article texts for a single stock-day.\n",
        "        sentiment_map (Dict[str, Dict[str, Any]]): A dictionary mapping\n",
        "            normalized article texts to their sentiment analysis results\n",
        "            (including 'polarity' and 'confidence').\n",
        "\n",
        "    Returns:\n",
        "        float: The aggregated sentiment score, bounded between -1.0 and 1.0.\n",
        "               Returns 0.0 if there are no articles or if total confidence is zero.\n",
        "    \"\"\"\n",
        "    # Initialize numerator and denominator for the weighted average calculation.\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    # Input validation: ensure the cell content is a list.\n",
        "    if not isinstance(articles, list):\n",
        "        return 0.0\n",
        "\n",
        "    # Iterate through each article for the given stock-day.\n",
        "    for article_text in articles:\n",
        "        # Normalize the article text in the same way as the sentiment_map keys\n",
        "        # to ensure a successful lookup.\n",
        "        normalized_text = article_text.strip().lower()\n",
        "\n",
        "        # Look up the sentiment results for the article.\n",
        "        result = sentiment_map.get(normalized_text)\n",
        "\n",
        "        # If the article has a sentiment score, incorporate it.\n",
        "        if result:\n",
        "            # Retrieve the polarity (s_k) and confidence (p_k).\n",
        "            polarity = result.get('polarity', 0.0)\n",
        "            confidence = result.get('confidence', 0.0)\n",
        "\n",
        "            # Add to the weighted sum.\n",
        "            # Numerator: Σ (confidence * polarity)\n",
        "            numerator += confidence * polarity\n",
        "            # Denominator: Σ confidence\n",
        "            denominator += confidence\n",
        "\n",
        "    # --- Final Score Calculation ---\n",
        "    # Avoid division by zero. If there are no articles or all have zero\n",
        "    # confidence, the sentiment is defined as neutral (0.0).\n",
        "    if denominator == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        # Compute the final weighted average.\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Add the sentiment score as a new column in the DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _add_sentiment_score_to_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    sentiment_map: Dict[str, Dict[str, Any]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the sentiment aggregation to the entire DataFrame and adds the\n",
        "    result as a new column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'News_Articles' column.\n",
        "        sentiment_map (Dict[str, Dict[str, Any]]): The map of article sentiment results.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with the new 'Sentiment_Score' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects.\n",
        "    df_with_sentiment = df.copy()\n",
        "\n",
        "    logging.info(\"Aggregating article-level sentiment into stock-day scores...\")\n",
        "    # Use the .apply() method to run the aggregation function on each row's\n",
        "    # 'News_Articles' list. This is the most direct way to perform this\n",
        "    # custom row-wise operation.\n",
        "    sentiment_scores = df_with_sentiment['News_Articles'].apply(\n",
        "        _calculate_weighted_sentiment,\n",
        "        args=(sentiment_map,)\n",
        "    )\n",
        "\n",
        "    # Assign the resulting Series to a new column in the DataFrame.\n",
        "    df_with_sentiment['Sentiment_Score'] = sentiment_scores\n",
        "\n",
        "    # --- Post-computation Validation ---\n",
        "    # The final score must be mathematically bounded between -1 and 1.\n",
        "    # We use a small tolerance for floating-point comparisons.\n",
        "    tolerance = 1e-9\n",
        "    if not df_with_sentiment['Sentiment_Score'].between(-1 - tolerance, 1 + tolerance).all():\n",
        "        raise ValueError(\"Calculated 'Sentiment_Score' is out of the expected [-1, 1] bounds.\")\n",
        "\n",
        "    # Log summary statistics for the newly created feature.\n",
        "    logging.info(\"'Sentiment_Score' column added successfully. Summary statistics:\")\n",
        "    logging.info(df_with_sentiment['Sentiment_Score'].describe().to_string())\n",
        "\n",
        "    return df_with_sentiment\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_stock_day_sentiment(\n",
        "    df_filtered: pd.DataFrame,\n",
        "    sentiment_results: Dict[str, Dict[str, Any]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of article-level sentiment into a stock-day\n",
        "    level time series feature.\n",
        "\n",
        "    This function takes the raw sentiment scores for individual articles and\n",
        "    computes a single, confidence-weighted sentiment score for each\n",
        "    stock-day observation in the main DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df_filtered (pd.DataFrame): The DataFrame with the filtered\n",
        "                                    'News_Articles' column from Task 8.\n",
        "        sentiment_results (Dict[str, Dict[str, Any]]): The dictionary mapping\n",
        "            each unique article to its sentiment analysis results, including\n",
        "            'polarity' and 'confidence', from Task 9.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame, now enriched with a 'Sentiment_Score'\n",
        "                      column representing the aggregated sentiment for each\n",
        "                      stock-day.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating stock-day sentiment aggregation pipeline...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'News_Articles' not in df_filtered.columns:\n",
        "        raise ValueError(\"Input DataFrame is missing the 'News_Articles' column.\")\n",
        "    if not isinstance(sentiment_results, dict) or not sentiment_results:\n",
        "        raise ValueError(\"'sentiment_results' must be a non-empty dictionary.\")\n",
        "\n",
        "    # --- Steps 1, 2, and 3 are combined in this single, efficient function call ---\n",
        "    # The weighting logic (Step 1) is inside _calculate_weighted_sentiment.\n",
        "    # The computation (Step 2) and column addition (Step 3) are handled by\n",
        "    # the _add_sentiment_score_to_dataframe function.\n",
        "    df_with_sentiment = _add_sentiment_score_to_dataframe(df_filtered, sentiment_results)\n",
        "\n",
        "    logging.info(\"Stock-day sentiment aggregation pipeline finished successfully.\")\n",
        "\n",
        "    return df_with_sentiment\n"
      ],
      "metadata": {
        "id": "6KkNeodS2X41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Aggregate article-level sentiment into market-level sentiment shares\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Aggregate article-level sentiment into market-level sentiment shares S_t^c\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Construct confidence-weighted one-hot vectors per article.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _augment_with_one_hot_vectors(\n",
        "    sentiment_results: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Augments the sentiment results dictionary with confidence-weighted one-hot vectors.\n",
        "\n",
        "    For each article, this function computes a vector representing the sentiment\n",
        "    distribution, where the predicted class's position holds the confidence\n",
        "    score and all others are zero.\n",
        "\n",
        "    Equation: s^c = p if ĉ = c, else 0\n",
        "\n",
        "    Args:\n",
        "        sentiment_results (Dict[str, Dict[str, Any]]): The dictionary of\n",
        "            inference results from Task 9.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: The augmented dictionary with an added\n",
        "                                   'one_hot_weighted' key for each article.\n",
        "    \"\"\"\n",
        "    # Define the universe of possible sentiment classes.\n",
        "    sentiment_classes = ['positive', 'neutral', 'negative']\n",
        "\n",
        "    # Iterate through each article's sentiment result to add the one-hot vector.\n",
        "    for result in sentiment_results.values():\n",
        "        # Initialize a one-hot vector with zeros.\n",
        "        one_hot_vector = {s_class: 0.0 for s_class in sentiment_classes}\n",
        "\n",
        "        # Get the predicted class and confidence score.\n",
        "        predicted_class = result.get('class')\n",
        "        confidence = result.get('confidence', 0.0)\n",
        "\n",
        "        # If the predicted class is valid, place the confidence score in the\n",
        "        # corresponding position of the one-hot vector.\n",
        "        if predicted_class in one_hot_vector:\n",
        "            one_hot_vector[predicted_class] = confidence\n",
        "        else:\n",
        "            logging.warning(f\"Found unexpected sentiment class '{predicted_class}'. It will be ignored in one-hot vector creation.\")\n",
        "\n",
        "        # Add the computed vector to the results dictionary.\n",
        "        result['one_hot_weighted'] = one_hot_vector\n",
        "\n",
        "    return sentiment_results\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Aggregate by date to obtain market-level sentiment shares.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_daily_market_sentiment(\n",
        "    df: pd.DataFrame,\n",
        "    sentiment_map: Dict[str, Dict[str, Any]],\n",
        "    master_calendar: pd.DatetimeIndex\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates article sentiments to compute daily market-wide sentiment shares.\n",
        "\n",
        "    This function creates a long-form DataFrame of all article occurrences,\n",
        "    joins the sentiment data, and then performs a highly efficient groupby\n",
        "    operation to calculate the daily shares.\n",
        "\n",
        "    Equation: S_t^c = (Σ s_k^c) / (Σ p_k) for all articles k on day t.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The main DataFrame with the 'News_Articles' column.\n",
        "        sentiment_map (Dict[str, Dict[str, Any]]): The augmented map of article\n",
        "                                                   sentiment results.\n",
        "        master_calendar (pd.DatetimeIndex): The master list of all trading days.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by date with columns for the share\n",
        "                      of negative, neutral, and positive sentiment.\n",
        "    \"\"\"\n",
        "    logging.info(\"Creating long-form DataFrame of all article occurrences for efficient aggregation.\")\n",
        "\n",
        "    # --- Create a mapping from normalized text to sentiment data ---\n",
        "    # This avoids repeated normalization during the main loop.\n",
        "    normalized_sentiment_map = {\n",
        "        text.strip().lower(): data\n",
        "        for text, data in sentiment_map.items()\n",
        "    }\n",
        "\n",
        "    # --- Build a list of records for all article occurrences ---\n",
        "    article_records = []\n",
        "    # Iterate through each row of the main DataFrame.\n",
        "    for (date, _), row in df.iterrows():\n",
        "        articles = row['News_Articles']\n",
        "        if isinstance(articles, list):\n",
        "            for article_text in articles:\n",
        "                # For each article, create a record with its date and normalized text.\n",
        "                article_records.append({\n",
        "                    'Date': date,\n",
        "                    'normalized_text': article_text.strip().lower()\n",
        "                })\n",
        "\n",
        "    if not article_records:\n",
        "        logging.warning(\"No articles found in the DataFrame to aggregate. Returning empty sentiment shares.\")\n",
        "        # Return a correctly structured but empty DataFrame.\n",
        "        return pd.DataFrame(columns=['Market_Sentiment_Neg', 'Market_Sentiment_Neu', 'Market_Sentiment_Pos']).reindex(master_calendar).fillna(0)\n",
        "\n",
        "    # Convert the list of records into a DataFrame.\n",
        "    articles_df = pd.DataFrame(article_records)\n",
        "\n",
        "    # --- Map sentiment data to the articles DataFrame ---\n",
        "    # Extract one-hot vectors and confidence scores using the map.\n",
        "    articles_df['sentiment_data'] = articles_df['normalized_text'].map(normalized_sentiment_map)\n",
        "    articles_df.dropna(subset=['sentiment_data'], inplace=True) # Drop articles not in map\n",
        "\n",
        "    articles_df['confidence'] = articles_df['sentiment_data'].apply(lambda x: x['confidence'])\n",
        "    # Create columns for each component of the one-hot vector.\n",
        "    one_hot_df = pd.DataFrame(articles_df['sentiment_data'].apply(lambda x: x['one_hot_weighted']).tolist(), index=articles_df.index)\n",
        "\n",
        "    # Combine into a single DataFrame for aggregation.\n",
        "    aggregation_df = pd.concat([articles_df[['Date', 'confidence']], one_hot_df], axis=1)\n",
        "\n",
        "    # --- Perform GroupBy Aggregation ---\n",
        "    # Group by date and sum the confidence and one-hot vector components.\n",
        "    daily_sums = aggregation_df.groupby('Date').sum()\n",
        "\n",
        "    # --- Calculate Sentiment Shares ---\n",
        "    # The denominator is the total confidence for the day.\n",
        "    total_daily_confidence = daily_sums['confidence']\n",
        "\n",
        "    # The numerators are the sums of the weighted one-hot vectors.\n",
        "    market_sentiment_df = pd.DataFrame(index=daily_sums.index)\n",
        "    market_sentiment_df['Market_Sentiment_Neg'] = daily_sums['negative'].div(total_daily_confidence).fillna(0)\n",
        "    market_sentiment_df['Market_Sentiment_Neu'] = daily_sums['neutral'].div(total_daily_confidence).fillna(0)\n",
        "    market_sentiment_df['Market_Sentiment_Pos'] = daily_sums['positive'].div(total_daily_confidence).fillna(0)\n",
        "\n",
        "    # --- Finalize the DataFrame ---\n",
        "    # Ensure all days from the master calendar are present, filling missing days with a neutral default.\n",
        "    market_sentiment_df = market_sentiment_df.reindex(master_calendar)\n",
        "    market_sentiment_df.fillna({'Market_Sentiment_Neu': 1.0, 'Market_Sentiment_Neg': 0.0, 'Market_Sentiment_Pos': 0.0}, inplace=True)\n",
        "\n",
        "    return market_sentiment_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Merge market-level sentiment shares into the DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _merge_market_sentiment_shares(\n",
        "    df: pd.DataFrame,\n",
        "    market_sentiment_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges the daily market sentiment shares into the main DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The main DataFrame.\n",
        "        market_sentiment_df (pd.DataFrame): Date-indexed DataFrame of sentiment shares.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the main DataFrame with three new market\n",
        "                      sentiment columns.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_merged = df.copy()\n",
        "\n",
        "    logging.info(\"Merging daily market sentiment shares into the main DataFrame.\")\n",
        "    # Perform a left merge. This broadcasts the daily market sentiment values\n",
        "    # to all tickers present on that day.\n",
        "    df_merged = df_merged.merge(\n",
        "        market_sentiment_df,\n",
        "        left_on='Date',\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- Post-merge Validation ---\n",
        "    # Check that the merge did not introduce NaNs.\n",
        "    new_cols = ['Market_Sentiment_Neg', 'Market_Sentiment_Neu', 'Market_Sentiment_Pos']\n",
        "    if df_merged[new_cols].isna().any().any():\n",
        "        raise RuntimeError(\"Merging market sentiment shares resulted in unexpected NaNs.\")\n",
        "\n",
        "    # Check that the shares on each day sum to 1.0 (within a tolerance).\n",
        "    daily_sums = df_merged.groupby('Date')[new_cols].first().sum(axis=1)\n",
        "    if not np.allclose(daily_sums, 1.0, atol=1e-6):\n",
        "        logging.warning(\"Market sentiment shares do not sum to 1.0 on all days. Check aggregation logic.\")\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_market_level_sentiment(\n",
        "    df_with_sentiment: pd.DataFrame,\n",
        "    sentiment_results: Dict[str, Dict[str, Any]],\n",
        "    master_calendar: pd.DatetimeIndex\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of article sentiment into market-level shares.\n",
        "\n",
        "    This function computes the daily proportion of negative, neutral, and\n",
        "    positive sentiment across all articles in the corpus and merges these\n",
        "    features into the main DataFrame. The pipeline includes:\n",
        "    1.  Augmenting sentiment results with confidence-weighted one-hot vectors.\n",
        "    2.  Aggregating these vectors on a daily basis to compute sentiment shares.\n",
        "    3.  Merging these daily shares back into the main panel DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df_with_sentiment (pd.DataFrame): The DataFrame from Task 10.\n",
        "        sentiment_results (Dict[str, Dict[str, Any]]): The dictionary of\n",
        "            sentiment results from Task 9.\n",
        "        master_calendar (pd.DatetimeIndex): The master list of all trading days\n",
        "                                            from Task 6.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame enriched with three new columns:\n",
        "                      'Market_Sentiment_Neg', 'Market_Sentiment_Neu',\n",
        "                      'Market_Sentiment_Pos'.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating market-level sentiment aggregation pipeline...\")\n",
        "\n",
        "    # --- Step 1: Construct confidence-weighted one-hot vectors. ---\n",
        "    augmented_sentiment_map = _augment_with_one_hot_vectors(sentiment_results)\n",
        "    logging.info(\"Step 1/3: Augmented sentiment results with one-hot vectors.\")\n",
        "\n",
        "    # --- Step 2: Aggregate by date to get market sentiment shares. ---\n",
        "    market_sentiment_df = _aggregate_daily_market_sentiment(\n",
        "        df_with_sentiment, augmented_sentiment_map, master_calendar\n",
        "    )\n",
        "    logging.info(\"Step 2/3: Aggregated daily market sentiment shares.\")\n",
        "\n",
        "    # --- Step 3: Merge the market-level shares into the main DataFrame. ---\n",
        "    df_final = _merge_market_sentiment_shares(df_with_sentiment, market_sentiment_df)\n",
        "    logging.info(\"Step 3/3: Merged market sentiment shares into the main DataFrame.\")\n",
        "\n",
        "    logging.info(\"Market-level sentiment aggregation pipeline finished successfully.\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "G501nmBJ3cUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Construct the Hype Index\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Construct the Hype Index H_i,t (attention share)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Count articles per stock-day N_i,t.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _count_articles_per_stock_day(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Counts the number of articles for each stock-day observation.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'News_Articles' column,\n",
        "                           where each cell is a list of strings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with a new 'N_Articles'\n",
        "                      column of integer counts.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects.\n",
        "    df_with_counts = df.copy()\n",
        "\n",
        "    logging.info(\"Counting articles for each stock-day...\")\n",
        "    # Define a safe length function that returns 0 for non-list inputs.\n",
        "    def safe_len(item):\n",
        "        return len(item) if isinstance(item, list) else 0\n",
        "\n",
        "    # Apply the safe length function to the 'News_Articles' column.\n",
        "    df_with_counts['N_Articles'] = df_with_counts['News_Articles'].apply(safe_len)\n",
        "\n",
        "    # Ensure the new column is of integer type.\n",
        "    df_with_counts['N_Articles'] = df_with_counts['N_Articles'].astype('int64')\n",
        "\n",
        "    return df_with_counts\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Compute the market-wide article count N_mkt,t per day.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_daily_market_article_counts(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Aggregates article counts to get the total number of articles per day.\n",
        "\n",
        "    Equation: N_mkt,t = Σ_i N_i,t\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'N_Articles' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series indexed by 'Date' containing the total article\n",
        "                   count for each day.\n",
        "    \"\"\"\n",
        "    logging.info(\"Computing total market-wide article counts per day...\")\n",
        "    # Group by the 'Date' level of the index and sum the 'N_Articles'.\n",
        "    # This is a highly efficient, vectorized operation.\n",
        "    market_counts = df.groupby(level='Date')['N_Articles'].sum()\n",
        "\n",
        "    return market_counts\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Compute the Hype Index and add it to the DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_and_validate_hype_index(\n",
        "    df: pd.DataFrame,\n",
        "    market_counts: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the Hype Index and validates its properties as a probability measure.\n",
        "\n",
        "    Equation: H_i,t = N_i,t / N_mkt,t\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with 'N_Articles'.\n",
        "        market_counts (pd.Series): The date-indexed series of total daily counts.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new 'Hype_Index'\n",
        "                      column.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_with_hype = df.copy()\n",
        "\n",
        "    logging.info(\"Computing the Hype Index (daily attention share)...\")\n",
        "    # --- Hype Index Calculation ---\n",
        "    # Map the daily market counts to each row based on its date.\n",
        "    # This creates a column where each row has the total for its day.\n",
        "    df_with_hype['N_mkt'] = df_with_hype.index.get_level_values('Date').map(market_counts)\n",
        "\n",
        "    # Compute the Hype Index via vectorized division.\n",
        "    df_with_hype['Hype_Index'] = df_with_hype['N_Articles'] / df_with_hype['N_mkt']\n",
        "\n",
        "    # Handle division by zero: on days with no news (N_mkt = 0), the\n",
        "    # Hype Index for all stocks should be 0, not NaN or inf.\n",
        "    df_with_hype['Hype_Index'].fillna(0, inplace=True)\n",
        "    df_with_hype.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "    # Clean up the intermediate market count column.\n",
        "    df_with_hype.drop(columns=['N_mkt'], inplace=True)\n",
        "\n",
        "    # --- Post-computation Validation ---\n",
        "    # The Hype Index for each day must sum to 1.0 (or 0.0 on no-news days).\n",
        "    # This confirms it is a valid probability measure.\n",
        "    daily_hype_sums = df_with_hype.groupby(level='Date')['Hype_Index'].sum()\n",
        "    # Use np.allclose for robust floating-point comparison.\n",
        "    is_valid_measure = np.allclose(\n",
        "        daily_hype_sums, 1.0, atol=1e-9\n",
        "    ) | np.allclose(\n",
        "        daily_hype_sums, 0.0, atol=1e-9\n",
        "    )\n",
        "\n",
        "    if not is_valid_measure.all():\n",
        "        # Find and log specific dates that fail the check for easier debugging.\n",
        "        invalid_dates = daily_hype_sums[~is_valid_measure]\n",
        "        logging.warning(f\"Hype Index validation failed. The sum across tickers does not equal 1.0 on {len(invalid_dates)} days.\")\n",
        "        logging.warning(f\"Example invalid daily sums:\\n{invalid_dates.head().to_string()}\")\n",
        "    else:\n",
        "        logging.info(\"Hype Index validated successfully as a daily probability measure.\")\n",
        "\n",
        "    return df_with_hype\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_hype_index(df_market_sentiment: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the Hype Index feature.\n",
        "\n",
        "    This function calculates a measure of relative media attention for each\n",
        "    stock on each day. The pipeline includes:\n",
        "    1.  Counting the number of articles per stock-day (N_i,t).\n",
        "    2.  Aggregating these to find the total number of articles market-wide\n",
        "        each day (N_mkt,t).\n",
        "    3.  Computing the Hype Index as the ratio H_i,t = N_i,t / N_mkt,t and\n",
        "        validating its properties.\n",
        "\n",
        "    Args:\n",
        "        df_market_sentiment (pd.DataFrame): The DataFrame from Task 11, which\n",
        "            contains the filtered 'News_Articles' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame enriched with the 'Hype_Index' and\n",
        "                      'N_Articles' columns.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating Hype Index construction pipeline...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'News_Articles' not in df_market_sentiment.columns:\n",
        "        raise ValueError(\"Input DataFrame is missing the 'News_Articles' column.\")\n",
        "\n",
        "    # --- Step 1: Count articles per stock-day. ---\n",
        "    df_with_counts = _count_articles_per_stock_day(df_market_sentiment)\n",
        "    logging.info(\"Step 1/3: Article counts per stock-day computed.\")\n",
        "\n",
        "    # --- Step 2: Compute market-wide article counts per day. ---\n",
        "    market_counts = _compute_daily_market_article_counts(df_with_counts)\n",
        "    logging.info(\"Step 2/3: Market-wide daily article counts computed.\")\n",
        "\n",
        "    # --- Step 3: Compute and validate the Hype Index. ---\n",
        "    df_final = _compute_and_validate_hype_index(df_with_counts, market_counts)\n",
        "    logging.info(\"Step 3/3: Hype Index computed and validated.\")\n",
        "\n",
        "    logging.info(\"Hype Index construction pipeline finished successfully.\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "UyXMthnT4mzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Define rolling windows for LPPL calibration\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Define rolling windows for LPPL calibration\n",
        "# ==============================================================================\n",
        "\n",
        "# Define a structured data container for window metadata for clarity and type safety.\n",
        "class LPPLWindow(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured data container for storing the metadata and data of a single\n",
        "    rolling window intended for LPPL (Log-Periodic Power Law) model calibration.\n",
        "\n",
        "    This class uses a NamedTuple for immutability and clarity, ensuring that\n",
        "    each window object is a lightweight, self-contained package of information\n",
        "    required by the downstream optimization routines.\n",
        "\n",
        "    Attributes:\n",
        "        ticker (str):\n",
        "            The unique identifier (e.g., stock ticker) for the time series\n",
        "            from which the window was extracted. This is essential for tracking\n",
        "            and aggregating results on a per-security basis.\n",
        "\n",
        "        end_date (pd.Timestamp):\n",
        "            The calendar date corresponding to the last observation in this\n",
        "            window. This acts as the anchor point in time for the window,\n",
        "            allowing results of the LPPL fit to be mapped back to the correct\n",
        "            point in the master DataFrame.\n",
        "\n",
        "        log_price_series (pd.Series):\n",
        "            A pandas Series containing the log-price data for the window.\n",
        "            Crucially, the index of this Series is not a DatetimeIndex but a\n",
        "            simple integer index ranging from 1 to W (where W is the window\n",
        "            size). This numerical time index `t` is required for the mathematical\n",
        "            formulation of the LPPL model during the non-linear optimization process.\n",
        "    \"\"\"\n",
        "    # The unique identifier for the security (e.g., 'AAPL').\n",
        "    ticker: str\n",
        "\n",
        "    # The timestamp of the last data point in the window.\n",
        "    end_date: pd.Timestamp\n",
        "\n",
        "    # The series of log-prices for the window, indexed from 1 to W.\n",
        "    log_price_series: pd.Series\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Extract the rolling window size from configuration.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_lppl_window_size(config: Dict[str, Any]) -> int:\n",
        "    \"\"\"\n",
        "    Retrieves and validates the rolling window size for LPPL fitting from the\n",
        "    configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        int: The validated rolling window size.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required key is missing from the configuration.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested key for the window size.\n",
        "        window_size = config['descriptive_model']['lppl_fitting']['rolling_window_size']\n",
        "        # The type and range of this parameter were already validated in Task 1,\n",
        "        # so we can be confident in its value here.\n",
        "        logging.info(f\"LPPL rolling window size set to {window_size} trading days.\")\n",
        "        return window_size\n",
        "    except KeyError as e:\n",
        "        # This error should not occur if the config was validated by Task 1.\n",
        "        logging.error(\"Configuration key for LPPL rolling window size is missing.\")\n",
        "        raise KeyError(\"Missing 'descriptive_model.lppl_fitting.rolling_window_size' in configuration.\") from e\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Generate window start and end indices for each ticker.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_lppl_windows(df: pd.DataFrame, window_size: int) -> List[LPPLWindow]:\n",
        "    \"\"\"\n",
        "    Generates a list of valid rolling windows for LPPL calibration.\n",
        "\n",
        "    For each ticker, this function creates overlapping windows of a specified\n",
        "    size. A window is considered valid only if it is full (contains no NaNs\n",
        "    from data gaps). Each window's data is prepared with a simple integer time\n",
        "    index (1 to W) required for the LPPL optimization algorithm.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'Log_Price' column.\n",
        "        window_size (int): The number of observations in each rolling window.\n",
        "\n",
        "    Returns:\n",
        "        List[LPPLWindow]: A list of structured window metadata objects.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Generating rolling windows of size {window_size} for LPPL fitting...\")\n",
        "\n",
        "    # This list will store the metadata for all valid windows across all tickers.\n",
        "    all_windows: List[LPPLWindow] = []\n",
        "\n",
        "    # Group the DataFrame by ticker to process each time series independently.\n",
        "    grouped_by_ticker = df.groupby(level='TICKER')['Log_Price']\n",
        "\n",
        "    for ticker, log_price_series in grouped_by_ticker:\n",
        "        # Check if the ticker has enough data points to form at least one window.\n",
        "        if len(log_price_series) < window_size:\n",
        "            continue # Skip tickers with insufficient history.\n",
        "\n",
        "        # Use the .rolling() method to create an iterator of windows.\n",
        "        # This is highly memory-efficient as it doesn't load all windows at once.\n",
        "        for window in log_price_series.rolling(window=window_size):\n",
        "            # The rolling object yields partial windows at the start. We only want full ones.\n",
        "            if len(window) < window_size:\n",
        "                continue\n",
        "\n",
        "            # A window is invalid if it contains any NaNs (e.g., from data gaps).\n",
        "            if window.isna().any():\n",
        "                continue\n",
        "\n",
        "            # --- Prepare the window data for LPPL fitting ---\n",
        "            # The LPPL formula requires a simple integer time index from 1 to W.\n",
        "            # We reset the index of the window Series to create this.\n",
        "            window_data = window.reset_index(drop=True)\n",
        "            window_data.index = window_data.index + 1 # Index from 1 to W\n",
        "\n",
        "            # The end date of the window is the last date in its original DatetimeIndex.\n",
        "            end_date = window.index[-1][0] # window.index is a MultiIndex, get date from first level\n",
        "\n",
        "            # Create the structured metadata object for this valid window.\n",
        "            lppl_window = LPPLWindow(\n",
        "                ticker=ticker,\n",
        "                end_date=end_date,\n",
        "                log_price_series=window_data\n",
        "            )\n",
        "            all_windows.append(lppl_window)\n",
        "\n",
        "    return all_windows\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Persist window metadata and log statistics.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_windows_and_log_stats(\n",
        "    windows: List[LPPLWindow],\n",
        "    output_path: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Saves the list of generated LPPL windows to disk and logs summary statistics.\n",
        "\n",
        "    Args:\n",
        "        windows (List[LPPLWindow]): The list of window metadata objects.\n",
        "        output_path (Path): The file path to save the pickled list to.\n",
        "    \"\"\"\n",
        "    # --- Persistence ---\n",
        "    # Ensure the parent directory for the output file exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    logging.info(f\"Saving {len(windows):,} generated LPPL windows to '{output_path}'...\")\n",
        "    # Use pickle to serialize the list of NamedTuple objects.\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(windows, f)\n",
        "\n",
        "    # --- Logging Statistics ---\n",
        "    if windows:\n",
        "        # Calculate statistics on the generated windows.\n",
        "        num_windows = len(windows)\n",
        "        tickers_covered = {w.ticker for w in windows}\n",
        "        num_tickers = len(tickers_covered)\n",
        "\n",
        "        logging.info(f\"Successfully generated and saved {num_windows:,} valid windows.\")\n",
        "        logging.info(f\"Coverage: {num_tickers} tickers.\")\n",
        "    else:\n",
        "        logging.warning(\"No valid LPPL windows were generated. Check data for sufficient length and continuity.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def define_lppl_calibration_windows(\n",
        "    df_features: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_path: Union[str, Path] = \"data_intermediate/lppl_windows.pkl\"\n",
        ") -> List[LPPLWindow]:\n",
        "    \"\"\"\n",
        "    Orchestrates the definition and generation of rolling windows for LPPL fitting.\n",
        "\n",
        "    This function is idempotent: it will load the window definitions from disk\n",
        "    if they have been previously generated. Otherwise, it performs:\n",
        "    1.  Extracting the rolling window size from the configuration.\n",
        "    2.  Generating all valid, complete rolling windows for each ticker.\n",
        "    3.  Persisting the list of window metadata to disk for reproducibility and\n",
        "        to avoid re-computation.\n",
        "\n",
        "    Args:\n",
        "        df_features (pd.DataFrame): The DataFrame containing all engineered\n",
        "                                    features, including 'Log_Price'.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_path (Union[str, Path]): The file path to save/load the list\n",
        "                                        of LPPL windows.\n",
        "\n",
        "    Returns:\n",
        "        List[LPPLWindow]: A list of structured objects, each containing the\n",
        "                          metadata and data for a single window to be fitted.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating LPPL calibration window definition pipeline...\")\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # --- Idempotency Check ---\n",
        "    # If the window file already exists, load and return it to skip computation.\n",
        "    if output_path.exists():\n",
        "        logging.info(f\"Found existing LPPL windows at '{output_path}'. Loading from file.\")\n",
        "        with open(output_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'Log_Price' not in df_features.columns:\n",
        "        raise ValueError(\"Input DataFrame is missing the required 'Log_Price' column.\")\n",
        "\n",
        "    # --- Step 1: Extract the rolling window size. ---\n",
        "    window_size = _get_lppl_window_size(study_parameters)\n",
        "    logging.info(\"Step 1/3: LPPL rolling window size extracted.\")\n",
        "\n",
        "    # --- Step 2: Generate all valid rolling windows. ---\n",
        "    windows = _generate_lppl_windows(df_features, window_size)\n",
        "    logging.info(\"Step 2/3: All valid rolling windows generated.\")\n",
        "\n",
        "    # --- Step 3: Persist the windows and log summary statistics. ---\n",
        "    _persist_windows_and_log_stats(windows, output_path)\n",
        "    logging.info(\"Step 3/3: Window metadata persisted and statistics logged.\")\n",
        "\n",
        "    logging.info(\"LPPL calibration window definition pipeline finished successfully.\")\n",
        "\n",
        "    return windows\n"
      ],
      "metadata": {
        "id": "nh6vrhnf6GYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Initialize LPPL parameter bounds and multi-start seeds\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Initialize LPPL parameter bounds and multi-start seeds\n",
        "# ==============================================================================\n",
        "\n",
        "# Standardized order of LPPL parameters to be used throughout the fitting process.\n",
        "LPPL_PARAM_ORDER = ['A', 'B', 'C', 'm', 'omega', 'phi', 't_c']\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Initialize LPPL parameter bounds.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _define_lppl_parameter_bounds(\n",
        "    config: Dict[str, Any],\n",
        "    window_size: int\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Defines the lower and upper bounds for the 7 LPPL model parameters.\n",
        "\n",
        "    The bounds are derived from the study configuration and theoretical\n",
        "    constraints of the LPPL model. The bounds for the critical time `t_c` are\n",
        "    dynamically calculated based on the window size.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "        window_size (int): The size of the fitting window (W).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[float]]: A tuple containing two lists:\n",
        "            - The lower bounds for all 7 parameters in a standard order.\n",
        "            - The upper bounds for all 7 parameters in the same standard order.\n",
        "            This format is directly compatible with `scipy.optimize.least_squares`.\n",
        "    \"\"\"\n",
        "    # Retrieve the static parameter constraints from the configuration.\n",
        "    constraints = config['descriptive_model']['lppl_fitting']['parameter_constraints']\n",
        "\n",
        "    # Define bounds for all 7 parameters. Note that t_c is relative to the\n",
        "    # window's integer index (1 to W). The end of the window is at t=W.\n",
        "    bounds_map = {\n",
        "        'A': (-np.inf, np.inf),\n",
        "        'B': (constraints['B']['min'], constraints['B']['max']),\n",
        "        'C': (-np.inf, np.inf), # Unconstrained amplitude\n",
        "        'm': (constraints['m']['min'], constraints['m']['max']),\n",
        "        'omega': (constraints['omega']['min'], constraints['omega']['max']),\n",
        "        'phi': (-2 * np.pi, 2 * np.pi), # Phase can be unconstrained over a full cycle\n",
        "        't_c': (window_size + 5, window_size + 250) # t_c must be in the future\n",
        "    }\n",
        "\n",
        "    # Create the lower and upper bound lists in the standardized order.\n",
        "    lower_bounds = [bounds_map[param][0] for param in LPPL_PARAM_ORDER]\n",
        "    upper_bounds = [bounds_map[param][1] for param in LPPL_PARAM_ORDER]\n",
        "\n",
        "    return lower_bounds, upper_bounds\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Generate multi-start initialization seeds.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_lppl_initial_seeds(\n",
        "    log_price_series: pd.Series,\n",
        "    window_size: int,\n",
        "    num_seeds: int = 10,\n",
        "    seed: int = 42\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates a set of random initial parameter guesses (seeds) for the LPPL\n",
        "    optimization, using a multi-start strategy to avoid local minima.\n",
        "\n",
        "    Args:\n",
        "        log_price_series (pd.Series): The log-price series for a single window.\n",
        "        window_size (int): The size of the fitting window (W).\n",
        "        num_seeds (int): The number of different initial guesses to generate.\n",
        "        seed (int): The seed for the random number generator for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        List[np.ndarray]: A list of 7-element numpy arrays, where each array\n",
        "                          is a complete set of initial parameter guesses.\n",
        "    \"\"\"\n",
        "    # Initialize a seeded random number generator for reproducible results.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # This list will store the generated seed vectors.\n",
        "    initial_seeds = []\n",
        "\n",
        "    # Data-driven parameters for initialization.\n",
        "    mean_log_price = log_price_series.mean()\n",
        "\n",
        "    for _ in range(num_seeds):\n",
        "        # Generate one set of random initial parameters based on the specified distributions.\n",
        "        seed_params = {\n",
        "            'A': rng.normal(loc=mean_log_price, scale=0.1),\n",
        "            'B': rng.uniform(-1.0, -0.01),\n",
        "            'C': rng.uniform(-0.5, 0.5),\n",
        "            'm': rng.uniform(0.1, 0.9),\n",
        "            'omega': rng.uniform(2.0, 20.0),\n",
        "            'phi': rng.uniform(-np.pi, np.pi),\n",
        "            't_c': rng.uniform(window_size + 10, window_size + 100)\n",
        "        }\n",
        "\n",
        "        # Assemble the seed vector in the standardized order.\n",
        "        seed_vector = np.array([seed_params[param] for param in LPPL_PARAM_ORDER])\n",
        "        initial_seeds.append(seed_vector)\n",
        "\n",
        "    return initial_seeds\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Document the initialization strategy.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _document_initialization_strategy(\n",
        "    bounds: Tuple[List[float], List[float]],\n",
        "    config: Dict[str, Any],\n",
        "    output_path: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Saves a JSON file documenting the LPPL initialization strategy.\n",
        "\n",
        "    This creates a reproducible record of the parameter bounds, number of seeds,\n",
        "    sampling distributions, and convergence tolerances used in the optimization.\n",
        "\n",
        "    Args:\n",
        "        bounds (Tuple[List[float], List[float]]): The lower and upper parameter bounds.\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "        output_path (Path): The file path for the JSON metadata log.\n",
        "    \"\"\"\n",
        "    # Unpack the bounds tuple.\n",
        "    lower_bounds, upper_bounds = bounds\n",
        "\n",
        "    # Construct a dictionary of the metadata to be saved.\n",
        "    metadata = {\n",
        "        \"parameter_order\": LPPL_PARAM_ORDER,\n",
        "        \"parameter_bounds\": {\n",
        "            param: (lower, upper)\n",
        "            for param, lower, upper in zip(LPPL_PARAM_ORDER, lower_bounds, upper_bounds)\n",
        "        },\n",
        "        \"multi_start_seeds\": {\n",
        "            \"num_seeds\": 10, # As specified in the instructions\n",
        "            \"sampling_distributions\": {\n",
        "                'A': \"Normal(mean(log_price), 0.1)\",\n",
        "                'B': \"Uniform(-1.0, -0.01)\",\n",
        "                'C': \"Uniform(-0.5, 0.5)\",\n",
        "                'm': \"Uniform(0.1, 0.9)\",\n",
        "                'omega': \"Uniform(2.0, 20.0)\",\n",
        "                'phi': \"Uniform(-pi, pi)\",\n",
        "                't_c': \"Uniform(W+10, W+100)\"\n",
        "            }\n",
        "        },\n",
        "        \"convergence_tolerances\": {\n",
        "            \"ftol\": 1e-8, # Standard tolerance for function value change\n",
        "            \"xtol\": 1e-8, # Standard tolerance for parameter change\n",
        "            \"max_nfev\": 1000 # Max number of function evaluations\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Ensure the parent directory exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Convert any numpy types to be JSON serializable.\n",
        "    serializable_metadata = _make_json_serializable(metadata)\n",
        "\n",
        "    # Write the metadata to the JSON file.\n",
        "    logging.info(f\"Saving LPPL initialization metadata to '{output_path}'.\")\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(serializable_metadata, f, indent=4)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_lppl_fitter(\n",
        "    study_parameters: Dict[str, Any],\n",
        "    log_dir: Union[str, Path] = \"logs\"\n",
        ") -> Tuple[Tuple[List[float], List[float]], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the setup of parameters for the LPPL fitting process.\n",
        "\n",
        "    This function prepares the two key components required for a robust,\n",
        "    constrained, multi-start optimization:\n",
        "    1.  Defines the strict lower and upper bounds for each of the 7 LPPL parameters.\n",
        "    2.  Generates a strategy for creating multiple random starting points (seeds)\n",
        "        to mitigate the risk of converging to local minima.\n",
        "    3.  Documents this entire strategy in a metadata file for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        log_dir (Union[str, Path]): Directory to save the metadata log.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tuple[List[float], List[float]], Dict[str, Any]]: A tuple containing:\n",
        "            - A tuple of (lower_bounds, upper_bounds) for the optimizer.\n",
        "            - The original study_parameters dictionary (passed through).\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating LPPL fitter initialization pipeline...\")\n",
        "\n",
        "    # --- Step 1: Define parameter bounds. ---\n",
        "    # This requires the window size from the config.\n",
        "    window_size = study_parameters['descriptive_model']['lppl_fitting']['rolling_window_size']\n",
        "    bounds = _define_lppl_parameter_bounds(study_parameters, window_size)\n",
        "    logging.info(\"Step 1/3: LPPL parameter bounds defined successfully.\")\n",
        "\n",
        "    # --- Step 2: The seed generation logic is encapsulated in its own function. ---\n",
        "    # This function (`generate_lppl_initial_seeds`) will be called within the\n",
        "    # main fitting loop (Task 15) for each specific window, as it is data-dependent.\n",
        "    # This step is therefore a conceptual preparation.\n",
        "    logging.info(\"Step 2/3: Multi-start seed generation strategy is defined.\")\n",
        "\n",
        "    # --- Step 3: Document the entire initialization strategy. ---\n",
        "    metadata_path = Path(log_dir) / \"lppl_initialization_metadata.json\"\n",
        "    _document_initialization_strategy(bounds, study_parameters, metadata_path)\n",
        "    logging.info(\"Step 3/3: Initialization strategy documented successfully.\")\n",
        "\n",
        "    logging.info(\"LPPL fitter initialization pipeline finished successfully.\")\n",
        "\n",
        "    # Return the bounds and config needed for the next step.\n",
        "    return bounds, study_parameters\n"
      ],
      "metadata": {
        "id": "vjP1FqLXCjz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Fit the LPPL model to each window via constrained nonlinear least squares\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Fit the LPPL model to each window via constrained nonlinear\n",
        "#          least squares\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Define the LPPL objective function.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def lppl_objective_func(\n",
        "    theta: np.ndarray,\n",
        "    t: np.ndarray,\n",
        "    log_price: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the residuals for the LPPL model given a set of parameters.\n",
        "\n",
        "    This function is designed to be used with `scipy.optimize.least_squares`,\n",
        "    so it returns the vector of residuals (observed - predicted), not the\n",
        "    sum of squared errors.\n",
        "\n",
        "    Equation:\n",
        "    ln_p_hat = A + B*(t_c - t)^m + C*(t_c - t)^m * cos(omega*ln(t_c - t) + phi)\n",
        "    Residuals = log_price - ln_p_hat\n",
        "\n",
        "    Args:\n",
        "        theta (np.ndarray): A 7-element array of the LPPL parameters in the\n",
        "                            standard order: [A, B, C, m, omega, phi, t_c].\n",
        "        t (np.ndarray): A 1D array of time indices (e.g., 1 to W).\n",
        "        log_price (np.ndarray): A 1D array of observed log-prices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array of the residuals.\n",
        "    \"\"\"\n",
        "    # Unpack the parameter vector for clarity.\n",
        "    A, B, C, m, omega, phi, t_c = theta\n",
        "\n",
        "    # --- Robustness Check ---\n",
        "    # The optimizer might test t_c values that are inside the window.\n",
        "    # This would cause a domain error in np.log. We must prevent this.\n",
        "    # If t_c is not greater than all t, the parameters are invalid.\n",
        "    if t_c <= t.max():\n",
        "        # Return a large residual vector to penalize this invalid region.\n",
        "        return np.full(t.shape, 1e12)\n",
        "\n",
        "    # --- LPPL Equation Implementation ---\n",
        "    # Calculate the (t_c - t) term, which is used multiple times.\n",
        "    dt = t_c - t\n",
        "\n",
        "    # Calculate the predicted log-price using the LPPL formula.\n",
        "    # This is performed using vectorized numpy operations for efficiency.\n",
        "    log_p_hat = A + B * (dt**m) + C * (dt**m) * np.cos(omega * np.log(dt) + phi)\n",
        "\n",
        "    # Return the vector of residuals.\n",
        "    return log_price - log_p_hat\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 2 & 3: Run optimization and select the best fit.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _fit_single_window(\n",
        "    window: LPPLWindow,\n",
        "    bounds: Tuple[List[float], List[float]],\n",
        "    num_seeds: int = 10\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fits the LPPL model to a single window of data using a multi-start,\n",
        "    constrained non-linear least squares optimization.\n",
        "\n",
        "    Args:\n",
        "        window (LPPLWindow): The metadata and data for the window to fit.\n",
        "        bounds (Tuple[List[float], List[float]]): Lower and upper bounds for the\n",
        "                                                  7 LPPL parameters.\n",
        "        num_seeds (int): The number of initial guesses to try.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: A dictionary containing the best-fit parameters\n",
        "                                  and final SSE if a valid fit is found,\n",
        "                                  otherwise None.\n",
        "    \"\"\"\n",
        "    # Extract the data needed for fitting from the window object.\n",
        "    log_price_series = window.log_price_series\n",
        "    t_vector = log_price_series.index.to_numpy()\n",
        "    log_price_vector = log_price_series.to_numpy()\n",
        "    window_size = len(log_price_series)\n",
        "\n",
        "    # Generate a set of random initial guesses for this specific window.\n",
        "    initial_seeds = generate_lppl_initial_seeds(log_price_series, window_size, num_seeds)\n",
        "\n",
        "    # This list will store the results of all successful optimization runs.\n",
        "    successful_fits: List[OptimizeResult] = []\n",
        "\n",
        "    # --- Multi-Start Optimization Loop ---\n",
        "    for i, seed in enumerate(initial_seeds):\n",
        "        try:\n",
        "            # Run the constrained non-linear least squares optimization.\n",
        "            result = least_squares(\n",
        "                fun=lppl_objective_func,\n",
        "                x0=seed,\n",
        "                args=(t_vector, log_price_vector),\n",
        "                bounds=bounds,\n",
        "                method='trf', # Trust Region Reflective is suitable for bounds.\n",
        "                ftol=1e-8,\n",
        "                xtol=1e-8,\n",
        "                max_nfev=1000\n",
        "            )\n",
        "            # If the optimizer reports success, add the result to our list.\n",
        "            if result.success:\n",
        "                successful_fits.append(result)\n",
        "        except Exception:\n",
        "            # Silently ignore optimization failures for a single seed.\n",
        "            # This is expected as some seeds may be in difficult regions.\n",
        "            continue\n",
        "\n",
        "    # --- Best Fit Selection ---\n",
        "    # If no optimization runs converged, this window has failed.\n",
        "    if not successful_fits:\n",
        "        return None\n",
        "\n",
        "    # Find the best fit by selecting the one with the lowest cost (residual sum of squares).\n",
        "    # The `cost` from least_squares is 0.5 * sum(residuals^2).\n",
        "    best_fit = min(successful_fits, key=lambda r: r.cost)\n",
        "\n",
        "    # The Sum of Squared Errors (SSE) is 2 * cost.\n",
        "    sse = 2 * best_fit.cost\n",
        "\n",
        "    # Structure the final results into a dictionary.\n",
        "    fit_results = {param: value for param, value in zip(LPPL_PARAM_ORDER, best_fit.x)}\n",
        "    fit_results['SSE'] = sse\n",
        "    fit_results['Ticker'] = window.ticker\n",
        "    fit_results['End_Date'] = window.end_date\n",
        "\n",
        "    return fit_results\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def fit_lppl_model_to_windows(\n",
        "    windows: List[LPPLWindow],\n",
        "    bounds: Tuple[List[float], List[float]],\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_path: Union[str, Path] = \"data_intermediate/lppl_fit_parameters.csv\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the fitting of the LPPL model to all generated rolling windows.\n",
        "\n",
        "    This function is idempotent. If the output file exists, it loads the results.\n",
        "    Otherwise, it iterates through each window, performs a multi-start\n",
        "    constrained optimization, selects the best fit, and aggregates all results.\n",
        "\n",
        "    Args:\n",
        "        windows (List[LPPLWindow]): The list of all windows to be fitted.\n",
        "        bounds (Tuple[List[float], List[float]]): Parameter bounds from Task 14.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_path (Union[str, Path]): The path to save the final CSV of fit\n",
        "                                        parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the best-fit LPPL parameters and\n",
        "                      SSE for each successfully fitted window.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating LPPL model fitting pipeline...\")\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # --- Idempotency Check ---\n",
        "    if output_path.exists():\n",
        "        logging.info(f\"Found existing LPPL fit results at '{output_path}'. Loading from file.\")\n",
        "        return pd.read_csv(output_path, parse_dates=['End_Date'])\n",
        "\n",
        "    if not windows:\n",
        "        logging.warning(\"Window list is empty. No LPPL fitting will be performed.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # This list will store the dictionary of results for each successful fit.\n",
        "    all_fit_results = []\n",
        "\n",
        "    # --- Main Fitting Loop ---\n",
        "    # Use tqdm for a progress bar, as this is a very long-running process.\n",
        "    for window in tqdm(windows, desc=\"Fitting LPPL to windows\"):\n",
        "        # Fit the model to the current window.\n",
        "        best_fit = _fit_single_window(window, bounds)\n",
        "\n",
        "        # If a successful fit was found, add it to our results list.\n",
        "        if best_fit is not None:\n",
        "            all_fit_results.append(best_fit)\n",
        "\n",
        "    # --- Result Aggregation and Persistence ---\n",
        "    if not all_fit_results:\n",
        "        logging.warning(\"LPPL fitting completed, but no windows converged successfully.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert the list of dictionaries into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame(all_fit_results)\n",
        "\n",
        "    # Reorder columns for clarity.\n",
        "    column_order = ['Ticker', 'End_Date'] + LPPL_PARAM_ORDER + ['SSE']\n",
        "    results_df = results_df[column_order]\n",
        "\n",
        "    # Save the final DataFrame to a CSV file.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Saving {len(results_df)} successful LPPL fits to '{output_path}'.\")\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "\n",
        "    # Log summary statistics.\n",
        "    success_rate = len(results_df) / len(windows)\n",
        "    logging.info(f\"LPPL fitting pipeline finished. Success rate: {success_rate:.2%}\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "S6eDycqTFYXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Compute residuals and normalize to obtain ε_norm(t)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Compute residuals and normalize to obtain ε_norm(t)\n",
        "# ==============================================================================\n",
        "\n",
        "# Standardized order of LPPL parameters.\n",
        "LPPL_PARAM_ORDER = ['A', 'B', 'C', 'm', 'omega', 'phi', 't_c']\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Compute the raw residuals ε(t) for each window.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _lppl_predict(theta: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the predicted log-price using the LPPL model formula.\n",
        "\n",
        "    This is a helper function to reconstruct the fitted LPPL trajectory.\n",
        "\n",
        "    Args:\n",
        "        theta (np.ndarray): The 7-element array of fitted LPPL parameters.\n",
        "        t (np.ndarray): A 1D array of time indices (1 to W).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array of the predicted log-prices.\n",
        "    \"\"\"\n",
        "    # Unpack the parameter vector.\n",
        "    A, B, C, m, omega, phi, t_c = theta\n",
        "\n",
        "    # Defensive check for numerical stability.\n",
        "    if t_c <= t.max():\n",
        "        return np.full(t.shape, np.nan) # Return NaNs if t_c is invalid\n",
        "\n",
        "    # Calculate the (t_c - t) term.\n",
        "    dt = t_c - t\n",
        "\n",
        "    # Calculate the predicted log-price using the LPPL formula.\n",
        "    log_p_hat = A + B * (dt**m) + C * (dt**m) * np.cos(omega * np.log(dt) + phi)\n",
        "\n",
        "    return log_p_hat\n",
        "\n",
        "\n",
        "def _compute_raw_residuals(\n",
        "    df_features: pd.DataFrame,\n",
        "    lppl_fits: pd.DataFrame,\n",
        "    window_size: int\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the raw LPPL residuals for all successfully fitted windows.\n",
        "\n",
        "    Equation: ε(t) = ln p(t) - ln p̂(t)\n",
        "\n",
        "    Args:\n",
        "        df_features (pd.DataFrame): The main DataFrame with 'Log_Price'.\n",
        "        lppl_fits (pd.DataFrame): DataFrame of fitted LPPL parameters from Task 15.\n",
        "        window_size (int): The size of the fitting window (W).\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series containing all raw residuals, indexed by the\n",
        "                   original (Date, Ticker) MultiIndex.\n",
        "    \"\"\"\n",
        "    logging.info(\"Computing raw residuals for all fitted windows...\")\n",
        "\n",
        "    # This list will hold Series objects of residuals for each window.\n",
        "    all_residuals: List[pd.Series] = []\n",
        "\n",
        "    # Group the main DataFrame by ticker for efficient slicing.\n",
        "    grouped_df = df_features.groupby(level='TICKER')\n",
        "\n",
        "    # Iterate through each successful fit in the parameters DataFrame.\n",
        "    for _, fit in tqdm(lppl_fits.iterrows(), total=len(lppl_fits), desc=\"Calculating Residuals\"):\n",
        "        # Extract the ticker and end date to identify the window.\n",
        "        ticker, end_date = fit['Ticker'], fit['End_Date']\n",
        "\n",
        "        # Retrieve the original log price series for this window.\n",
        "        try:\n",
        "            ticker_series = grouped_df.get_group(ticker)['Log_Price']\n",
        "            # Find the integer position of the end date.\n",
        "            end_idx_pos = ticker_series.index.get_loc((end_date, ticker))\n",
        "            # Slice the window using integer positions for speed and accuracy.\n",
        "            window_series = ticker_series.iloc[end_idx_pos - window_size + 1 : end_idx_pos + 1]\n",
        "        except (KeyError, IndexError):\n",
        "            continue # Skip if the window can't be reconstructed.\n",
        "\n",
        "        # Ensure the reconstructed window is valid.\n",
        "        if len(window_series) != window_size:\n",
        "            continue\n",
        "\n",
        "        # Extract the fitted parameters for this window.\n",
        "        theta = fit[LPPL_PARAM_ORDER].to_numpy()\n",
        "\n",
        "        # Reconstruct the time vector (1 to W).\n",
        "        t_vector = np.arange(1, window_size + 1)\n",
        "\n",
        "        # Calculate the predicted log prices for the window.\n",
        "        log_p_hat = _lppl_predict(theta, t_vector)\n",
        "\n",
        "        # Compute the raw residuals.\n",
        "        raw_residuals = window_series.values - log_p_hat\n",
        "\n",
        "        # Create a Series for these residuals with the original MultiIndex.\n",
        "        residuals_series = pd.Series(raw_residuals, index=window_series.index)\n",
        "        all_residuals.append(residuals_series)\n",
        "\n",
        "    if not all_residuals:\n",
        "        logging.warning(\"No raw residuals were computed. Check LPPL fits.\")\n",
        "        return pd.Series(dtype=np.float64)\n",
        "\n",
        "    # Concatenate all individual residual Series into a single Series.\n",
        "    # This may have duplicate index entries due to overlapping windows.\n",
        "    combined_residuals = pd.concat(all_residuals)\n",
        "\n",
        "    # For overlapping windows, multiple residual values exist for the same\n",
        "    # (Date, Ticker). We take the mean as a simple and robust way to resolve this.\n",
        "    final_residuals = combined_residuals.groupby(combined_residuals.index).mean()\n",
        "\n",
        "    return final_residuals\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Normalize residuals to the range [-1, 1].\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_residuals(raw_residuals: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Normalizes raw residuals using a running maximum of their absolute value.\n",
        "\n",
        "    Equations:\n",
        "    1. M(t) = max_{s <= t} |ε(s)|\n",
        "    2. ε_norm(t) = ε(t) / M(t)\n",
        "\n",
        "    Args:\n",
        "        raw_residuals (pd.Series): A Series of raw residuals indexed by\n",
        "                                   (Date, Ticker).\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series of normalized residuals in the range [-1, 1].\n",
        "    \"\"\"\n",
        "    logging.info(\"Normalizing residuals using a running maximum...\")\n",
        "\n",
        "    # Ensure the series is sorted by date for the expanding window to work correctly.\n",
        "    residuals_sorted = raw_residuals.sort_index()\n",
        "\n",
        "    # --- Calculate Running Maximum M(t) ---\n",
        "    # Group by ticker to compute the running max only within each security's history.\n",
        "    # .expanding() creates a window that grows from the beginning of each group.\n",
        "    running_max_abs_residual = residuals_sorted.abs().groupby(level='TICKER').expanding().max()\n",
        "\n",
        "    # The result of expanding is a MultiIndex Series. We need to drop the extra level\n",
        "    # to align it with our original residuals Series.\n",
        "    running_max_abs_residual = running_max_abs_residual.droplevel(0)\n",
        "\n",
        "    # --- Normalize ---\n",
        "    # Divide the raw residuals by the running maximum.\n",
        "    normalized_residuals = residuals_sorted / running_max_abs_residual\n",
        "\n",
        "    # Handle the case where the running max is zero (residuals are zero),\n",
        "    # which results in NaNs. These should be 0.\n",
        "    normalized_residuals.fillna(0, inplace=True)\n",
        "\n",
        "    return normalized_residuals\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Merge normalized residuals into the main DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_and_merge_lppl_residuals(\n",
        "    df_features: pd.DataFrame,\n",
        "    lppl_fits: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation and normalization of LPPL residuals and merges\n",
        "    them into the main feature DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df_features (pd.DataFrame): The main DataFrame with all prior features.\n",
        "        lppl_fits (pd.DataFrame): The DataFrame of fitted LPPL parameters.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new\n",
        "                      'Residual_Norm' column.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating LPPL residual computation pipeline...\")\n",
        "    # Work on a copy.\n",
        "    df_final = df_features.copy()\n",
        "\n",
        "    # --- Step 1: Compute raw residuals for all windows. ---\n",
        "    window_size = study_parameters['descriptive_model']['lppl_fitting']['rolling_window_size']\n",
        "    raw_residuals = _compute_raw_residuals(df_final, lppl_fits, window_size)\n",
        "    logging.info(f\"Step 1/3: Computed raw residuals for {len(raw_residuals)} data points.\")\n",
        "\n",
        "    if raw_residuals.empty:\n",
        "        logging.warning(\"Raw residuals Series is empty. Adding a NaN column for 'Residual_Norm'.\")\n",
        "        df_final['Residual_Norm'] = np.nan\n",
        "        return df_final\n",
        "\n",
        "    # --- Step 2: Normalize the raw residuals. ---\n",
        "    normalized_residuals = _normalize_residuals(raw_residuals)\n",
        "    logging.info(\"Step 2/3: Normalized residuals successfully.\")\n",
        "\n",
        "    # --- Step 3: Merge the normalized residuals into the main DataFrame. ---\n",
        "    # Assigning the Series as a new column will automatically align by the index.\n",
        "    # Points in the DataFrame that don't have a residual will get NaN, which is correct.\n",
        "    df_final['Residual_Norm'] = normalized_residuals\n",
        "    logging.info(\"Step 3/3: Merged normalized residuals into the main DataFrame.\")\n",
        "\n",
        "    # --- Final Logging ---\n",
        "    coverage = df_final['Residual_Norm'].notna().sum() / len(df_final)\n",
        "    logging.info(f\"LPPL residual computation pipeline finished. Coverage: {coverage:.2%}\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "DdCQEQlOG6KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Construct the BubbleScore by fusing residuals with behavioral signals\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Construct the BubbleScore by fusing residuals with behavioral signals\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Extract the BubbleScore weights from configuration.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_bubblescore_weights(config: Dict[str, Any]) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Retrieves the weights for the Hype and Sentiment components of the BubbleScore.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing (alpha_1_hype_weight,\n",
        "                             alpha_2_sentiment_weight).\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required weight keys are missing from the configuration.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested dictionary for bubble score synthesis parameters.\n",
        "        synthesis_config = config['descriptive_model']['bubble_score_synthesis']\n",
        "\n",
        "        # Extract the weight for the Hype Index component.\n",
        "        alpha_1 = synthesis_config['alpha_1_hype_weight']\n",
        "\n",
        "        # Extract the weight for the Sentiment Score component.\n",
        "        alpha_2 = synthesis_config['alpha_2_sentiment_weight']\n",
        "\n",
        "        # The validity of these parameters was confirmed in Task 1.\n",
        "        logging.info(f\"BubbleScore weights extracted: alpha_1 (Hype) = {alpha_1}, alpha_2 (Sentiment) = {alpha_2}.\")\n",
        "\n",
        "        return alpha_1, alpha_2\n",
        "    except KeyError as e:\n",
        "        # This error indicates a problem with the configuration structure.\n",
        "        logging.error(f\"Missing a required BubbleScore weight in the configuration: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Compute the BubbleScore using the regime-aware formula.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_bubblescore(\n",
        "    df: pd.DataFrame,\n",
        "    alpha_1: float,\n",
        "    alpha_2: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the BubbleScore by fusing the LPPL residual with behavioral signals.\n",
        "\n",
        "    This function implements the core piecewise formula from the paper, which\n",
        "    treats the amplifying effect of hype asymmetrically based on the sign of\n",
        "    the LPPL residual (the \"regime\").\n",
        "\n",
        "    Equation (14):\n",
        "    - If ε_norm > 0: BubbleScore = ε_norm + α1*Hype + α2*Sentiment\n",
        "    - If ε_norm <= 0: BubbleScore = ε_norm - α1*Hype + α2*Sentiment\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing 'Residual_Norm', 'Hype_Index',\n",
        "                           and 'Sentiment_Score'.\n",
        "        alpha_1 (float): The weight for the Hype Index.\n",
        "        alpha_2 (float): The weight for the Sentiment Score.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the input DataFrame with the new 'BubbleScore'\n",
        "                      column.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects.\n",
        "    df_scored = df.copy()\n",
        "\n",
        "    # --- Identify the Regime ---\n",
        "    # Create a boolean mask to identify the \"bubble\" regime (positive residual).\n",
        "    # Where this is False, it's the \"negative behavior\" regime.\n",
        "    is_bubble_regime = df_scored['Residual_Norm'] > 0\n",
        "\n",
        "    # --- Calculate Components ---\n",
        "    # For clarity, calculate the value of each term in the equation.\n",
        "    residual_term = df_scored['Residual_Norm']\n",
        "    hype_term = alpha_1 * df_scored['Hype_Index']\n",
        "    sentiment_term = alpha_2 * df_scored['Sentiment_Score']\n",
        "\n",
        "    # --- Apply Piecewise Formula using np.where ---\n",
        "    # This is a highly efficient, vectorized way to apply conditional logic.\n",
        "    # np.where(condition, value_if_true, value_if_false)\n",
        "    bubble_score = np.where(\n",
        "        is_bubble_regime,\n",
        "        # Formula for the positive regime (ε_norm > 0)\n",
        "        residual_term + hype_term + sentiment_term,\n",
        "        # Formula for the negative regime (ε_norm <= 0)\n",
        "        residual_term - hype_term + sentiment_term\n",
        "    )\n",
        "\n",
        "    # Assign the computed array to the new 'BubbleScore' column.\n",
        "    # NaNs in any component will correctly propagate to the result.\n",
        "    df_scored['BubbleScore'] = bubble_score\n",
        "\n",
        "    return df_scored\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Validate and persist the BubbleScore series.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_and_persist_bubblescore(\n",
        "    df: pd.DataFrame,\n",
        "    output_path: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the computed BubbleScore and saves a snapshot for auditing.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'BubbleScore' column.\n",
        "        output_path (Path): The file path to save the CSV snapshot.\n",
        "    \"\"\"\n",
        "    # --- Validation ---\n",
        "    # Check for any non-finite values (inf, -inf) that could indicate errors.\n",
        "    if not np.all(np.isfinite(df['BubbleScore'].dropna())):\n",
        "        raise ValueError(\"BubbleScore column contains non-finite values (inf/-inf). Check input data and weights.\")\n",
        "\n",
        "    # Log summary statistics for the final signal.\n",
        "    logging.info(\"BubbleScore computed successfully. Summary statistics:\")\n",
        "    logging.info(df['BubbleScore'].describe().to_string())\n",
        "\n",
        "    # --- Persistence for Auditing ---\n",
        "    # Define the columns relevant to the BubbleScore calculation for the snapshot.\n",
        "    snapshot_cols = ['BubbleScore', 'Residual_Norm', 'Hype_Index', 'Sentiment_Score']\n",
        "\n",
        "    # Ensure the parent directory exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save the snapshot to a CSV file.\n",
        "    logging.info(f\"Saving BubbleScore component snapshot to '{output_path}' for audit.\")\n",
        "    df[snapshot_cols].to_csv(output_path)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_bubblescore(\n",
        "    df_residuals: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_dir: Union[str, Path] = \"data_intermediate\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the final BubbleScore signal.\n",
        "\n",
        "    This function fuses the technical LPPL residual with the behavioral Hype\n",
        "    and Sentiment signals according to the paper's regime-dependent formula.\n",
        "\n",
        "    Args:\n",
        "        df_residuals (pd.DataFrame): The DataFrame from Task 16, containing\n",
        "            'Residual_Norm' and all prior behavioral features.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_dir (Union[str, Path]): Directory to save the audit snapshot.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame enriched with the final 'BubbleScore' column.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating BubbleScore construction pipeline...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    required_cols = ['Residual_Norm', 'Hype_Index', 'Sentiment_Score']\n",
        "    if not all(col in df_residuals.columns for col in required_cols):\n",
        "        raise ValueError(f\"Input DataFrame is missing one or more required columns for BubbleScore construction: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Extract the BubbleScore weights from configuration. ---\n",
        "    alpha_1, alpha_2 = _get_bubblescore_weights(study_parameters)\n",
        "    logging.info(\"Step 1/3: BubbleScore weights extracted.\")\n",
        "\n",
        "    # --- Step 2: Compute the BubbleScore using the regime-aware formula. ---\n",
        "    df_scored = _compute_bubblescore(df_residuals, alpha_1, alpha_2)\n",
        "    logging.info(\"Step 2/3: BubbleScore computed using regime-aware formula.\")\n",
        "\n",
        "    # --- Step 3: Validate and persist a snapshot of the BubbleScore. ---\n",
        "    snapshot_path = Path(output_dir) / \"bubblescore_snapshot.csv\"\n",
        "    _validate_and_persist_bubblescore(df_scored, snapshot_path)\n",
        "    logging.info(\"Step 3/3: BubbleScore validated and snapshot persisted.\")\n",
        "\n",
        "    logging.info(\"BubbleScore construction pipeline finished successfully.\")\n",
        "\n",
        "    return df_scored\n"
      ],
      "metadata": {
        "id": "KH7FOVrKIVSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Label bubble and negative-bubble episodes via thresholding and persistence\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Label bubble and negative-bubble episodes via thresholding and\n",
        "#          persistence\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Extract episode detection parameters from configuration.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_episode_detection_params(config: Dict[str, Any]) -> Tuple[float, int]:\n",
        "    \"\"\"\n",
        "    Retrieves the parameters for episode detection from the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, int]: A tuple containing (tau_threshold, d_min_duration).\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required keys are missing from the configuration.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested dictionary for episode labeling parameters.\n",
        "        labeling_config = config['descriptive_model']['episode_labeling']\n",
        "\n",
        "        # Extract the significance threshold 'tau'.\n",
        "        tau = labeling_config['significance_threshold_tau']\n",
        "\n",
        "        # Extract the minimum duration 'd_min'.\n",
        "        d_min = labeling_config['min_duration_d_min']\n",
        "\n",
        "        logging.info(f\"Episode detection parameters extracted: tau = {tau}, d_min = {d_min} days.\")\n",
        "\n",
        "        return tau, d_min\n",
        "    except KeyError as e:\n",
        "        # This error indicates a problem with the configuration structure.\n",
        "        logging.error(f\"Missing a required episode labeling parameter in the configuration: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Step 2: Identify raw episodes by thresholding.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _identify_episodes_vectorized(\n",
        "    df: pd.DataFrame,\n",
        "    tau: float,\n",
        "    d_min: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies bubble and negative-bubble episodes using a vectorized approach.\n",
        "\n",
        "    This function uses an efficient pandas algorithm based on detecting state\n",
        "    changes to identify contiguous blocks of significant BubbleScore values that\n",
        "    meet the minimum duration requirement.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the 'BubbleScore' column.\n",
        "        tau (float): The significance threshold for the BubbleScore.\n",
        "        d_min (int): The minimum number of consecutive days for an episode.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row represents a valid episode,\n",
        "                      with columns for Ticker, Start_Date, End_Date, Type,\n",
        "                      and Intensity.\n",
        "    \"\"\"\n",
        "    logging.info(\"Identifying bubble episodes using vectorized state-change detection...\")\n",
        "\n",
        "    # Work on a temporary DataFrame with only the necessary data.\n",
        "    temp_df = df[['BubbleScore']].copy()\n",
        "\n",
        "    # --- Step A: Define the state for each day ---\n",
        "    # State is 1 for a positive bubble, -1 for a negative bubble, 0 otherwise.\n",
        "    temp_df['State'] = np.where(\n",
        "        temp_df['BubbleScore'] > tau, 1,\n",
        "        np.where(temp_df['BubbleScore'] < -tau, -1, 0)\n",
        "    )\n",
        "\n",
        "    # --- Step B: Detect state changes ---\n",
        "    # A change occurs if the current state is different from the previous day's state for the same ticker.\n",
        "    # .ne(0) converts the diff result (e.g., 1, -1, 2, -2) to a boolean.\n",
        "    temp_df['State_Change'] = temp_df.groupby(level='TICKER')['State'].diff().ne(0)\n",
        "\n",
        "    # --- Step C: Assign a unique ID to each contiguous episode block ---\n",
        "    # The cumulative sum of state changes creates a unique ID for each block.\n",
        "    temp_df['Episode_ID'] = temp_df['State_Change'].cumsum()\n",
        "\n",
        "    # --- Step D: Aggregate by episode ID ---\n",
        "    # Group by Ticker and the newly created Episode_ID.\n",
        "    episodes = temp_df.groupby(['TICKER', 'Episode_ID']).agg(\n",
        "        Start_Date=('BubbleScore', lambda x: x.index[0][0]), # Get date from MultiIndex\n",
        "        End_Date=('BubbleScore', lambda x: x.index[-1][0]),\n",
        "        Duration=('BubbleScore', 'size'),\n",
        "        Type=('State', 'first'), # State is constant within the group\n",
        "        Intensity=('BubbleScore', lambda x: x.abs().max())\n",
        "    )\n",
        "\n",
        "    # --- Step E: Filter for valid episodes ---\n",
        "    # A valid episode must have a non-zero type (i.e., not a neutral period)\n",
        "    # and a duration greater than or equal to the minimum requirement.\n",
        "    valid_episodes = episodes[\n",
        "        (episodes['Type'] != 0) & (episodes['Duration'] >= d_min)\n",
        "    ].reset_index()\n",
        "\n",
        "    # Clean up the final DataFrame.\n",
        "    valid_episodes.drop(columns=['Episode_ID'], inplace=True)\n",
        "    # Convert Type from number to a more descriptive string.\n",
        "    valid_episodes['Type'] = valid_episodes['Type'].map({1: 'Normal', -1: 'Negative'})\n",
        "\n",
        "    return valid_episodes\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Persist episode labels and create binary indicators.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_episodes_and_create_indicators(\n",
        "    df: pd.DataFrame,\n",
        "    episodes_df: pd.DataFrame,\n",
        "    output_path: Path\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Saves the detected episodes to a CSV and adds binary indicator columns\n",
        "    to the main DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The main DataFrame to add indicators to.\n",
        "        episodes_df (pd.DataFrame): The DataFrame of detected episodes.\n",
        "        output_path (Path): The file path to save the episodes CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the main DataFrame with two new binary\n",
        "                      indicator columns.\n",
        "    \"\"\"\n",
        "    # Work on a copy.\n",
        "    df_labeled = df.copy()\n",
        "\n",
        "    # --- Persistence ---\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Saving {len(episodes_df)} detected episodes to '{output_path}'.\")\n",
        "    episodes_df.to_csv(output_path, index=False)\n",
        "\n",
        "    # --- Create Binary Indicators ---\n",
        "    logging.info(\"Creating binary episode indicators in the main DataFrame...\")\n",
        "    # Initialize the new columns with 0.\n",
        "    df_labeled['In_Bubble_Episode'] = 0\n",
        "    df_labeled['In_Negative_Episode'] = 0\n",
        "\n",
        "    # Iterate through the (much smaller) episodes DataFrame to label the main DataFrame.\n",
        "    for _, episode in episodes_df.iterrows():\n",
        "        # Define the slice for the current episode using the MultiIndex.\n",
        "        # The slice selects all dates between Start_Date and End_Date for the specific Ticker.\n",
        "        idx_slice = (slice(episode['Start_Date'], episode['End_Date']), episode['Ticker'])\n",
        "\n",
        "        # Assign 1 to the appropriate indicator column for the sliced rows.\n",
        "        if episode['Type'] == 'Normal':\n",
        "            df_labeled.loc[idx_slice, 'In_Bubble_Episode'] = 1\n",
        "        elif episode['Type'] == 'Negative':\n",
        "            df_labeled.loc[idx_slice, 'In_Negative_Episode'] = 1\n",
        "\n",
        "    return df_labeled\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def label_bubble_episodes(\n",
        "    df_scored: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_dir: Union[str, Path] = \"data_intermediate\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the process of identifying and labeling bubble episodes.\n",
        "\n",
        "    This function translates the continuous BubbleScore into discrete event\n",
        "    windows (episodes) based on magnitude and duration thresholds.\n",
        "\n",
        "    Args:\n",
        "        df_scored (pd.DataFrame): The DataFrame from Task 17, containing the\n",
        "                                  'BubbleScore' column.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_dir (Union[str, Path]): Directory to save the episodes CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame enriched with two new binary indicator\n",
        "                      columns: 'In_Bubble_Episode' and 'In_Negative_Episode'.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating bubble episode labeling pipeline...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'BubbleScore' not in df_scored.columns:\n",
        "        raise ValueError(\"Input DataFrame is missing the required 'BubbleScore' column.\")\n",
        "\n",
        "    # --- Step 1: Extract episode detection parameters. ---\n",
        "    tau, d_min = _get_episode_detection_params(study_parameters)\n",
        "    logging.info(\"Step 1/3: Episode detection parameters extracted.\")\n",
        "\n",
        "    # --- Step 2: Identify all valid episodes using the vectorized method. ---\n",
        "    episodes_df = _identify_episodes_vectorized(df_scored, tau, d_min)\n",
        "    logging.info(f\"Step 2/3: Identified {len(episodes_df)} valid bubble/negative-bubble episodes.\")\n",
        "\n",
        "    # --- Step 3: Persist episodes and create binary indicators. ---\n",
        "    episodes_filepath = Path(output_dir) / \"bubble_episodes.csv\"\n",
        "    df_final = _persist_episodes_and_create_indicators(df_scored, episodes_df, episodes_filepath)\n",
        "    logging.info(\"Step 3/3: Episode list persisted and binary indicators created.\")\n",
        "\n",
        "    logging.info(\"Bubble episode labeling pipeline finished successfully.\")\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "DLciQG-pJk9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Engineer stock-level feature sequences for the Transformer\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Engineer stock-level feature sequences for the Transformer\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Select and normalize stock-level features.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_stock_features(\n",
        "    df: pd.DataFrame,\n",
        "    train_end_date: pd.Timestamp\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Selects and normalizes stock-level features using Z-score scaling.\n",
        "\n",
        "    Crucially, the scaling parameters (mean and standard deviation) are\n",
        "    calculated ONLY on the training portion of the data to prevent data leakage.\n",
        "    These parameters are then applied to the entire dataset.\n",
        "\n",
        "    Equation: x_norm = (x - μ_train) / σ_train\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The full feature DataFrame.\n",
        "        train_end_date (pd.Timestamp): The last date of the training period.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]: A tuple containing:\n",
        "            - A copy of the DataFrame with normalized features.\n",
        "            - A dictionary of the fitted scalers (mean and std for each column).\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid modifying the original DataFrame.\n",
        "    df_normalized = df.copy()\n",
        "\n",
        "    # Define the list of stock-level features to be processed.\n",
        "    stock_features = [\n",
        "        'Log_Price', 'Log_Volume', 'Log_Return',\n",
        "        'PE_Ratio', 'PB_Ratio', 'Month', 'Day'\n",
        "    ]\n",
        "    # Define which of these features require normalization (i.e., are continuous).\n",
        "    continuous_features = ['Log_Price', 'Log_Volume', 'Log_Return', 'PE_Ratio', 'PB_Ratio']\n",
        "\n",
        "    # --- Isolate the Training Set for Fitting Scalers ---\n",
        "    # This is the critical step to prevent data leakage from the validation/test sets.\n",
        "    train_df = df_normalized.loc[df_normalized.index.get_level_values('Date') <= train_end_date]\n",
        "\n",
        "    logging.info(f\"Fitting normalization scalers using training data up to {train_end_date.date()}...\")\n",
        "\n",
        "    # This dictionary will store the calculated mean and std for each feature.\n",
        "    scalers: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    for col in continuous_features:\n",
        "        # Calculate mean and standard deviation from the training data.\n",
        "        mean = train_df[col].mean()\n",
        "        std = train_df[col].std()\n",
        "\n",
        "        # --- Robustness Check for Zero Standard Deviation ---\n",
        "        # If std is zero or very close to it, the feature is constant.\n",
        "        if std < 1e-8:\n",
        "            logging.warning(f\"Feature '{col}' has near-zero standard deviation in the training set. It will not be scaled.\")\n",
        "            # Store scalers that result in a transform of all zeros.\n",
        "            scalers[col] = {'mean': mean, 'std': np.inf} # Division by inf -> 0\n",
        "        else:\n",
        "            # Store the valid scalers.\n",
        "            scalers[col] = {'mean': mean, 'std': std}\n",
        "\n",
        "            # --- Apply the Transformation to the Entire Dataset ---\n",
        "            # Apply the z-score normalization using the training set parameters.\n",
        "            df_normalized[col] = (df_normalized[col] - mean) / std\n",
        "\n",
        "    logging.info(\"Stock-level features normalized successfully.\")\n",
        "    return df_normalized, scalers\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Handle missing values in fundamental features.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _handle_missing_fundamental_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Handles missing values in fundamental feature columns by dropping rows.\n",
        "\n",
        "    This function implements the \"drop if missing\" policy for 'PE_Ratio' and\n",
        "    'PB_Ratio' as specified in the paper's methodology.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with normalized features.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with rows containing NaNs in fundamental\n",
        "                      columns removed.\n",
        "    \"\"\"\n",
        "    # Columns to check for missing values.\n",
        "    fundamental_cols = ['PE_Ratio', 'PB_Ratio']\n",
        "\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # Drop rows where any of the specified fundamental columns have NaN values.\n",
        "    df_cleaned = df.dropna(subset=fundamental_cols)\n",
        "\n",
        "    final_rows = len(df_cleaned)\n",
        "    rows_dropped = initial_rows - final_rows\n",
        "\n",
        "    if rows_dropped > 0:\n",
        "        logging.info(f\"Dropped {rows_dropped:,} rows ({rows_dropped/initial_rows:.2%}) due to missing fundamental data ('PE_Ratio', 'PB_Ratio').\")\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Construct stock-level sequences.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_sequences_for_ticker(\n",
        "    ticker_df: pd.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    sequence_length: int\n",
        ") -> Tuple[List[np.ndarray], pd.MultiIndex]:\n",
        "    \"\"\"\n",
        "    Constructs all possible fixed-length sequences for a single ticker.\n",
        "\n",
        "    Args:\n",
        "        ticker_df (pd.DataFrame): The DataFrame for a single ticker, sorted by date.\n",
        "        feature_cols (List[str]): The names of the columns to include in the sequences.\n",
        "        sequence_length (int): The desired length of each sequence (L).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], pd.MultiIndex]: A tuple containing:\n",
        "            - A list of 2D numpy arrays, each of shape (L, d_s).\n",
        "            - The MultiIndex corresponding to the end-date of each sequence.\n",
        "    \"\"\"\n",
        "    # Convert the relevant feature columns to a numpy array for efficient slicing.\n",
        "    feature_matrix = ticker_df[feature_cols].to_numpy()\n",
        "    num_obs, num_features = feature_matrix.shape\n",
        "\n",
        "    # This list will store the generated sequence arrays.\n",
        "    sequences = []\n",
        "\n",
        "    # The number of possible sequences is num_obs - sequence_length + 1.\n",
        "    for i in range(num_obs - sequence_length + 1):\n",
        "        # Slice the feature matrix to create a sequence of length L.\n",
        "        sequence = feature_matrix[i : i + sequence_length]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    # Get the index labels for the end-points of each sequence.\n",
        "    anchor_indices = ticker_df.index[sequence_length - 1:]\n",
        "\n",
        "    return sequences, anchor_indices\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def engineer_stock_level_sequences(\n",
        "    df_labeled: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> Tuple[List[np.ndarray], pd.MultiIndex, Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full pipeline for creating stock-level feature sequences.\n",
        "\n",
        "    This function prepares the primary input for the stock-specific stream of\n",
        "    the Dual-Stream Transformer. The pipeline includes:\n",
        "    1.  Selecting and normalizing features, crucially using only training data\n",
        "        to fit the scalers to prevent data leakage.\n",
        "    2.  Handling missing values in fundamental ratio columns by dropping rows.\n",
        "    3.  Constructing fixed-length sequences for each ticker.\n",
        "\n",
        "    Args:\n",
        "        df_labeled (pd.DataFrame): The full feature DataFrame from Task 18.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], pd.MultiIndex, Dict[str, Dict[str, float]]]:\n",
        "            - A list of all generated stock-level sequences (numpy arrays).\n",
        "            - A pandas MultiIndex aligning each sequence to its anchor (Ticker, Date).\n",
        "            - A dictionary containing the fitted normalization scalers.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating stock-level sequence engineering pipeline...\")\n",
        "\n",
        "    # --- Define Split Date for Normalization ---\n",
        "    # This is essential to prevent data leakage.\n",
        "    all_dates = df_labeled.index.get_level_values('Date').unique().sort_values()\n",
        "    split_ratio = study_parameters['predictive_model']['data_preparation']['dataset_split_ratio']['train']\n",
        "    train_end_idx = int(len(all_dates) * split_ratio)\n",
        "    train_end_date = all_dates[train_end_idx]\n",
        "\n",
        "    # --- Step 1: Select and normalize stock-level features. ---\n",
        "    df_normalized, scalers = _normalize_stock_features(df_labeled, train_end_date)\n",
        "    logging.info(\"Step 1/3: Stock-level features normalized.\")\n",
        "\n",
        "    # --- Step 2: Handle missing values in fundamental features. ---\n",
        "    df_cleaned = _handle_missing_fundamental_features(df_normalized)\n",
        "    logging.info(\"Step 2/3: Missing fundamental data handled.\")\n",
        "\n",
        "    # --- Step 3: Construct fixed-length sequences for each ticker. ---\n",
        "    sequence_length = study_parameters['predictive_model']['data_preparation']['sequence_length']\n",
        "    feature_cols = [\n",
        "        'Log_Price', 'Log_Volume', 'Log_Return', 'PE_Ratio', 'PB_Ratio', 'Month', 'Day'\n",
        "    ]\n",
        "\n",
        "    all_sequences: List[np.ndarray] = []\n",
        "    all_anchor_indices: List[pd.MultiIndex] = []\n",
        "\n",
        "    # Group by ticker and apply the sequence generation function to each group.\n",
        "    logging.info(f\"Constructing sequences of length {sequence_length} for each ticker...\")\n",
        "    grouped = df_cleaned[feature_cols].groupby(level='TICKER')\n",
        "\n",
        "    for _, ticker_df in tqdm(grouped, desc=\"Generating Stock Sequences\"):\n",
        "        if len(ticker_df) >= sequence_length:\n",
        "            sequences, anchor_indices = _construct_sequences_for_ticker(\n",
        "                ticker_df, feature_cols, sequence_length\n",
        "            )\n",
        "            all_sequences.extend(sequences)\n",
        "            all_anchor_indices.append(anchor_indices)\n",
        "\n",
        "    if not all_anchor_indices:\n",
        "        raise ValueError(\"No sequences could be generated. Check sequence_length and data availability.\")\n",
        "\n",
        "    # Concatenate the indices from all tickers into a single MultiIndex.\n",
        "    final_anchor_indices = pd.MultiIndex.from_tuples(\n",
        "        [idx for multi_idx in all_anchor_indices for idx in multi_idx]\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Step 3/3: Successfully generated {len(all_sequences):,} stock-level sequences.\")\n",
        "    logging.info(\"Stock-level sequence engineering pipeline finished successfully.\")\n",
        "\n",
        "    return all_sequences, final_anchor_indices, scalers\n"
      ],
      "metadata": {
        "id": "UxvketaxK_N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Engineer market-level feature sequences for the Transformer\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Engineer market-level feature sequences for the Transformer\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Select and normalize market-level features.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_market_features(\n",
        "    df: pd.DataFrame,\n",
        "    train_end_date: pd.Timestamp\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Selects, de-duplicates, and normalizes market-level features.\n",
        "\n",
        "    This function first creates a compact, date-indexed DataFrame of market\n",
        "    features. It then applies Z-score normalization to continuous variables\n",
        "    (like VIX), fitting the scaler ONLY on the training data to prevent leakage.\n",
        "    Probabilistic features (sentiment shares, Hype Index) are not scaled.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The full feature DataFrame.\n",
        "        train_end_date (pd.Timestamp): The last date of the training period.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]: A tuple containing:\n",
        "            - A date-indexed DataFrame of normalized market features.\n",
        "            - A dictionary of the fitted scalers.\n",
        "    \"\"\"\n",
        "    # Define the list of market-level features.\n",
        "    market_features_cols = [\n",
        "        'VIX_Close', 'Hype_Index', 'Market_Sentiment_Neg',\n",
        "        'Market_Sentiment_Neu', 'Market_Sentiment_Pos'\n",
        "    ]\n",
        "    # Define which of these are continuous and require scaling.\n",
        "    continuous_market_features = ['VIX_Close']\n",
        "\n",
        "    # --- Create a Compact, Date-Indexed DataFrame of Market Features ---\n",
        "    # This is highly efficient as it avoids processing redundant data.\n",
        "    # We select the columns, get the date level of the index, and drop duplicates.\n",
        "    market_features_df = df[market_features_cols].copy()\n",
        "    market_features_df = market_features_df.reset_index(level='TICKER', drop=True)\n",
        "    market_features_df = market_features_df[~market_features_df.index.duplicated(keep='first')]\n",
        "\n",
        "    # --- Fit Scalers on Training Data Only ---\n",
        "    train_market_df = market_features_df.loc[market_features_df.index <= train_end_date]\n",
        "    logging.info(f\"Fitting market feature scalers using training data up to {train_end_date.date()}...\")\n",
        "\n",
        "    scalers: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    for col in continuous_market_features:\n",
        "        # Calculate mean and standard deviation from the training data.\n",
        "        mean = train_market_df[col].mean()\n",
        "        std = train_market_df[col].std()\n",
        "\n",
        "        # Handle constant features to avoid division by zero.\n",
        "        if std < 1e-8:\n",
        "            logging.warning(f\"Market feature '{col}' has near-zero standard deviation. It will not be scaled.\")\n",
        "            scalers[col] = {'mean': mean, 'std': np.inf}\n",
        "        else:\n",
        "            scalers[col] = {'mean': mean, 'std': std}\n",
        "            # Apply the z-score transformation to the entire market feature DataFrame.\n",
        "            market_features_df[col] = (market_features_df[col] - mean) / std\n",
        "\n",
        "    logging.info(\"Market-level features selected and normalized successfully.\")\n",
        "    return market_features_df, scalers\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Construct market-level sequences.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_market_sequences(\n",
        "    market_features_df: pd.DataFrame,\n",
        "    sequence_length: int\n",
        ") -> Dict[pd.Timestamp, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs fixed-length sequences from the date-indexed market features.\n",
        "\n",
        "    This creates a dictionary mapping each valid anchor date to its\n",
        "    corresponding market sequence. This lookup map is highly efficient for\n",
        "    aligning with stock-level sequences later.\n",
        "\n",
        "    Args:\n",
        "        market_features_df (pd.DataFrame): Date-indexed DataFrame of market features.\n",
        "        sequence_length (int): The desired length of each sequence (L).\n",
        "\n",
        "    Returns:\n",
        "        Dict[pd.Timestamp, np.ndarray]: A dictionary mapping the anchor date\n",
        "                                        (end of sequence) to the sequence array.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Constructing market-level sequences of length {sequence_length}...\")\n",
        "\n",
        "    # Convert to numpy for efficient slicing.\n",
        "    feature_matrix = market_features_df.to_numpy()\n",
        "    dates = market_features_df.index\n",
        "    num_obs = len(dates)\n",
        "\n",
        "    market_sequence_map: Dict[pd.Timestamp, np.ndarray] = {}\n",
        "\n",
        "    # Iterate through all possible end-points of a sequence.\n",
        "    for i in range(sequence_length - 1, num_obs):\n",
        "        # The anchor date is the date at the end of the sequence.\n",
        "        anchor_date = dates[i]\n",
        "        # Slice the feature matrix to get the sequence.\n",
        "        sequence = feature_matrix[i - sequence_length + 1 : i + 1]\n",
        "        # Store the sequence in the map with its anchor date as the key.\n",
        "        market_sequence_map[anchor_date] = sequence\n",
        "\n",
        "    return market_sequence_map\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function (Step 3 is part of the final dataset assembly)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def engineer_market_level_sequences(\n",
        "    df_labeled: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> Tuple[Dict[pd.Timestamp, np.ndarray], Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of market-level feature sequences.\n",
        "\n",
        "    This function prepares the second input stream for the Dual-Stream\n",
        "    Transformer. The pipeline includes:\n",
        "    1.  Selecting, de-duplicating, and normalizing market-level features,\n",
        "        using only training data to fit scalers.\n",
        "    2.  Constructing fixed-length sequences for each valid anchor date and\n",
        "        storing them in an efficient lookup map.\n",
        "\n",
        "    Args:\n",
        "        df_labeled (pd.DataFrame): The full feature DataFrame from Task 18.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[pd.Timestamp, np.ndarray], Dict[str, Dict[str, float]]]:\n",
        "            - A dictionary mapping each anchor date to its market sequence array.\n",
        "            - A dictionary containing the fitted normalization scalers.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating market-level sequence engineering pipeline...\")\n",
        "\n",
        "    # --- Define Split Date for Normalization (Consistent with Task 19) ---\n",
        "    all_dates = df_labeled.index.get_level_values('Date').unique().sort_values()\n",
        "    split_ratio = study_parameters['predictive_model']['data_preparation']['dataset_split_ratio']['train']\n",
        "    train_end_idx = int(len(all_dates) * split_ratio)\n",
        "    train_end_date = all_dates[train_end_idx]\n",
        "\n",
        "    # --- Step 1: Select and normalize market-level features. ---\n",
        "    market_features_df, market_scalers = _normalize_market_features(df_labeled, train_end_date)\n",
        "    logging.info(\"Step 1/2: Market-level features normalized.\")\n",
        "\n",
        "    # --- Step 2: Construct market-level sequences. ---\n",
        "    sequence_length = study_parameters['predictive_model']['data_preparation']['sequence_length']\n",
        "    market_sequence_map = _construct_market_sequences(market_features_df, sequence_length)\n",
        "    logging.info(f\"Step 2/2: Successfully generated {len(market_sequence_map):,} market-level sequences.\")\n",
        "\n",
        "    logging.info(\"Market-level sequence engineering pipeline finished successfully.\")\n",
        "\n",
        "    return market_sequence_map, market_scalers\n"
      ],
      "metadata": {
        "id": "6IO113l9MZxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Construct target sequences for multi-horizon BubbleScore forecasting\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Construct target sequences for multi-horizon BubbleScore forecasting\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Define the prediction horizons.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_prediction_horizons(config: Dict[str, Any]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Retrieves and validates the list of prediction horizons from the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: A validated list of positive integer prediction horizons.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the required key is missing from the configuration.\n",
        "        ValueError: If the horizons are not positive integers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested key for the prediction horizons.\n",
        "        horizons = config['backtesting']['strategy_rules']['prediction_horizons_to_test']\n",
        "\n",
        "        # --- Validation ---\n",
        "        # Ensure it's a list and all elements are positive integers.\n",
        "        if not isinstance(horizons, list) or not all(isinstance(h, int) and h > 0 for h in horizons):\n",
        "            raise ValueError(\"'prediction_horizons_to_test' must be a list of positive integers.\")\n",
        "\n",
        "        logging.info(f\"Prediction horizons extracted and validated: {horizons}\")\n",
        "        return sorted(horizons) # Return sorted for consistent ordering\n",
        "\n",
        "    except KeyError as e:\n",
        "        logging.error(f\"Missing prediction horizons in the configuration: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Steps 2 & 3: Handle edge cases and structure the dataset.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_multi_horizon_targets(\n",
        "    df: pd.DataFrame,\n",
        "    anchor_indices: pd.MultiIndex,\n",
        "    horizons: List[int]\n",
        ") -> Tuple[pd.MultiIndex, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs a matrix of multi-horizon targets for each valid anchor point.\n",
        "\n",
        "    This function uses efficient, vectorized `shift` operations to look up\n",
        "    future BubbleScore values. It handles edge cases by dropping any anchor\n",
        "    points for which a complete set of future targets is not available.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The main DataFrame containing the 'BubbleScore' column.\n",
        "        anchor_indices (pd.MultiIndex): The (Date, Ticker) indices corresponding\n",
        "                                        to the end of each input sequence.\n",
        "        horizons (List[int]): The list of forecast horizons (e.g., [1, 2, 3, 4, 5]).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.MultiIndex, np.ndarray]: A tuple containing:\n",
        "            - The final, filtered MultiIndex of valid anchor points.\n",
        "            - A 2D numpy array of shape (n_valid_samples, n_horizons)\n",
        "              containing the corresponding target values.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Constructing multi-horizon targets for horizons: {horizons}...\")\n",
        "\n",
        "    # --- Create Shifted Target Columns ---\n",
        "    # Create a temporary DataFrame to hold the shifted target columns.\n",
        "    target_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Group by ticker to prevent data leakage across securities during shifting.\n",
        "    grouped = df.groupby(level='TICKER')['BubbleScore']\n",
        "\n",
        "    for h in horizons:\n",
        "        # Use a negative shift to pull future values into the present.\n",
        "        # df.shift(-h) at time t gives the value from time t+h.\n",
        "        target_df[f'Target_H{h}'] = grouped.shift(-h)\n",
        "\n",
        "    # --- Align Targets with Anchor Points ---\n",
        "    # Select only the rows corresponding to our sequence anchor points.\n",
        "    aligned_targets = target_df.loc[anchor_indices]\n",
        "\n",
        "    # --- Handle Edge Cases by Dropping NaNs ---\n",
        "    # Any row with a NaN value indicates that at least one of its future targets\n",
        "    # was outside the available data range (i.e., too close to the end).\n",
        "    initial_samples = len(aligned_targets)\n",
        "    final_targets = aligned_targets.dropna()\n",
        "    final_samples = len(final_targets)\n",
        "\n",
        "    samples_dropped = initial_samples - final_samples\n",
        "    if samples_dropped > 0:\n",
        "        logging.info(f\"Dropped {samples_dropped:,} samples ({samples_dropped/initial_samples:.2%}) due to insufficient forward data for targets.\")\n",
        "\n",
        "    # The index of this cleaned DataFrame is our final set of valid anchor points.\n",
        "    final_valid_anchor_indices = final_targets.index\n",
        "\n",
        "    # Convert the final target DataFrame to a numpy array for use in ML models.\n",
        "    target_matrix = final_targets.to_numpy()\n",
        "\n",
        "    return final_valid_anchor_indices, target_matrix\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_and_align_targets(\n",
        "    df_bubblescore: pd.DataFrame,\n",
        "    anchor_indices: pd.MultiIndex,\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> Tuple[pd.MultiIndex, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of multi-horizon forecast targets.\n",
        "\n",
        "    This function prepares the target variable (y) for the supervised learning\n",
        "    problem. It ensures that for every input sequence, there is a corresponding\n",
        "    vector of future BubbleScore values to predict.\n",
        "\n",
        "    Args:\n",
        "        df_bubblescore (pd.DataFrame): The DataFrame from Task 17, containing\n",
        "                                       the 'BubbleScore' column.\n",
        "        anchor_indices (pd.MultiIndex): The (Date, Ticker) indices from Task 19\n",
        "                                        that mark the end of each input sequence.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.MultiIndex, np.ndarray]: A tuple containing:\n",
        "            - The final, filtered MultiIndex of valid anchor points for which\n",
        "              both inputs and a full set of targets exist.\n",
        "            - A 2D numpy array of the corresponding multi-horizon targets.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating multi-horizon target construction pipeline...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'BubbleScore' not in df_bubblescore.columns:\n",
        "        raise ValueError(\"Input DataFrame is missing the required 'BubbleScore' column.\")\n",
        "\n",
        "    # --- Step 1: Define the prediction horizons. ---\n",
        "    horizons = _get_prediction_horizons(study_parameters)\n",
        "    logging.info(\"Step 1/2: Prediction horizons defined.\")\n",
        "\n",
        "    # --- Step 2 & 3: Construct targets and handle edge cases. ---\n",
        "    final_anchor_indices, target_matrix = _construct_multi_horizon_targets(\n",
        "        df_bubblescore, anchor_indices, horizons\n",
        "    )\n",
        "    logging.info(f\"Step 2/2: Constructed target matrix with shape {target_matrix.shape}.\")\n",
        "\n",
        "    logging.info(\"Multi-horizon target construction pipeline finished successfully.\")\n",
        "\n",
        "    return final_anchor_indices, target_matrix\n"
      ],
      "metadata": {
        "id": "B6JuKhiFN4ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Split the dataset into training, validation, and test sets chronologically\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Split the dataset into training, validation, and test sets\n",
        "#          chronologically\n",
        "# ==============================================================================\n",
        "\n",
        "# Define a simple data structure to hold the partitioned datasets for clarity.\n",
        "class ModelDataset(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured data container for holding a complete, partitioned dataset\n",
        "    ready for a deep learning model like the Dual-Stream Transformer.\n",
        "\n",
        "    This class uses a NamedTuple to group the different input streams (stock,\n",
        "    market) and the corresponding targets into a single, immutable object. This\n",
        "    improves code clarity and makes passing partitioned data between functions\n",
        "    less error-prone. Each instance of this class represents one full data split\n",
        "    (e.g., training, validation, or test set).\n",
        "\n",
        "    Attributes:\n",
        "        stock_sequences (np.ndarray):\n",
        "            A 3D numpy array containing the stock-specific feature sequences.\n",
        "            The shape is (n_samples, sequence_length, n_stock_features), where\n",
        "            `n_samples` is the number of observations in this particular data split.\n",
        "            This array forms the input to the stock-specific stream of the\n",
        "            Transformer model.\n",
        "\n",
        "        market_sequences (np.ndarray):\n",
        "            A 3D numpy array containing the market-level feature sequences.\n",
        "            The shape is (n_samples, sequence_length, n_market_features).\n",
        "            Each sequence `market_sequences[i]` corresponds to the same time\n",
        "            period as `stock_sequences[i]`, providing the market context for\n",
        "            that specific sample. This array forms the input to the market-level\n",
        "            stream of the Transformer model.\n",
        "\n",
        "        targets (np.ndarray):\n",
        "            A 2D numpy array containing the multi-horizon forecast targets.\n",
        "            The shape is (n_samples, n_horizons). Each row `targets[i]` is a\n",
        "            vector of future BubbleScore values `[y_{t+1}, y_{t+2}, ..., y_{t+H}]`\n",
        "            corresponding to the input sequences `stock_sequences[i]` and\n",
        "            `market_sequences[i]`, where `t` is the anchor date of the sequences.\n",
        "    \"\"\"\n",
        "    # A 3D array of stock-specific input sequences.\n",
        "    stock_sequences: np.ndarray\n",
        "\n",
        "    # A 3D array of market-level input sequences, aligned with the stock sequences.\n",
        "    market_sequences: np.ndarray\n",
        "\n",
        "    # A 2D array of multi-horizon target values, aligned with the input sequences.\n",
        "    targets: np.ndarray\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Extract the split ratios from configuration.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_and_validate_split_ratios(config: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Retrieves and validates the dataset split ratios from the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary of validated split ratios for\n",
        "                          'train', 'validation', and 'test'.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the split ratio configuration is missing.\n",
        "        ValueError: If the ratios do not sum to 1.0.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested dictionary for split ratios.\n",
        "        ratios = config['predictive_model']['data_preparation']['dataset_split_ratio']\n",
        "\n",
        "        # --- Validation ---\n",
        "        # Ensure all required keys are present.\n",
        "        if not all(k in ratios for k in ['train', 'validation', 'test']):\n",
        "            raise ValueError(\"Split ratios must contain 'train', 'validation', and 'test' keys.\")\n",
        "\n",
        "        # Ensure the ratios sum to 1.0, using a tolerance for floating-point math.\n",
        "        if not np.isclose(sum(ratios.values()), 1.0):\n",
        "            raise ValueError(f\"Split ratios must sum to 1.0, but sum to {sum(ratios.values())}.\")\n",
        "\n",
        "        logging.info(f\"Dataset split ratios validated: {ratios}\")\n",
        "        return ratios\n",
        "\n",
        "    except KeyError as e:\n",
        "        logging.error(f\"Missing dataset split ratio configuration: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Determine the chronological split dates.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _determine_chronological_split_dates(\n",
        "    anchor_indices: pd.MultiIndex,\n",
        "    ratios: Dict[str, float]\n",
        ") -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
        "    \"\"\"\n",
        "    Determines the date boundaries for the train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        anchor_indices (pd.MultiIndex): The MultiIndex of all valid samples.\n",
        "        ratios (Dict[str, float]): The validated split ratios.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Timestamp, pd.Timestamp]: A tuple containing:\n",
        "            - The end date of the training set.\n",
        "            - The end date of the validation set.\n",
        "    \"\"\"\n",
        "    # Get the unique, sorted timeline of all anchor dates in the dataset.\n",
        "    unique_dates = anchor_indices.get_level_values('Date').unique().sort_values()\n",
        "\n",
        "    # Calculate the integer index for the end of the training period.\n",
        "    train_end_idx = int(len(unique_dates) * ratios['train'])\n",
        "\n",
        "    # Calculate the integer index for the end of the validation period.\n",
        "    validation_end_idx = train_end_idx + int(len(unique_dates) * ratios['validation'])\n",
        "\n",
        "    # Retrieve the actual timestamp for the training set boundary.\n",
        "    train_end_date = unique_dates[train_end_idx]\n",
        "\n",
        "    # Retrieve the actual timestamp for the validation set boundary.\n",
        "    validation_end_date = unique_dates[validation_end_idx]\n",
        "\n",
        "    logging.info(f\"Chronological split dates determined:\")\n",
        "    logging.info(f\"  - Training ends on:   {train_end_date.date()}\")\n",
        "    logging.info(f\"  - Validation ends on: {validation_end_date.date()}\")\n",
        "\n",
        "    return train_end_date, validation_end_date\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Partition the dataset.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _partition_datasets(\n",
        "    stock_sequences: List[np.ndarray],\n",
        "    market_sequence_map: Dict[pd.Timestamp, np.ndarray],\n",
        "    target_matrix: np.ndarray,\n",
        "    anchor_indices: pd.MultiIndex,\n",
        "    train_end_date: pd.Timestamp,\n",
        "    validation_end_date: pd.Timestamp\n",
        ") -> Dict[str, ModelDataset]:\n",
        "    \"\"\"\n",
        "    Partitions the complete dataset into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        stock_sequences (List[np.ndarray]): List of all stock-level sequences.\n",
        "        market_sequence_map (Dict[pd.Timestamp, np.ndarray]): Map of market sequences.\n",
        "        target_matrix (np.ndarray): Matrix of all multi-horizon targets.\n",
        "        anchor_indices (pd.MultiIndex): The anchor index for all samples.\n",
        "        train_end_date (pd.Timestamp): The training set boundary date.\n",
        "        validation_end_date (pd.Timestamp): The validation set boundary date.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, ModelDataset]: A dictionary containing the partitioned\n",
        "                                 'train', 'validation', and 'test' datasets.\n",
        "    \"\"\"\n",
        "    logging.info(\"Partitioning data into train, validation, and test sets...\")\n",
        "\n",
        "    # Extract the date component of the anchor index for masking.\n",
        "    anchor_dates = anchor_indices.get_level_values('Date')\n",
        "\n",
        "    # --- Create Boolean Masks for Each Set ---\n",
        "    train_mask = anchor_dates <= train_end_date\n",
        "    validation_mask = (anchor_dates > train_end_date) & (anchor_dates <= validation_end_date)\n",
        "    test_mask = anchor_dates > validation_end_date\n",
        "\n",
        "    # --- Assemble Final Datasets ---\n",
        "    datasets: Dict[str, ModelDataset] = {}\n",
        "\n",
        "    # Convert list of arrays to a single 3D numpy array for easier slicing.\n",
        "    stock_sequences_arr = np.array(stock_sequences)\n",
        "\n",
        "    for name, mask in [('train', train_mask), ('validation', validation_mask), ('test', test_mask)]:\n",
        "        # Apply the mask to get the anchor indices for the current split.\n",
        "        split_indices = anchor_indices[mask]\n",
        "        split_dates = split_indices.get_level_values('Date')\n",
        "\n",
        "        # Slice the stock sequences and targets using the boolean mask.\n",
        "        split_stock_sequences = stock_sequences_arr[mask]\n",
        "        split_targets = target_matrix[mask]\n",
        "\n",
        "        # Retrieve the corresponding market sequences using the date map.\n",
        "        # This is efficient as we only construct the array we need.\n",
        "        split_market_sequences = np.array([market_sequence_map[date] for date in split_dates])\n",
        "\n",
        "        # Store the partitioned data in the results dictionary.\n",
        "        datasets[name] = ModelDataset(\n",
        "            stock_sequences=split_stock_sequences,\n",
        "            market_sequences=split_market_sequences,\n",
        "            targets=split_targets\n",
        "        )\n",
        "        logging.info(f\"  - {name.capitalize()} set created with {len(split_targets):,} samples.\")\n",
        "\n",
        "    # --- Final Sanity Check ---\n",
        "    total_samples = sum(len(ds.targets) for ds in datasets.values())\n",
        "    if total_samples != len(anchor_indices):\n",
        "        raise RuntimeError(\"Sample count mismatch after partitioning. Check split logic.\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def split_dataset_chronologically(\n",
        "    # Inputs from previous tasks\n",
        "    stock_sequences: List[np.ndarray],\n",
        "    market_sequence_map: Dict[pd.Timestamp, np.ndarray],\n",
        "    target_matrix: np.ndarray,\n",
        "    final_anchor_indices: pd.MultiIndex,\n",
        "    # Configuration\n",
        "    study_parameters: Dict[str, Any]\n",
        ") -> Dict[str, ModelDataset]:\n",
        "    \"\"\"\n",
        "    Orchestrates the chronological splitting of the dataset for time-series modeling.\n",
        "\n",
        "    This function is critical for preventing look-ahead bias. It ensures that\n",
        "    the training, validation, and test sets represent distinct, sequential\n",
        "    periods of time.\n",
        "\n",
        "    Args:\n",
        "        stock_sequences (List[np.ndarray]): All stock-level feature sequences.\n",
        "        market_sequence_map (Dict[pd.Timestamp, np.ndarray]): Map of market sequences.\n",
        "        target_matrix (np.ndarray): All corresponding multi-horizon targets.\n",
        "        final_anchor_indices (pd.MultiIndex): The (Date, Ticker) index for all samples.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, ModelDataset]: A dictionary containing the 'train',\n",
        "                                 'validation', and 'test' ModelDataset objects.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating chronological dataset splitting pipeline...\")\n",
        "\n",
        "    # --- Step 1: Get and validate the split ratios from the configuration. ---\n",
        "    ratios = _get_and_validate_split_ratios(study_parameters)\n",
        "    logging.info(\"Step 1/3: Dataset split ratios validated.\")\n",
        "\n",
        "    # --- Step 2: Determine the chronological date boundaries for the splits. ---\n",
        "    train_end_date, validation_end_date = _determine_chronological_split_dates(\n",
        "        final_anchor_indices, ratios\n",
        "    )\n",
        "    logging.info(\"Step 2/3: Chronological split boundaries determined.\")\n",
        "\n",
        "    # --- Step 3: Partition all data components into three sets. ---\n",
        "    partitioned_datasets = _partition_datasets(\n",
        "        stock_sequences,\n",
        "        market_sequence_map,\n",
        "        target_matrix,\n",
        "        final_anchor_indices,\n",
        "        train_end_date,\n",
        "        validation_end_date\n",
        "    )\n",
        "    logging.info(\"Step 3/3: All data components partitioned successfully.\")\n",
        "\n",
        "    logging.info(\"Chronological dataset splitting pipeline finished successfully.\")\n",
        "\n",
        "    return partitioned_datasets\n"
      ],
      "metadata": {
        "id": "pHH5NKCDPNka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Define the Dual-Stream Transformer architecture\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Define the Dual-Stream Transformer architecture (Enhanced Docs)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Component: Positional Encoding\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Injects positional information into sequence embeddings.\n",
        "\n",
        "    This module implements the fixed sinusoidal positional encoding described in\n",
        "    the \"Attention Is All You Need\" paper. It generates a matrix of sine and\n",
        "    cosine functions of different frequencies, which are then added to the\n",
        "    input embeddings. This allows the model, which is otherwise permutation-\n",
        "    invariant, to understand the relative or absolute position of elements\n",
        "    in a sequence.\n",
        "\n",
        "    The encoding is not a trainable parameter but is registered as a buffer,\n",
        "    meaning it is part of the model's state and will be moved to the correct\n",
        "    device (e.g., GPU) along with the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        \"\"\"\n",
        "        Initializes the PositionalEncoding module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input embeddings.\n",
        "            dropout (float): The dropout rate to apply to the final output.\n",
        "            max_len (int): The maximum possible sequence length.\n",
        "        \"\"\"\n",
        "        # Call the parent class constructor.\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize a dropout layer for regularization.\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a tensor representing the positions in the sequence (0, 1, ..., max_len-1).\n",
        "        # Shape: (max_len, 1)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "\n",
        "        # Calculate the division term for the sinusoidal functions. This creates\n",
        "        # a geometric progression of frequencies.\n",
        "        # Shape: (d_model / 2)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Initialize the positional encoding matrix with zeros.\n",
        "        # Shape: (max_len, 1, d_model)\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "\n",
        "        # Apply the sine function to even indices in the embedding dimension.\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cosine function to odd indices in the embedding dimension.\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer. This makes it part of the model's state_dict\n",
        "        # but not a parameter that is updated by the optimizer.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Adds positional encoding to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of sequence embeddings.\n",
        "                              Shape: (seq_len, batch_size, d_model).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor with positional information added.\n",
        "                          Shape: (seq_len, batch_size, d_model).\n",
        "        \"\"\"\n",
        "        # Add the pre-computed positional encodings to the input embeddings.\n",
        "        # We slice `self.pe` to match the length of the input sequence `x`.\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "\n",
        "        # Apply dropout to the combined embeddings for regularization.\n",
        "        return self.dropout(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Component: Bi-directional Cross-Attention\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class BiDirectionalCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module for bi-directional cross-attention between two parallel sequences.\n",
        "\n",
        "    This module is a key component of the Dual-Stream architecture. It contains\n",
        "    two multi-head attention layers:\n",
        "    1.  One where the stock sequence acts as the query and the market sequence\n",
        "        acts as the key and value.\n",
        "    2.  One where the market sequence acts as the query and the stock sequence\n",
        "        acts as the key and value.\n",
        "\n",
        "    This allows information to flow between the two streams, enabling the model\n",
        "    to learn context-dependent representations. Each attention operation is\n",
        "    followed by a residual connection and layer normalization, as is standard\n",
        "    in Transformer architectures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes the BiDirectionalCrossAttention module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The embedding dimension of the sequences.\n",
        "            nhead (int): The number of attention heads.\n",
        "            dropout (float): The dropout rate for the attention layers.\n",
        "        \"\"\"\n",
        "        # Call the parent class constructor.\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the attention layer for the stock stream to attend to the market stream.\n",
        "        # `batch_first=False` is specified because we will be working with tensors of\n",
        "        # shape (seq_len, batch_size, d_model).\n",
        "        self.stock_to_market_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
        "\n",
        "        # Initialize the attention layer for the market stream to attend to the stock stream.\n",
        "        self.market_to_stock_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
        "\n",
        "        # Initialize layer normalization for the output of each attention block.\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Initialize dropout layers for regularization within the residual connections.\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, stock_seq: torch.Tensor, market_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Performs the forward pass for bi-directional cross-attention.\n",
        "\n",
        "        Args:\n",
        "            stock_seq (torch.Tensor): The encoded stock sequence tensor.\n",
        "                                      Shape: (seq_len, batch_size, d_model).\n",
        "            market_seq (torch.Tensor): The encoded market sequence tensor.\n",
        "                                       Shape: (seq_len, batch_size, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the updated\n",
        "                stock and market sequence tensors after cross-attention.\n",
        "        \"\"\"\n",
        "        # --- Stock-to-Market Attention Block ---\n",
        "        # The stock sequence is the query (Q); the market sequence is the key (K) and value (V).\n",
        "        attended_stock, _ = self.stock_to_market_attn(query=stock_seq, key=market_seq, value=market_seq)\n",
        "\n",
        "        # Apply a residual connection (Add) and layer normalization (Norm).\n",
        "        # This is a standard building block of Transformer architectures.\n",
        "        stock_seq = self.norm1(stock_seq + self.dropout1(attended_stock))\n",
        "\n",
        "        # --- Market-to-Stock Attention Block ---\n",
        "        # The market sequence is the query (Q); the stock sequence is the key (K) and value (V).\n",
        "        attended_market, _ = self.market_to_stock_attn(query=market_seq, key=stock_seq, value=stock_seq)\n",
        "\n",
        "        # Apply a second residual connection and layer normalization.\n",
        "        market_seq = self.norm2(market_seq + self.dropout2(attended_market))\n",
        "\n",
        "        # Return the two updated sequences.\n",
        "        return stock_seq, market_seq\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Main Architecture: Dual-Stream Transformer\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class DualStreamTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the full Dual-Stream Transformer architecture for bubble prediction.\n",
        "\n",
        "    This model is designed to process two parallel time-series inputs:\n",
        "    1.  A stock-specific stream containing features like price, volume, and ratios.\n",
        "    2.  A market-level stream containing features like VIX and market sentiment.\n",
        "\n",
        "    The architecture consists of the following stages:\n",
        "    - Input Projection: Each stream's features are projected into a high-dimensional space.\n",
        "    - Positional Encoding: Sinusoidal encodings are added to inform the model of sequence order.\n",
        "    - Parallel Self-Attention: Each stream is processed by a separate Transformer encoder.\n",
        "    - Bi-Directional Cross-Attention: The two streams interact, allowing them to share information.\n",
        "    - Pooling & Fusion: The sequence representations are pooled and fused into a single vector.\n",
        "    - Multi-Horizon Prediction: A set of independent MLP heads predict the BubbleScore at different future horizons.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any], num_stock_features: int, num_market_features: int):\n",
        "        \"\"\"\n",
        "        Initializes the DualStreamTransformer model.\n",
        "\n",
        "        Args:\n",
        "            config (Dict[str, Any]): The main study configuration dictionary.\n",
        "            num_stock_features (int): The number of features in the stock-specific input stream.\n",
        "            num_market_features (int): The number of features in the market-level input stream.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the embedding dimension is not divisible by the number of attention heads.\n",
        "        \"\"\"\n",
        "        # Call the parent class constructor.\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Step 1: Hyperparameter Extraction and Validation ---\n",
        "        # Extract architectural hyperparameters from the configuration dictionary.\n",
        "        arch_config = config['architecture']\n",
        "        d_model = arch_config['embedding_dim']\n",
        "        nhead = arch_config['num_attention_heads']\n",
        "        d_hid = d_model * arch_config['mlp_hidden_dim_ratio']\n",
        "        nlayers = arch_config['num_encoder_layers']\n",
        "        dropout = config['training']['dropout_rate']\n",
        "        pred_head_hid_dim = arch_config['prediction_head_hidden_dim']\n",
        "        self.num_horizons = len(config['backtesting']['strategy_rules']['prediction_horizons_to_test'])\n",
        "\n",
        "        # Validate a critical architectural constraint for multi-head attention.\n",
        "        if d_model % nhead != 0:\n",
        "            raise ValueError(f\"embedding_dim ({d_model}) must be divisible by num_attention_heads ({nhead}).\")\n",
        "\n",
        "        # --- Step 2 & 3: Architecture Definition ---\n",
        "        # A linear layer to project the raw stock features into the model's embedding space.\n",
        "        self.stock_projector = nn.Linear(num_stock_features, d_model)\n",
        "        # A separate linear layer for the market features.\n",
        "        self.market_projector = nn.Linear(num_market_features, d_model)\n",
        "\n",
        "        # The positional encoding module.\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Define a standard Transformer encoder layer, which will be cloned for the encoder stacks.\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
        "\n",
        "        # Create the Transformer encoder for the stock stream.\n",
        "        self.stock_transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
        "        # Create a separate, independent Transformer encoder for the market stream.\n",
        "        self.market_transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
        "\n",
        "        # The custom bi-directional cross-attention module.\n",
        "        self.cross_attention = BiDirectionalCrossAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # A small MLP to fuse the representations from the two streams after pooling.\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(d_model)\n",
        "        )\n",
        "\n",
        "        # Create a list of independent prediction heads, one for each forecast horizon.\n",
        "        self.prediction_heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, pred_head_hid_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(pred_head_hid_dim, 1),\n",
        "                nn.Tanh() # Tanh activation is crucial to bound the output in [-1, 1].\n",
        "            ) for _ in range(self.num_horizons)\n",
        "        ])\n",
        "\n",
        "    def forward(self, stock_seq: torch.Tensor, market_seq: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model from inputs to final predictions.\n",
        "\n",
        "        Args:\n",
        "            stock_seq (torch.Tensor): A batch of stock-specific sequences.\n",
        "                                      Shape: (batch_size, seq_len, num_stock_features).\n",
        "            market_seq (torch.Tensor): A batch of market-level sequences.\n",
        "                                       Shape: (batch_size, seq_len, num_market_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of multi-horizon predictions.\n",
        "                          Shape: (batch_size, num_horizons).\n",
        "        \"\"\"\n",
        "        # --- 1. Input Projection ---\n",
        "        # Map the input features of each stream to the model's embedding dimension `d_model`.\n",
        "        stock_emb = self.stock_projector(stock_seq)\n",
        "        market_emb = self.market_projector(market_seq)\n",
        "\n",
        "        # --- 2. Reshape and Add Positional Encoding ---\n",
        "        # PyTorch's native Transformer modules expect shape (seq_len, batch_size, d_model).\n",
        "        # We permute the dimensions from (batch, seq, feat) to (seq, batch, feat).\n",
        "        stock_emb = stock_emb.permute(1, 0, 2)\n",
        "        market_emb = market_emb.permute(1, 0, 2)\n",
        "\n",
        "        # Add positional information to the embeddings.\n",
        "        stock_pos = self.pos_encoder(stock_emb)\n",
        "        market_pos = self.pos_encoder(market_emb)\n",
        "\n",
        "        # --- 3. Independent Self-Attention (Encoders) ---\n",
        "        # Each stream is processed independently by its own Transformer encoder.\n",
        "        stock_encoded = self.stock_transformer_encoder(stock_pos)\n",
        "        market_encoded = self.market_transformer_encoder(market_pos)\n",
        "\n",
        "        # --- 4. Bi-directional Cross-Attention ---\n",
        "        # The two streams interact, sharing contextual information.\n",
        "        stock_cross, market_cross = self.cross_attention(stock_encoded, market_encoded)\n",
        "\n",
        "        # --- 5. Pooling and Fusion ---\n",
        "        # Aggregate the sequence information by taking the mean across the time dimension (dim=0).\n",
        "        stock_pooled = stock_cross.mean(dim=0)\n",
        "        market_pooled = market_cross.mean(dim=0)\n",
        "\n",
        "        # Concatenate the two resulting vectors into a single, larger representation.\n",
        "        fused = torch.cat((stock_pooled, market_pooled), dim=1)\n",
        "\n",
        "        # Pass the concatenated vector through a final fusion layer.\n",
        "        final_repr = self.fusion_layer(fused)\n",
        "\n",
        "        # --- 6. Multi-Horizon Prediction ---\n",
        "        # Apply each independent prediction head to the final fused representation.\n",
        "        predictions = [head(final_repr) for head in self.prediction_heads]\n",
        "\n",
        "        # Concatenate the scalar outputs from each head into a single prediction tensor.\n",
        "        # The result is a tensor of shape (batch_size, num_horizons).\n",
        "        return torch.cat(predictions, dim=1)\n"
      ],
      "metadata": {
        "id": "naVSmjB_RGL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Implement the composite training loss function\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Implement the composite training loss function\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Steps 1, 2, & 3: Define and combine all loss components.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class CompositeLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the composite, multi-component loss function described in Eq. (15).\n",
        "\n",
        "    This loss function is a weighted sum of five distinct components, each\n",
        "    targeting a different desirable property for the model's predictions:\n",
        "    1. Huber Loss: Robustness to outliers (point-wise accuracy).\n",
        "    2. Correlation Loss: Encourages linear relationship between predictions and targets.\n",
        "    3. R-squared Loss: Directly optimizes the coefficient of determination.\n",
        "    4. Consistency Loss: Penalizes mismatches in the day-to-day changes.\n",
        "    5. Smoothness Loss: Regularizes the predictions to prevent abrupt fluctuations.\n",
        "\n",
        "    Equation (15):\n",
        "    L = λ1*L_Huber + λ2*L_Corr + λ3*L_R2 + λ4*L_Cons + λ5*L_Smooth\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any], epsilon: float = 1e-8):\n",
        "        \"\"\"\n",
        "        Initializes the CompositeLoss module and extracts weights from config.\n",
        "\n",
        "        Args:\n",
        "            config (Dict[str, Any]): The main study configuration dictionary.\n",
        "            epsilon (float): A small value to add to denominators for\n",
        "                             numerical stability.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If the loss function weights are not found in the config.\n",
        "        \"\"\"\n",
        "        # Call the parent class constructor.\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Step 1: Extract loss component weights ---\n",
        "        try:\n",
        "            # Access the nested dictionary of loss weights.\n",
        "            weights = config['predictive_model']['training']['loss_function_weights']\n",
        "            # Store each weight as an attribute of the class.\n",
        "            self.lambda_huber = weights['lambda_1_huber']\n",
        "            self.lambda_corr = weights['lambda_2_corr']\n",
        "            self.lambda_r_squared = weights['lambda_3_r_squared']\n",
        "            self.lambda_cons = weights['lambda_4_cons']\n",
        "            self.lambda_smooth = weights['lambda_5_smooth']\n",
        "        except KeyError as e:\n",
        "            logging.error(f\"Missing a required loss weight in the configuration: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Store the epsilon value for numerical stability.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the total composite loss.\n",
        "\n",
        "        Args:\n",
        "            predictions (torch.Tensor): The model's output tensor.\n",
        "                                        Shape: (batch_size, num_horizons).\n",
        "            targets (torch.Tensor): The ground-truth target tensor.\n",
        "                                    Shape: (batch_size, num_horizons).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A single scalar tensor representing the total loss.\n",
        "        \"\"\"\n",
        "        # --- Step 2: Define each loss component ---\n",
        "\n",
        "        # 1. Huber Loss (L_Huber): Robust to outliers.\n",
        "        # PyTorch's built-in Huber loss is efficient and stable.\n",
        "        loss_huber = F.huber_loss(predictions, targets, reduction='mean')\n",
        "\n",
        "        # 2. Correlation Loss (L_Corr): 1 - Pearson Correlation.\n",
        "        # We compute this manually for full control over stability.\n",
        "        # Center the predictions and targets by subtracting their means.\n",
        "        pred_mean = predictions.mean()\n",
        "        targ_mean = targets.mean()\n",
        "        pred_centered = predictions - pred_mean\n",
        "        targ_centered = targets - targ_mean\n",
        "        # Calculate the covariance.\n",
        "        covariance = (pred_centered * targ_centered).mean()\n",
        "        # Calculate the standard deviations.\n",
        "        pred_std = torch.sqrt((pred_centered**2).mean())\n",
        "        targ_std = torch.sqrt((targ_centered**2).mean())\n",
        "        # Calculate the Pearson correlation coefficient.\n",
        "        correlation = covariance / (pred_std * targ_std + self.epsilon)\n",
        "        # The loss is 1 minus the correlation.\n",
        "        loss_corr = 1 - correlation\n",
        "\n",
        "        # 3. R-squared Loss (L_R2): 1 - R^2, which simplifies to SS_res / SS_tot.\n",
        "        # This directly encourages the model to explain variance.\n",
        "        # Calculate the residual sum of squares.\n",
        "        ss_res = torch.sum((targets - predictions)**2)\n",
        "        # Calculate the total sum of squares.\n",
        "        ss_tot = torch.sum((targets - targets.mean())**2)\n",
        "        # The loss is the ratio. Add epsilon for stability.\n",
        "        loss_r_squared = ss_res / (ss_tot + self.epsilon)\n",
        "\n",
        "        # 4. Consistency Loss (L_Cons): MSE of the first differences.\n",
        "        # This encourages the model to match the direction and magnitude of changes.\n",
        "        # Calculate the difference between consecutive horizons.\n",
        "        delta_pred = torch.diff(predictions, dim=1)\n",
        "        delta_targ = torch.diff(targets, dim=1)\n",
        "        # Calculate the Mean Squared Error of these differences.\n",
        "        loss_cons = F.mse_loss(delta_pred, delta_targ)\n",
        "\n",
        "        # 5. Smoothness Loss (L_Smooth): MSE of the prediction's first differences.\n",
        "        # This acts as a regularizer to penalize overly volatile predictions.\n",
        "        loss_smooth = F.mse_loss(delta_pred, torch.zeros_like(delta_pred))\n",
        "\n",
        "        # --- Step 3: Combine into the total loss ---\n",
        "        # Calculate the final weighted sum of all loss components.\n",
        "        total_loss = (\n",
        "            self.lambda_huber * loss_huber +\n",
        "            self.lambda_corr * loss_corr +\n",
        "            self.lambda_r_squared * loss_r_squared +\n",
        "            self.lambda_cons * loss_cons +\n",
        "            self.lambda_smooth * loss_smooth\n",
        "        )\n",
        "\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "yWEX7tGmTo4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Train the Dual-Stream Transformer model and Task 26: Validate the model and implement early stopping.\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Train the Dual-Stream Transformer model without validation and\n",
        "#          Early stopping\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(\n",
        "    model: DualStreamTransformer,\n",
        "    partitioned_datasets: Dict[str, ModelDataset],\n",
        "    loss_function: CompositeLoss,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    device: torch.device,\n",
        "    output_dir: Union[str, Path] = \"models\"\n",
        ") -> Tuple[DualStreamTransformer, List[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end training of the Dual-Stream Transformer model.\n",
        "\n",
        "    This function manages the entire training process, including:\n",
        "    1.  Initializing the AdamW optimizer and the OneCycleLR learning rate scheduler.\n",
        "    2.  Running the main training loop over the specified number of epochs.\n",
        "    3.  Implementing the complete forward/backward pass, including gradient clipping.\n",
        "    4.  Logging training progress (loss, learning rate) and saving periodic\n",
        "        checkpoints for resumability.\n",
        "\n",
        "    Args:\n",
        "        model (DualStreamTransformer): The instantiated model to be trained.\n",
        "        partitioned_datasets (Dict[str, ModelDataset]): The dictionary containing\n",
        "            the 'train', 'validation', and 'test' data splits.\n",
        "        loss_function (CompositeLoss): The instantiated composite loss function.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        device (torch.device): The device (CPU or CUDA) to train on.\n",
        "        output_dir (Union[str, Path]): The directory to save checkpoints.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[DualStreamTransformer, List[Dict[str, float]]]: A tuple containing:\n",
        "            - The model with its trained weights.\n",
        "            - A list of dictionaries logging the training history per epoch.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating model training pipeline...\")\n",
        "\n",
        "    # --- Extract Configuration ---\n",
        "    # Retrieve training and optimizer parameters from the configuration.\n",
        "    train_config = study_parameters['predictive_model']['training']\n",
        "    optim_config = study_parameters['predictive_model']['optimizer']\n",
        "\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    batch_size = train_config['batch_size']\n",
        "    lr = optim_config['learning_rate']\n",
        "    weight_decay = optim_config['weight_decay']\n",
        "    clip_threshold = optim_config['gradient_clipping_threshold']\n",
        "\n",
        "    # --- Prepare DataLoader ---\n",
        "    # Convert numpy arrays from the training set into PyTorch tensors.\n",
        "    train_data = partitioned_datasets['train']\n",
        "    train_tensors = [\n",
        "        torch.from_numpy(train_data.stock_sequences).float(),\n",
        "        torch.from_numpy(train_data.market_sequences).float(),\n",
        "        torch.from_numpy(train_data.targets).float()\n",
        "    ]\n",
        "    # Create a TensorDataset and a DataLoader for efficient batching and shuffling.\n",
        "    train_dataset = TensorDataset(*train_tensors)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # --- Step 1: Initialize Optimizer and Scheduler ---\n",
        "    # Instantiate the AdamW optimizer, which is well-suited for Transformer models.\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Instantiate the OneCycleLR scheduler, which dynamically adjusts the learning rate.\n",
        "    # This requires knowing the total number of training steps.\n",
        "    total_steps = num_epochs * len(train_dataloader)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=total_steps)\n",
        "\n",
        "    logging.info(f\"Optimizer (AdamW) and Scheduler (OneCycleLR) initialized. Total training steps: {total_steps}.\")\n",
        "\n",
        "    # --- Step 2 & 3: Implement Training Loop with Logging and Checkpointing ---\n",
        "    # Move the model to the designated training device.\n",
        "    model.to(device)\n",
        "\n",
        "    # This list will store the loss and learning rate for each epoch.\n",
        "    training_history = []\n",
        "\n",
        "    # Create directories for saving checkpoints.\n",
        "    checkpoint_dir = Path(output_dir) / \"checkpoints\"\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    logging.info(f\"Starting training for {num_epochs} epochs...\")\n",
        "    # The main training loop iterates over epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set the model to training mode. This enables layers like Dropout.\n",
        "        model.train()\n",
        "\n",
        "        # Accumulator for the loss over an epoch.\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # The inner loop iterates over batches of data.\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "            # Unpack the batch and move all tensors to the training device.\n",
        "            stock_seq, market_seq, targets = [b.to(device) for b in batch]\n",
        "\n",
        "            # --- Forward Pass ---\n",
        "            # Get model predictions for the current batch.\n",
        "            predictions = model(stock_seq, market_seq)\n",
        "\n",
        "            # --- Loss Calculation ---\n",
        "            # Compute the composite loss between predictions and targets.\n",
        "            loss = loss_function(predictions, targets)\n",
        "\n",
        "            # --- Backward Pass and Optimization ---\n",
        "            # 1. Reset the gradients from the previous step.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2. Compute gradients of the loss with respect to model parameters.\n",
        "            loss.backward()\n",
        "\n",
        "            # 3. Clip gradients to prevent them from exploding, a common issue in deep networks.\n",
        "            # This is a critical regularization step.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_threshold)\n",
        "\n",
        "            # 4. Update the model's weights based on the computed gradients.\n",
        "            optimizer.step()\n",
        "\n",
        "            # 5. Update the learning rate. For OneCycleLR, this is done after every batch.\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate the loss for this batch.\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # --- End of Epoch Logging ---\n",
        "        # Calculate the average loss for the epoch.\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        # Get the current learning rate from the optimizer.\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Log the epoch's performance.\n",
        "        logging.info(f\"Epoch {epoch + 1} Complete | Average Loss: {avg_loss:.6f} | Current LR: {current_lr:.8f}\")\n",
        "\n",
        "        # Store the metrics in the history log.\n",
        "        training_history.append({'epoch': epoch + 1, 'loss': avg_loss, 'lr': current_lr})\n",
        "\n",
        "        # --- Step 3: Checkpointing ---\n",
        "        # Save a checkpoint periodically (e.g., every 10 epochs).\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = checkpoint_dir / f\"epoch_{epoch + 1}.pth\"\n",
        "            logging.info(f\"Saving checkpoint to '{checkpoint_path}'...\")\n",
        "            # Save a comprehensive state dictionary for full resumability.\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    logging.info(\"Model training pipeline finished successfully.\")\n",
        "\n",
        "    # Return the trained model and its training history.\n",
        "    return model, training_history\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Train the Dual-Stream Transformer model with validation and\n",
        "#          Early stopping\n",
        "# ==============================================================================\n",
        "\n",
        "def _run_validation_epoch(\n",
        "    model: DualStreamTransformer,\n",
        "    val_dataloader: DataLoader,\n",
        "    loss_function: CompositeLoss,\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs a full evaluation pass on the validation dataset for a single epoch.\n",
        "\n",
        "    This function is a critical part of the training loop. It sets the model\n",
        "    to evaluation mode, disables gradient calculations for efficiency, and\n",
        "    iterates through the entire validation set to compute performance metrics.\n",
        "    Metrics are calculated on the full validation set (not per-batch) to ensure\n",
        "    statistical robustness.\n",
        "\n",
        "    Args:\n",
        "        model (DualStreamTransformer):\n",
        "            The model instance to be evaluated.\n",
        "        val_dataloader (DataLoader):\n",
        "            The PyTorch DataLoader for the validation dataset.\n",
        "        loss_function (CompositeLoss):\n",
        "            The instantiated loss function used to calculate the validation loss.\n",
        "        device (torch.device):\n",
        "            The device (e.g., 'cuda' or 'cpu') on which to perform the evaluation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]:\n",
        "            A dictionary containing key validation metrics for the epoch,\n",
        "            including 'val_loss', 'val_corr', 'val_mse', 'val_rmse', and 'val_mae'.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode. This is a critical step that disables\n",
        "    # layers like Dropout and LayerNorm's training-specific behavior, ensuring\n",
        "    # deterministic and reproducible evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize a variable to accumulate the loss over all batches.\n",
        "    running_val_loss = 0.0\n",
        "    # Initialize lists to store all predictions and targets from the validation set.\n",
        "    all_predictions: List[torch.Tensor] = []\n",
        "    all_targets: List[torch.Tensor] = []\n",
        "\n",
        "    # Use the `torch.no_grad()` context manager to disable gradient computation.\n",
        "    # This significantly speeds up the forward pass and reduces memory consumption.\n",
        "    with torch.no_grad():\n",
        "        # Iterate over all batches provided by the validation DataLoader.\n",
        "        for batch in val_dataloader:\n",
        "            # Unpack the batch and move all tensors to the specified evaluation device.\n",
        "            stock_seq, market_seq, targets = [b.to(device) for b in batch]\n",
        "\n",
        "            # --- Forward Pass ---\n",
        "            # Pass the input sequences through the model to get predictions.\n",
        "            predictions = model(stock_seq, market_seq)\n",
        "\n",
        "            # --- Loss Calculation ---\n",
        "            # Compute the loss for the current batch using the provided loss function.\n",
        "            loss = loss_function(predictions, targets)\n",
        "            # Add the scalar loss value of the batch to the running total.\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Append the batch's predictions and targets to the master lists.\n",
        "            # Move them to the CPU to free up GPU memory.\n",
        "            all_predictions.append(predictions.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "    # Concatenate the lists of batch tensors into single, large tensors.\n",
        "    all_predictions_tensor = torch.cat(all_predictions)\n",
        "    all_targets_tensor = torch.cat(all_targets)\n",
        "\n",
        "    # --- Calculate Full-Set Metrics ---\n",
        "    # Calculate the average validation loss across all batches.\n",
        "    avg_val_loss = running_val_loss / len(val_dataloader)\n",
        "\n",
        "    # Manually calculate the Pearson correlation coefficient for the entire validation set.\n",
        "    # This is more accurate than averaging per-batch correlations.\n",
        "    # Center the prediction and target tensors.\n",
        "    vx = all_predictions_tensor - torch.mean(all_predictions_tensor)\n",
        "    vy = all_targets_tensor - torch.mean(all_targets_tensor)\n",
        "    # Compute correlation using the formula: cov(X, Y) / (std(X) * std(Y)).\n",
        "    corr = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE) using PyTorch's functional implementation.\n",
        "    mse = F.mse_loss(all_predictions_tensor, all_targets_tensor)\n",
        "    # Calculate Mean Absolute Error (MAE).\n",
        "    mae = F.l1_loss(all_predictions_tensor, all_targets_tensor)\n",
        "\n",
        "    # Return a dictionary containing all computed metrics for this epoch.\n",
        "    return {\n",
        "        'val_loss': avg_val_loss,\n",
        "        'val_corr': corr.item(),\n",
        "        'val_mse': mse.item(),\n",
        "        'val_rmse': torch.sqrt(mse).item(),\n",
        "        'val_mae': mae.item()\n",
        "    }\n",
        "\n",
        "\n",
        "def train_and_validate_model(\n",
        "    model: DualStreamTransformer,\n",
        "    partitioned_datasets: Dict[str, ModelDataset],\n",
        "    loss_function: CompositeLoss,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    device: torch.device,\n",
        "    output_dir: Union[str, Path] = \"models\"\n",
        ") -> Tuple[DualStreamTransformer, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete model training and validation pipeline, including\n",
        "    early stopping and checkpointing of the best model.\n",
        "\n",
        "    This function integrates a training loop with a validation loop at the end\n",
        "    of each epoch. It monitors the validation loss to prevent overfitting by\n",
        "    stopping the training process when performance on the validation set ceases\n",
        "    to improve. It saves the model state at the point of best performance.\n",
        "\n",
        "    Args:\n",
        "        model (DualStreamTransformer):\n",
        "            The instantiated model to be trained.\n",
        "        partitioned_datasets (Dict[str, ModelDataset]):\n",
        "            A dictionary containing the 'train', 'validation', and 'test' data splits.\n",
        "        loss_function (CompositeLoss):\n",
        "            The instantiated composite loss function.\n",
        "        study_parameters (Dict[str, Any]):\n",
        "            The main configuration dictionary for the study.\n",
        "        device (torch.device):\n",
        "            The device (e.g., 'cuda' or 'cpu') on which to perform training.\n",
        "        output_dir (Union[str, Path]):\n",
        "            The directory where model checkpoints and training logs will be saved.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[DualStreamTransformer, pd.DataFrame]: A tuple containing:\n",
        "            - The model instance loaded with the weights from the best performing epoch.\n",
        "            - A pandas DataFrame logging the complete training and validation history.\n",
        "    \"\"\"\n",
        "    # Announce the start of the training process.\n",
        "    logging.info(\"Initiating model training and validation pipeline...\")\n",
        "\n",
        "    # --- Configuration Extraction ---\n",
        "    # Retrieve all necessary hyperparameters from the configuration dictionary.\n",
        "    train_config = study_parameters['predictive_model']['training']\n",
        "    optim_config = study_parameters['predictive_model']['optimizer']\n",
        "    early_stop_config = study_parameters['predictive_model']['early_stopping']\n",
        "\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    batch_size = train_config['batch_size']\n",
        "    lr = optim_config['learning_rate']\n",
        "    weight_decay = optim_config['weight_decay']\n",
        "    clip_threshold = optim_config['gradient_clipping_threshold']\n",
        "    patience = early_stop_config['patience']\n",
        "\n",
        "    # --- Prepare DataLoaders ---\n",
        "    # Create a PyTorch TensorDataset from the training data numpy arrays.\n",
        "    train_data = partitioned_datasets['train']\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.from_numpy(train_data.stock_sequences).float(),\n",
        "        torch.from_numpy(train_data.market_sequences).float(),\n",
        "        torch.from_numpy(train_data.targets).float()\n",
        "    )\n",
        "    # Create a DataLoader to handle batching and shuffling of the training data.\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Create a TensorDataset and DataLoader for the validation data (no shuffling).\n",
        "    val_data = partitioned_datasets['validation']\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.from_numpy(val_data.stock_sequences).float(),\n",
        "        torch.from_numpy(val_data.market_sequences).float(),\n",
        "        torch.from_numpy(val_data.targets).float()\n",
        "    )\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # --- Initialize Optimizer and Scheduler ---\n",
        "    # Instantiate the AdamW optimizer with the model's parameters and configured hyperparameters.\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # Calculate the total number of training steps (batches) for the scheduler.\n",
        "    total_steps = num_epochs * len(train_dataloader)\n",
        "    # Instantiate the OneCycleLR scheduler, which manages the learning rate over the training run.\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=total_steps)\n",
        "\n",
        "    # --- Initialize Early Stopping and Logging Variables ---\n",
        "    # Track the best validation loss seen so far, initialized to infinity.\n",
        "    best_val_loss = float('inf')\n",
        "    # Counter for epochs without improvement in validation loss.\n",
        "    epochs_no_improve = 0\n",
        "    # List to store the metrics from each epoch.\n",
        "    history = []\n",
        "    # Define paths for saving model and log files.\n",
        "    output_path = Path(output_dir)\n",
        "    best_model_path = output_path / \"best_model.pth\"\n",
        "    # Ensure the output directory exists.\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- Main Training & Validation Loop ---\n",
        "    # Move the model to the designated training device.\n",
        "    model.to(device)\n",
        "    logging.info(f\"Starting training for up to {num_epochs} epochs with early stopping patience of {patience}...\")\n",
        "\n",
        "    # The main loop iterates over the specified number of epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Training Step ---\n",
        "        # Set the model to training mode to enable dropout, etc.\n",
        "        model.train()\n",
        "        # Initialize a variable to accumulate the training loss for the epoch.\n",
        "        running_train_loss = 0.0\n",
        "        # Iterate over batches from the training DataLoader.\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"):\n",
        "            # Move the batch of data to the training device.\n",
        "            stock_seq, market_seq, targets = [b.to(device) for b in batch]\n",
        "            # Reset gradients to zero before the backward pass.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform the forward pass to get model predictions.\n",
        "            predictions = model(stock_seq, market_seq)\n",
        "            # Calculate the loss.\n",
        "            loss = loss_function(predictions, targets)\n",
        "            # Perform the backward pass to compute gradients.\n",
        "            loss.backward()\n",
        "            # Clip the norm of the gradients to prevent them from exploding.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_threshold)\n",
        "            # Update the model's weights using the optimizer.\n",
        "            optimizer.step()\n",
        "            # Update the learning rate according to the scheduler's policy.\n",
        "            scheduler.step()\n",
        "            # Add the batch loss to the running total for the epoch.\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        # Calculate the average training loss for the epoch.\n",
        "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
        "\n",
        "        # --- Step 1: Validation Step ---\n",
        "        # Run a full evaluation on the validation set.\n",
        "        val_metrics = _run_validation_epoch(model, val_dataloader, loss_function, device)\n",
        "\n",
        "        # --- Step 3: Logging ---\n",
        "        # Get the current learning rate for logging.\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        # Format and print a comprehensive log message for the epoch.\n",
        "        log_message = (\n",
        "            f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.6f} | \"\n",
        "            f\"Val Loss: {val_metrics['val_loss']:.6f} | Val Corr: {val_metrics['val_corr']:.4f} | \"\n",
        "            f\"LR: {current_lr:.8f}\"\n",
        "        )\n",
        "        logging.info(log_message)\n",
        "\n",
        "        # Store all metrics for this epoch in the history log.\n",
        "        epoch_history = {'epoch': epoch + 1, 'train_loss': avg_train_loss, 'lr': current_lr, **val_metrics}\n",
        "        history.append(epoch_history)\n",
        "\n",
        "        # --- Step 2: Early Stopping Logic ---\n",
        "        # Check if the current validation loss is the best seen so far.\n",
        "        if val_metrics['val_loss'] < best_val_loss:\n",
        "            # If so, update the best loss, save the model state, and reset the patience counter.\n",
        "            logging.info(f\"Validation loss improved from {best_val_loss:.6f} to {val_metrics['val_loss']:.6f}. Saving best model...\")\n",
        "            best_val_loss = val_metrics['val_loss']\n",
        "            epochs_no_improve = 0\n",
        "            # Save a checkpoint containing the model's state dictionary and other useful info.\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': best_val_loss,\n",
        "            }, best_model_path)\n",
        "        else:\n",
        "            # If validation loss did not improve, increment the patience counter.\n",
        "            epochs_no_improve += 1\n",
        "            logging.info(f\"Validation loss did not improve. Patience: {epochs_no_improve}/{patience}.\")\n",
        "\n",
        "        # If the patience counter exceeds the configured limit, stop training.\n",
        "        if epochs_no_improve >= patience:\n",
        "            logging.info(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "    # --- Finalization ---\n",
        "    logging.info(\"Model training pipeline finished.\")\n",
        "\n",
        "    # Load the state dictionary from the best saved model to ensure the returned\n",
        "    # model object has the best performing weights.\n",
        "    logging.info(f\"Loading best model from epoch with validation loss: {best_val_loss:.6f}\")\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Convert the history list of dictionaries to a pandas DataFrame.\n",
        "    history_df = pd.DataFrame(history)\n",
        "    # Define the path for the history log file.\n",
        "    history_path = output_path / \"training_history.csv\"\n",
        "    # Save the history DataFrame to a CSV file for later analysis.\n",
        "    history_df.to_csv(history_path, index=False)\n",
        "    logging.info(f\"Full training history saved to '{history_path}'.\")\n",
        "\n",
        "    # Return the best model and the training history.\n",
        "    return model, history_df\n"
      ],
      "metadata": {
        "id": "RbnCB977XATg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Persist the trained model and architecture metadata\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Persist the trained model and architecture metadata\n",
        "# ==============================================================================\n",
        "\n",
        "def persist_model_and_metadata(\n",
        "    study_parameters: Dict[str, Any],\n",
        "    training_history: pd.DataFrame,\n",
        "    model_dir: Union[str, Path] = \"models\",\n",
        "    log_dir: Union[str, Path] = \"logs\"\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Orchestrates the persistence of the final model and all related metadata,\n",
        "    culminating in a single, reproducible bundle.\n",
        "\n",
        "    This function performs three steps:\n",
        "    1.  Validates the existence and integrity of the best model checkpoint saved\n",
        "        during training.\n",
        "    2.  Creates a comprehensive JSON metadata file detailing the model's\n",
        "        architecture, training hyperparameters, performance, and the exact\n",
        "        code version (Git hash) used.\n",
        "    3.  Bundles all critical artifacts (model weights, metadata, config, logs)\n",
        "        into a single compressed .tar.gz archive for portability and\n",
        "        reproducibility.\n",
        "\n",
        "    Args:\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        training_history (pd.DataFrame): The DataFrame logging the training and\n",
        "                                         validation metrics for each epoch.\n",
        "        model_dir (Union[str, Path]): The directory where the best model was saved.\n",
        "        log_dir (Union[str, Path]): The directory containing configuration snapshots\n",
        "                                    and where training logs are saved.\n",
        "\n",
        "    Returns:\n",
        "        Path: The file path to the final reproducibility bundle (.tar.gz).\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If a required artifact for bundling is missing.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating model persistence and metadata creation pipeline...\")\n",
        "\n",
        "    model_path = Path(model_dir)\n",
        "    log_path = Path(log_dir)\n",
        "\n",
        "    # --- Step 1: Validate the final trained model state ---\n",
        "    # The best model was already saved by the training function. Here, we verify it.\n",
        "    best_model_filepath = model_path / \"best_model.pth\"\n",
        "    if not best_model_filepath.exists():\n",
        "        raise FileNotFoundError(f\"Best model checkpoint not found at '{best_model_filepath}'. Training may have failed.\")\n",
        "\n",
        "    try:\n",
        "        # Load the checkpoint to verify its integrity.\n",
        "        checkpoint = torch.load(best_model_filepath)\n",
        "        # Check for essential keys.\n",
        "        assert 'epoch' in checkpoint and 'model_state_dict' in checkpoint\n",
        "        best_epoch = checkpoint['epoch']\n",
        "        logging.info(f\"Step 1/3: Verified best model checkpoint from epoch {best_epoch} at '{best_model_filepath}'.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load or validate the best model checkpoint: {e}\")\n",
        "        raise\n",
        "\n",
        "    # --- Step 2: Document the model architecture and hyperparameters ---\n",
        "    logging.info(\"Creating comprehensive model metadata file...\")\n",
        "\n",
        "    # Find the performance metrics from the best epoch.\n",
        "    best_epoch_metrics = training_history.loc[training_history['val_loss'].idxmin()].to_dict()\n",
        "\n",
        "    # Get the current Git commit hash for perfect code versioning.\n",
        "    try:\n",
        "        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip().decode('utf-8')\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "        git_hash = \"N/A (Not a git repository or git is not installed)\"\n",
        "        logging.warning(\"Could not retrieve Git commit hash.\")\n",
        "\n",
        "    # Construct the complete metadata dictionary.\n",
        "    model_metadata = {\n",
        "        \"model_architecture\": study_parameters['predictive_model']['architecture'],\n",
        "        \"training_parameters\": study_parameters['predictive_model']['training'],\n",
        "        \"optimizer_parameters\": study_parameters['predictive_model']['optimizer'],\n",
        "        \"training_completion_timestamp\": datetime.now().isoformat(),\n",
        "        \"code_version_git_hash\": git_hash,\n",
        "        \"best_epoch_performance\": {\n",
        "            \"best_epoch\": best_epoch,\n",
        "            \"metrics\": best_epoch_metrics\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save the metadata to a JSON file.\n",
        "    metadata_filepath = model_path / \"model_metadata.json\"\n",
        "    serializable_metadata = _make_json_serializable(model_metadata)\n",
        "    with metadata_filepath.open('w') as f:\n",
        "        json.dump(serializable_metadata, f, indent=4)\n",
        "    logging.info(f\"Step 2/3: Model metadata saved to '{metadata_filepath}'.\")\n",
        "\n",
        "    # --- Step 3: Create a reproducibility bundle ---\n",
        "    logging.info(\"Creating reproducibility bundle...\")\n",
        "\n",
        "    # Find the most recent configuration snapshot in the log directory.\n",
        "    config_snapshots = sorted(log_path.glob(\"config_snapshot_*.json\"), reverse=True)\n",
        "    if not config_snapshots:\n",
        "        raise FileNotFoundError(f\"No configuration snapshot found in '{log_path}'.\")\n",
        "    config_snapshot_path = config_snapshots[0]\n",
        "\n",
        "    # Define all files to be included in the bundle.\n",
        "    files_to_bundle = {\n",
        "        \"best_model.pth\": best_model_filepath,\n",
        "        \"model_metadata.json\": metadata_filepath,\n",
        "        \"config_snapshot.json\": config_snapshot_path,\n",
        "        \"training_history.csv\": log_path / \"training_history.csv\" # Assuming it's saved here\n",
        "    }\n",
        "\n",
        "    # Verify that all required files exist before creating the archive.\n",
        "    for name, path in files_to_bundle.items():\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Cannot create bundle: required artifact '{name}' not found at '{path}'.\")\n",
        "\n",
        "    # Create the final .tar.gz archive.\n",
        "    bundle_filename = f\"model_bundle_{datetime.now().strftime('%Y%m%d_%H%M%S')}.tar.gz\"\n",
        "    bundle_filepath = model_path / bundle_filename\n",
        "\n",
        "    with tarfile.open(bundle_filepath, \"w:gz\") as tar:\n",
        "        for name_in_archive, path_on_disk in files_to_bundle.items():\n",
        "            # Add each file to the archive with a clean name.\n",
        "            tar.add(path_on_disk, arcname=name_in_archive)\n",
        "\n",
        "    logging.info(f\"Step 3/3: Reproducibility bundle created at '{bundle_filepath}'.\")\n",
        "    logging.info(\"Model persistence and metadata pipeline finished successfully.\")\n",
        "\n",
        "    return bundle_filepath\n"
      ],
      "metadata": {
        "id": "SUZUlbFbgLlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28: Implement the inference pipeline for generating multi-horizon BubbleScore forecasts\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Implement the inference pipeline for generating multi-horizon\n",
        "#          BubbleScore forecasts\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Load the trained model for inference.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _load_model_for_inference(\n",
        "    model_path: Path,\n",
        "    config: Dict[str, Any],\n",
        "    num_stock_features: int,\n",
        "    num_market_features: int,\n",
        "    device: torch.device\n",
        ") -> DualStreamTransformer:\n",
        "    \"\"\"\n",
        "    Loads the best trained model from a checkpoint for inference.\n",
        "\n",
        "    This function first instantiates the model architecture using the exact\n",
        "    configuration from the study, then loads the saved state dictionary from\n",
        "    the best checkpoint, and finally sets the model to evaluation mode.\n",
        "\n",
        "    Args:\n",
        "        model_path (Path): Path to the '.pth' model checkpoint file.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        num_stock_features (int): The number of features in the stock stream.\n",
        "        num_market_features (int): The number of features in the market stream.\n",
        "        device (torch.device): The device to load the model onto.\n",
        "\n",
        "    Returns:\n",
        "        DualStreamTransformer: The trained model, ready for inference.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading model for inference from '{model_path}'...\")\n",
        "\n",
        "    # --- Instantiate Architecture ---\n",
        "    # The model must be created with the same architecture as during training.\n",
        "    model = DualStreamTransformer(config, num_stock_features, num_market_features)\n",
        "\n",
        "    # --- Load State Dictionary ---\n",
        "    # Load the checkpoint from the specified path.\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    # Load the learned weights into the model instance.\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # --- Configure for Inference ---\n",
        "    # Move the model to the specified device.\n",
        "    model.to(device)\n",
        "    # Set the model to evaluation mode. This is a critical step.\n",
        "    model.eval()\n",
        "\n",
        "    logging.info(f\"Model successfully loaded from epoch {checkpoint.get('epoch', 'N/A')} and set to evaluation mode.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Generate predictions for the test set.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_test_set_predictions(\n",
        "    model: DualStreamTransformer,\n",
        "    test_dataset: ModelDataset,\n",
        "    anchor_indices: pd.MultiIndex,\n",
        "    device: torch.device,\n",
        "    batch_size: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates multi-horizon predictions for the entire test set.\n",
        "\n",
        "    Args:\n",
        "        model (DualStreamTransformer): The trained model in evaluation mode.\n",
        "        test_dataset (ModelDataset): The test data split.\n",
        "        anchor_indices (pd.MultiIndex): The anchor indices for the test set.\n",
        "        device (torch.device): The device to run inference on.\n",
        "        batch_size (int): The batch size for inference.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of predictions, indexed by (Date, Ticker).\n",
        "    \"\"\"\n",
        "    logging.info(f\"Generating predictions for {len(test_dataset.targets):,} test samples...\")\n",
        "\n",
        "    # --- Create DataLoader ---\n",
        "    # Create a DataLoader for the test set. Shuffling must be False to maintain order.\n",
        "    dataset = TensorDataset(\n",
        "        torch.from_numpy(test_dataset.stock_sequences).float(),\n",
        "        torch.from_numpy(test_dataset.market_sequences).float()\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_predictions: List[torch.Tensor] = []\n",
        "\n",
        "    # --- Inference Loop ---\n",
        "    # Disable gradient computation for efficiency.\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating Test Predictions\"):\n",
        "            # Move input tensors to the device.\n",
        "            stock_seq, market_seq = [b.to(device) for b in batch]\n",
        "            # Get model predictions.\n",
        "            predictions = model(stock_seq, market_seq)\n",
        "            # Append predictions to the list, moving them to CPU.\n",
        "            all_predictions.append(predictions.cpu())\n",
        "\n",
        "    # Concatenate all batch predictions into a single tensor.\n",
        "    predictions_tensor = torch.cat(all_predictions)\n",
        "\n",
        "    # --- Structure the Output ---\n",
        "    # Create a DataFrame from the predictions tensor.\n",
        "    num_horizons = predictions_tensor.shape[1]\n",
        "    pred_cols = [f'Pred_H{h+1}' for h in range(num_horizons)]\n",
        "    predictions_df = pd.DataFrame(\n",
        "        predictions_tensor.numpy(),\n",
        "        index=anchor_indices,\n",
        "        columns=pred_cols\n",
        "    )\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Compute test metrics.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_test_metrics(\n",
        "    predictions: np.ndarray,\n",
        "    targets: np.ndarray\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes and reports performance metrics for the test set predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): 2D array of model predictions.\n",
        "        targets (np.ndarray): 2D array of ground-truth targets.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing performance metrics per horizon.\n",
        "    \"\"\"\n",
        "    metrics = []\n",
        "    num_horizons = predictions.shape[1]\n",
        "\n",
        "    # Calculate metrics for each forecast horizon independently.\n",
        "    for h in range(num_horizons):\n",
        "        preds_h = predictions[:, h]\n",
        "        targs_h = targets[:, h]\n",
        "\n",
        "        # Pearson Correlation\n",
        "        corr = np.corrcoef(preds_h, targs_h)[0, 1]\n",
        "        # Mean Squared Error (MSE)\n",
        "        mse = np.mean((preds_h - targs_h)**2)\n",
        "        # Root Mean Squared Error (RMSE)\n",
        "        rmse = np.sqrt(mse)\n",
        "        # Mean Absolute Error (MAE)\n",
        "        mae = np.mean(np.abs(preds_h - targs_h))\n",
        "\n",
        "        metrics.append({\n",
        "            'Horizon': h + 1,\n",
        "            'Correlation': corr,\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae\n",
        "        })\n",
        "\n",
        "    # Create a DataFrame from the list of metric dictionaries.\n",
        "    metrics_df = pd.DataFrame(metrics).set_index('Horizon')\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_inference_pipeline(\n",
        "    partitioned_datasets: Dict[str, ModelDataset],\n",
        "    study_parameters: Dict[str, Any],\n",
        "    final_anchor_indices: pd.MultiIndex,\n",
        "    validation_end_date: pd.Timestamp,\n",
        "    model_dir: Union[str, Path] = \"models\",\n",
        "    log_dir: Union[str, Path] = \"logs\"\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full inference and evaluation pipeline on the test set.\n",
        "\n",
        "    This function takes the trained model and the held-out test data to\n",
        "    generate out-of-sample predictions and compute final performance metrics.\n",
        "    It is the ultimate test of the model's generalization capability.\n",
        "\n",
        "    Args:\n",
        "        partitioned_datasets (Dict[str, ModelDataset]):\n",
        "            The dictionary of data splits ('train', 'validation', 'test').\n",
        "        study_parameters (Dict[str, Any]):\n",
        "            The main configuration dictionary for the study.\n",
        "        final_anchor_indices (pd.MultiIndex):\n",
        "            The complete MultiIndex of all valid samples before splitting.\n",
        "        validation_end_date (pd.Timestamp):\n",
        "            The precise timestamp marking the end of the validation period. This\n",
        "            is used to define the start of the test set, ensuring a clean\n",
        "            chronological split and making the function's dependencies explicit.\n",
        "        model_dir (Union[str, Path]):\n",
        "            The directory where the best trained model checkpoint is saved.\n",
        "        log_dir (Union[str, Path]):\n",
        "            The directory where the final metrics report will be saved.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "            - A DataFrame of predictions for the test set, indexed by (Date, Ticker).\n",
        "            - A DataFrame summarizing the performance metrics per forecast horizon.\n",
        "    \"\"\"\n",
        "    # Announce the start of the inference process.\n",
        "    logging.info(\"Initiating inference and evaluation pipeline...\")\n",
        "\n",
        "    # --- Setup ---\n",
        "    # Determine the device for running inference (prefer GPU if available).\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Define the path to the best model checkpoint.\n",
        "    model_path = Path(model_dir) / \"best_model.pth\"\n",
        "    # Extract the test dataset from the partitioned datasets.\n",
        "    test_dataset = partitioned_datasets['test']\n",
        "\n",
        "    # Determine the number of input features from the shape of the test data arrays.\n",
        "    num_stock_features = test_dataset.stock_sequences.shape[2]\n",
        "    num_market_features = test_dataset.market_sequences.shape[2]\n",
        "\n",
        "    # --- Use the passed `validation_end_date` to create the test mask ---\n",
        "    # This removes the hardcoded dependency on the study_parameters dictionary.\n",
        "    # Create a boolean mask to select only the anchor indices that fall within the test period.\n",
        "    test_mask = final_anchor_indices.get_level_values('Date') > validation_end_date\n",
        "    # Apply the mask to get the precise anchor indices for the test set.\n",
        "    test_anchor_indices = final_anchor_indices[test_mask]\n",
        "\n",
        "    # --- Step 1: Load the trained model for inference. ---\n",
        "    # Instantiate the model architecture and load the best weights from the checkpoint.\n",
        "    model = _load_model_for_inference(\n",
        "        model_path, study_parameters, num_stock_features, num_market_features, device\n",
        "    )\n",
        "    logging.info(\"Step 1/3: Trained model loaded successfully.\")\n",
        "\n",
        "    # --- Step 2: Generate predictions for the test set. ---\n",
        "    # Retrieve the batch size from the configuration.\n",
        "    batch_size = study_parameters['predictive_model']['training']['batch_size']\n",
        "    # Run the model on the test data to get out-of-sample predictions.\n",
        "    predictions_df = _generate_test_set_predictions(\n",
        "        model, test_dataset, test_anchor_indices, device, batch_size\n",
        "    )\n",
        "    logging.info(\"Step 2/3: Test set predictions generated.\")\n",
        "\n",
        "    # --- Step 3: Align predictions and compute test metrics. ---\n",
        "    # Compare the generated predictions against the ground-truth targets.\n",
        "    metrics_df = _compute_test_metrics(\n",
        "        predictions_df.to_numpy(),\n",
        "        test_dataset.targets\n",
        "    )\n",
        "    logging.info(\"Step 3/3: Performance metrics computed on the test set.\")\n",
        "\n",
        "    # --- Reporting ---\n",
        "    # Log the detailed per-horizon performance summary to the console.\n",
        "    logging.info(\"Test Set Performance Summary:\\n\" + metrics_df.to_string())\n",
        "\n",
        "    # Log the average performance across all horizons, as reported in the paper.\n",
        "    avg_metrics = metrics_df.mean()\n",
        "    logging.info(\"Average Performance Across Horizons:\\n\" + avg_metrics.to_string())\n",
        "\n",
        "    # Define the path for the metrics report.\n",
        "    metrics_path = Path(log_dir) / \"test_metrics.csv\"\n",
        "    # Ensure the output directory exists.\n",
        "    metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the metrics DataFrame to a CSV file for a persistent record.\n",
        "    metrics_df.to_csv(metrics_path)\n",
        "    logging.info(f\"Test metrics report saved to '{metrics_path}'.\")\n",
        "\n",
        "    logging.info(\"Inference and evaluation pipeline finished successfully.\")\n",
        "\n",
        "    # Return the predictions and the performance metrics.\n",
        "    return predictions_df, metrics_df\n"
      ],
      "metadata": {
        "id": "60W2UL0kmhsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29: Convert multi-horizon BubbleScore forecasts into trading signals\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Convert multi-horizon BubbleScore forecasts into trading signals\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 29, Step 1: Extract trading thresholds from configuration.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_trading_thresholds(config: Dict[str, Any]) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Retrieves and validates the trading signal thresholds from the configuration.\n",
        "\n",
        "    This function extracts the entry threshold (theta_1) and exit threshold\n",
        "    (theta_2) from the study parameters. It also performs a validation check\n",
        "    to ensure the thresholds are logically consistent (0 < exit < entry < 1).\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "            A tuple containing (theta_1_entry, theta_2_exit).\n",
        "\n",
        "    Raises:\n",
        "        KeyError:\n",
        "            If the required threshold keys are missing from the configuration.\n",
        "        ValueError:\n",
        "            If the thresholds are not valid or logically inconsistent.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the nested dictionary for backtesting strategy rules.\n",
        "        rules_config = config['backtesting']['strategy_rules']\n",
        "\n",
        "        # Extract the entry threshold (theta_1), the level at which a position is initiated.\n",
        "        theta_1 = rules_config['entry_threshold_theta_1']\n",
        "        # Extract the exit threshold (theta_2), the level at which a position is closed.\n",
        "        theta_2 = rules_config['exit_threshold_theta_2']\n",
        "\n",
        "        # --- Validation ---\n",
        "        # Re-assert the logical consistency of the thresholds. This was checked in\n",
        "        # Task 1 but is a critical safety check at the point of use.\n",
        "        if not (0 < theta_2 < theta_1 < 1):\n",
        "            raise ValueError(f\"Trading thresholds are invalid. Must satisfy 0 < theta_2 < theta_1 < 1, but got theta_1={theta_1}, theta_2={theta_2}.\")\n",
        "\n",
        "        # Log the extracted parameters for auditability.\n",
        "        logging.info(f\"Trading thresholds extracted: Entry (theta_1) = {theta_1}, Exit (theta_2) = {theta_2}.\")\n",
        "\n",
        "        # Return the validated thresholds.\n",
        "        return theta_1, theta_2\n",
        "\n",
        "    except KeyError as e:\n",
        "        # If a key is missing, raise an error with a specific message.\n",
        "        logging.error(f\"Missing a required trading threshold in the configuration: {e}\")\n",
        "        raise\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 29, Step 2: Generate entry and exit signals for each horizon.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_signals_for_group(\n",
        "    group: pd.DataFrame,\n",
        "    theta_1: float,\n",
        "    theta_2: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates trading signals for a single time series (one ticker/horizon pair).\n",
        "\n",
        "    This function implements a state machine that tracks the current position\n",
        "    (flat, long, or short) and iterates through the time series of predictions.\n",
        "    It emits entry or exit signals based on the crossing of the `theta_1` and\n",
        "    `theta_2` thresholds.\n",
        "\n",
        "    Args:\n",
        "        group (pd.DataFrame):\n",
        "            A DataFrame for a single ticker and forecast horizon, sorted by date,\n",
        "            with a 'Prediction' column.\n",
        "        theta_1 (float):\n",
        "            The entry threshold. A position is considered if |Prediction| >= theta_1.\n",
        "        theta_2 (float):\n",
        "            The exit threshold. A position is closed if |Prediction| <= theta_2.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame containing the generated signals ('Date', 'Signal') for this group.\n",
        "    \"\"\"\n",
        "    # Initialize the position state: 0 for flat, 1 for long, -1 for short.\n",
        "    position = 0\n",
        "    # Initialize a list to store the generated signal events.\n",
        "    signals = []\n",
        "\n",
        "    # Iterate through each timestamp and corresponding row in the group's data.\n",
        "    for date, row in group.iterrows():\n",
        "        # Get the model's prediction for the current day.\n",
        "        prediction = row['Prediction']\n",
        "\n",
        "        # --- Exit Logic ---\n",
        "        # Exit logic is checked before entry logic to allow for a position to be\n",
        "        # closed and a new one opened on the same day if signals permit.\n",
        "\n",
        "        # If currently in a long position, check for a long exit signal.\n",
        "        if position == 1 and prediction >= -theta_2:\n",
        "            signals.append({'Date': date, 'Signal': 'LONG_EXIT'})\n",
        "            position = 0 # Reset position to flat.\n",
        "        # If currently in a short position, check for a short exit signal.\n",
        "        elif position == -1 and prediction <= theta_2:\n",
        "            signals.append({'Date': date, 'Signal': 'SHORT_EXIT'})\n",
        "            position = 0 # Reset position to flat.\n",
        "\n",
        "        # --- Entry Logic ---\n",
        "        # Entry signals are only considered if the current position is flat.\n",
        "        if position == 0:\n",
        "            # Check for a long entry signal (prediction is very negative).\n",
        "            if prediction <= -theta_1:\n",
        "                signals.append({'Date': date, 'Signal': 'LONG_ENTRY'})\n",
        "                position = 1 # Set position to long.\n",
        "            # Check for a short entry signal (prediction is very positive).\n",
        "            elif prediction >= theta_1:\n",
        "                signals.append({'Date': date, 'Signal': 'SHORT_ENTRY'})\n",
        "                position = -1 # Set position to short.\n",
        "\n",
        "    # Convert the list of signal dictionaries into a DataFrame.\n",
        "    return pd.DataFrame(signals)\n",
        "\n",
        "\n",
        "def _generate_threshold_signals(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    theta_1: float,\n",
        "    theta_2: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates all threshold-based entry and exit signals for all tickers and horizons.\n",
        "\n",
        "    This function first transforms the wide-format prediction DataFrame into a\n",
        "    long format, then applies the stateful signal generation logic to each\n",
        "    ticker-horizon group.\n",
        "\n",
        "    Args:\n",
        "        predictions_df (pd.DataFrame):\n",
        "            The wide-format DataFrame of predictions from the model.\n",
        "        theta_1 (float): The entry threshold.\n",
        "        theta_2 (float): The exit threshold.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A long-form DataFrame containing all generated threshold-based signals.\n",
        "    \"\"\"\n",
        "    # --- Step A: Melt DataFrame ---\n",
        "    # Convert the wide prediction DataFrame (columns Pred_H1, Pred_H2, ...)\n",
        "    # to a long format with columns ['Date', 'TICKER', 'Horizon', 'Prediction'].\n",
        "    # This simplifies group-wise operations.\n",
        "    long_predictions = predictions_df.reset_index().melt(\n",
        "        id_vars=['Date', 'TICKER'],\n",
        "        var_name='Horizon_Str',\n",
        "        value_name='Prediction'\n",
        "    )\n",
        "    # Convert the 'Horizon_Str' (e.g., 'Pred_H1') to a simple integer.\n",
        "    long_predictions['Horizon'] = long_predictions['Horizon_Str'].str.replace('Pred_H', '').astype(int)\n",
        "    long_predictions.drop(columns=['Horizon_Str'], inplace=True)\n",
        "\n",
        "    # --- Step B & C: Apply state machine logic to each group ---\n",
        "    logging.info(\"Generating threshold-based entry/exit signals for all ticker-horizon pairs...\")\n",
        "    # Group by ticker and horizon, then apply the state machine function to each group.\n",
        "    # This is a powerful pattern for applying a stateful function to many time series.\n",
        "    signals = long_predictions.groupby(['TICKER', 'Horizon'], group_keys=False).apply(\n",
        "        _generate_signals_for_group, theta_1=theta_1, theta_2=theta_2\n",
        "    ).reset_index()\n",
        "\n",
        "    # The .apply() can add an unwanted 'level_2' index; this removes it.\n",
        "    if 'level_2' in signals.columns:\n",
        "        signals.drop(columns=['level_2'], inplace=True)\n",
        "\n",
        "    return signals\n",
        "\n",
        "\n",
        "def _generate_reversal_signals(predictions_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates exit signals based on the prediction reversal rule.\n",
        "\n",
        "    This rule provides an additional layer of risk management by forcing an exit\n",
        "    if the model's forecast flips sign between consecutive horizons for the same\n",
        "    anchor date, indicating high uncertainty.\n",
        "\n",
        "    Args:\n",
        "        predictions_df (pd.DataFrame):\n",
        "            The wide-format DataFrame of model predictions.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame containing all generated reversal exit signals.\n",
        "    \"\"\"\n",
        "    logging.info(\"Generating prediction reversal exit signals...\")\n",
        "    # Initialize a list to store reversal signal events.\n",
        "    reversal_signals = []\n",
        "\n",
        "    # Define the pairs of consecutive horizons to check (e.g., (1, 2), (2, 3), ...).\n",
        "    num_horizons = len(predictions_df.columns)\n",
        "    horizon_pairs = [(h, h + 1) for h in range(1, num_horizons)]\n",
        "\n",
        "    # Iterate through each pair of consecutive horizons.\n",
        "    for h1, h2 in horizon_pairs:\n",
        "        # Select the prediction columns for the two horizons.\n",
        "        pred_h1 = predictions_df[f'Pred_H{h1}']\n",
        "        pred_h2 = predictions_df[f'Pred_H{h2}']\n",
        "\n",
        "        # --- Step D: Reversal Protection Logic ---\n",
        "        # A reversal occurs if the product of the two predictions is negative.\n",
        "        # Equation: B_t+h * B_t+h+1 < 0\n",
        "        reversal_mask = (pred_h1 * pred_h2) < 0\n",
        "\n",
        "        # If any reversals are detected for this horizon pair...\n",
        "        if reversal_mask.any():\n",
        "            # Get the (Date, Ticker) MultiIndex for the rows where reversals occurred.\n",
        "            reversal_indices = predictions_df.index[reversal_mask]\n",
        "            # For each reversal event, create a signal record.\n",
        "            # The signal is for horizon `h1`, as it's the earlier of the pair.\n",
        "            for date, ticker in reversal_indices:\n",
        "                reversal_signals.append({\n",
        "                    'Date': date,\n",
        "                    'TICKER': ticker,\n",
        "                    'Horizon': h1,\n",
        "                    'Signal': 'REVERSAL_EXIT'\n",
        "                })\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame.\n",
        "    return pd.DataFrame(reversal_signals)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 29, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_trading_signals(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_path: Union[str, Path] = \"data_final/trading_signals.csv\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the conversion of model predictions into discrete trading signals.\n",
        "\n",
        "    This function implements the full signal generation logic from the paper by\n",
        "    first generating primary entry/exit signals based on threshold crossings,\n",
        "    then generating additional risk-management signals based on the prediction\n",
        "    reversal rule. All signals are combined, sorted, and persisted to a file.\n",
        "\n",
        "    Args:\n",
        "        predictions_df (pd.DataFrame):\n",
        "            The DataFrame of out-of-sample predictions from the inference pipeline.\n",
        "        study_parameters (Dict[str, Any]):\n",
        "            The main configuration dictionary.\n",
        "        output_path (Union[str, Path]):\n",
        "            The file path to save the final signals CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A long-form DataFrame containing all generated trading signals,\n",
        "            ready for use in an event-driven backtester.\n",
        "    \"\"\"\n",
        "    # Announce the start of the signal generation process.\n",
        "    logging.info(\"Initiating trading signal generation pipeline...\")\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # --- Step 1: Extract and validate trading thresholds. ---\n",
        "    theta_1, theta_2 = _get_trading_thresholds(study_parameters)\n",
        "    logging.info(\"Step 1/3: Trading thresholds extracted.\")\n",
        "\n",
        "    # --- Step 2: Generate signals from both rules. ---\n",
        "    # Generate the primary entry/exit signals based on the stateful threshold logic.\n",
        "    threshold_signals = _generate_threshold_signals(predictions_df, theta_1, theta_2)\n",
        "\n",
        "    # Generate the secondary, risk-management exit signals from the reversal rule.\n",
        "    reversal_signals = _generate_reversal_signals(predictions_df)\n",
        "    logging.info(\"Step 2/3: Threshold and reversal signals generated.\")\n",
        "\n",
        "    # --- Step 3: Combine, persist, and log statistics. ---\n",
        "    # Concatenate the signals from both sources into a single DataFrame.\n",
        "    all_signals = pd.concat([threshold_signals, reversal_signals], ignore_index=True)\n",
        "\n",
        "    # Sort the signals chronologically for each ticker and horizon. This is\n",
        "    # essential for correct processing in a sequential backtester.\n",
        "    all_signals.sort_values(by=['TICKER', 'Horizon', 'Date'], inplace=True)\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the final, sorted signals DataFrame to a CSV file.\n",
        "    all_signals.to_csv(output_path, index=False)\n",
        "\n",
        "    # Log summary statistics of the generated signals for a high-level overview.\n",
        "    logging.info(f\"Step 3/3: Generated a total of {len(all_signals):,} signals. Signals saved to '{output_path}'.\")\n",
        "    if not all_signals.empty:\n",
        "        logging.info(\"Signal distribution:\\n\" + all_signals['Signal'].value_counts().to_string())\n",
        "\n",
        "    logging.info(\"Trading signal generation pipeline finished successfully.\")\n",
        "\n",
        "    # Return the final DataFrame of signals.\n",
        "    return all_signals\n"
      ],
      "metadata": {
        "id": "EMqC3v7Monj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30: Simulate position management, risk controls, and compute PnL\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 30: Simulate position management, risk controls, and compute PnL\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 30, Steps 1 & 2: Initialize and simulate a single strategy.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_single_backtest(\n",
        "    strategy_signals: pd.DataFrame,\n",
        "    price_series: pd.Series,\n",
        "    risk_params: Dict[str, float]\n",
        ") -> Tuple[pd.Series, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Runs a rigorous, event-driven backtest for a single strategy, featuring\n",
        "    corrected and robust state management.\n",
        "\n",
        "    This function simulates trading on a day-by-day basis. It processes signals\n",
        "    chronologically, manages the trading position state (flat, long, short),\n",
        "    applies transaction costs, and enforces a daily stop-loss rule. This\n",
        "    re-implementation corrects a latent bug by ensuring all state variables,\n",
        "    including the entry date, are atomically updated and reset.\n",
        "\n",
        "    Args:\n",
        "        strategy_signals (pd.DataFrame):\n",
        "            A DataFrame of trading signals for a single ticker-horizon pair,\n",
        "            indexed by date.\n",
        "        price_series (pd.Series):\n",
        "            A Series of adjusted closing prices for the corresponding ticker,\n",
        "            indexed by date.\n",
        "        risk_params (Dict[str, float]):\n",
        "            A dictionary containing risk parameters: 'stop_loss' (e.g., 0.15)\n",
        "            and 'txn_cost' (e.g., 0.001).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, List[Dict[str, Any]]]: A tuple containing:\n",
        "            - The daily equity curve as a pandas Series, indexed by date.\n",
        "            - A log of all executed trades as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Augment State Initialization ---\n",
        "    # Initialize all state variables for the simulation to a clean \"flat\" state.\n",
        "    position: int = 0  # 0: flat, 1: long, -1: short\n",
        "    entry_price: float = 0.0\n",
        "    entry_price_date: Optional[pd.Timestamp] = None # Correctly initialized to None\n",
        "    capital: float = 1.0\n",
        "\n",
        "    # Prepare data structures for storing the simulation results.\n",
        "    equity_curve = pd.Series(index=price_series.index, dtype=float)\n",
        "    trade_log: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Create a dictionary of signals for efficient O(1) lookup by date.\n",
        "    signals_dict = strategy_signals.set_index('Date')['Signal'].to_dict()\n",
        "\n",
        "    # --- Simulation Loop ---\n",
        "    # Iterate through each trading day in the chronological price series.\n",
        "    for date, price in price_series.items():\n",
        "        # The capital at the start of the day is the capital from the end of the previous day.\n",
        "        equity_curve[date] = capital\n",
        "\n",
        "        # Get the signal for the current day, if one exists.\n",
        "        signal = signals_dict.get(date)\n",
        "\n",
        "        # --- 1. Daily Risk Management (Stop-Loss Check) ---\n",
        "        # This check is performed every day there is an open position.\n",
        "        if position != 0:\n",
        "            # Calculate the current unrealized return on the open position.\n",
        "            unrealized_return = position * (price - entry_price) / entry_price\n",
        "            # Check if the stop-loss threshold has been breached.\n",
        "            if unrealized_return < -risk_params['stop_loss']:\n",
        "                # If breached, an exit is forced.\n",
        "                net_return = unrealized_return - risk_params['txn_cost']\n",
        "                # Update the capital based on the losing trade.\n",
        "                capital *= (1 + net_return)\n",
        "                # Log the details of the trade that was stopped out.\n",
        "                trade_log.append({\n",
        "                    'entry_date': entry_price_date, 'exit_date': date,\n",
        "                    'position': 'long' if position == 1 else 'short',\n",
        "                    'return': net_return, 'exit_reason': 'STOP_LOSS'\n",
        "                })\n",
        "                # --- Augment Exit Logic ---\n",
        "                # Atomically reset all state variables to 'flat'.\n",
        "                position = 0\n",
        "                entry_price = 0.0\n",
        "                entry_price_date = None\n",
        "                # Update the equity curve for the current day to reflect the capital change.\n",
        "                equity_curve[date] = capital\n",
        "\n",
        "        # --- 2. Process Trading Signals ---\n",
        "        # Only process signals if a position is still open or flat (i.e., not stopped out today).\n",
        "        if signal:\n",
        "            # Define exit conditions based on the signal and current position.\n",
        "            is_long_exit = (signal in ['LONG_EXIT', 'REVERSAL_EXIT']) and position == 1\n",
        "            is_short_exit = (signal in ['SHORT_EXIT', 'REVERSAL_EXIT']) and position == -1\n",
        "\n",
        "            if is_long_exit or is_short_exit:\n",
        "                # Calculate the return on the closed trade based on the current price.\n",
        "                trade_return = position * (price - entry_price) / entry_price\n",
        "                # Subtract transaction costs to get the net return.\n",
        "                net_return = trade_return - risk_params['txn_cost']\n",
        "                # Update capital with the result of the trade.\n",
        "                capital *= (1 + net_return)\n",
        "                # Log the trade details.\n",
        "                trade_log.append({\n",
        "                    'entry_date': entry_price_date, 'exit_date': date,\n",
        "                    'position': 'long' if position == 1 else 'short',\n",
        "                    'return': net_return, 'exit_reason': signal\n",
        "                })\n",
        "                # --- Augment Exit Logic ---\n",
        "                # Atomically reset all state variables to 'flat'.\n",
        "                position = 0\n",
        "                entry_price = 0.0\n",
        "                entry_price_date = None\n",
        "                # Update the equity curve for the current day.\n",
        "                equity_curve[date] = capital\n",
        "\n",
        "            # Entry signals are only processed if the position is currently flat.\n",
        "            # This allows an exit and a new entry on the same day.\n",
        "            if position == 0:\n",
        "                if signal == 'LONG_ENTRY':\n",
        "                    # --- Augment Entry Logic ---\n",
        "                    # Atomically set all state variables for a new long position.\n",
        "                    position = 1\n",
        "                    entry_price = price\n",
        "                    entry_price_date = date\n",
        "                elif signal == 'SHORT_ENTRY':\n",
        "                    # --- Augment Entry Logic ---\n",
        "                    # Atomically set all state variables for a new short position.\n",
        "                    position = -1\n",
        "                    entry_price = price\n",
        "                    entry_price_date = date\n",
        "\n",
        "    # Return the completed equity curve and the detailed log of all trades.\n",
        "    return equity_curve, trade_log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 30, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def simulate_all_strategies(\n",
        "    signals_df: pd.DataFrame,\n",
        "    prices_df: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_dir: Union[str, Path] = \"data_final\"\n",
        ") -> Tuple[Dict[Tuple[str, int], pd.Series], Dict[Tuple[str, int], List[Dict[str, Any]]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the backtesting simulation for all ticker-horizon strategies.\n",
        "\n",
        "    This function iterates through each unique strategy defined in the signals\n",
        "    DataFrame, runs an event-driven backtest for each, and persists the\n",
        "    resulting equity curves and trade logs.\n",
        "\n",
        "    Args:\n",
        "        signals_df (pd.DataFrame): The long-form DataFrame of all trading signals.\n",
        "        prices_df (pd.DataFrame): DataFrame containing 'Close_Price_Adj' for all tickers.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_dir (Union[str, Path]): The root directory to save results.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: A tuple containing:\n",
        "            - A dictionary mapping (Ticker, Horizon) to its equity curve Series.\n",
        "            - A dictionary mapping (Ticker, Horizon) to its trade log list.\n",
        "    \"\"\"\n",
        "    logging.info(\"Initiating backtesting simulation for all strategies...\")\n",
        "\n",
        "    # --- Extract Risk Parameters ---\n",
        "    risk_params = {\n",
        "        'stop_loss': study_parameters['backtesting']['risk_management']['stop_loss_percentage'],\n",
        "        'txn_cost': study_parameters['backtesting']['market_assumptions']['transaction_cost_per_trade']\n",
        "    }\n",
        "\n",
        "    # --- Prepare Output Directories (Step 3) ---\n",
        "    equity_curve_dir = Path(output_dir) / \"equity_curves\"\n",
        "    trade_log_dir = Path(output_dir) / \"trade_logs\"\n",
        "    equity_curve_dir.mkdir(parents=True, exist_ok=True)\n",
        "    trade_log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- Group Signals by Strategy ---\n",
        "    # Each (Ticker, Horizon) pair is a unique strategy to be backtested.\n",
        "    strategy_groups = signals_df.groupby(['TICKER', 'Horizon'])\n",
        "\n",
        "    # Dictionaries to hold the in-memory results.\n",
        "    all_equity_curves: Dict[Tuple[str, int], pd.Series] = {}\n",
        "    all_trade_logs: Dict[Tuple[str, int], List[Dict[str, Any]]] = {}\n",
        "\n",
        "    # --- Main Simulation Loop ---\n",
        "    # Iterate through each strategy group.\n",
        "    for (ticker, horizon), group_signals in tqdm(strategy_groups, desc=\"Backtesting Strategies\"):\n",
        "        # Get the price series for the current ticker.\n",
        "        if ticker not in prices_df.index.get_level_values('TICKER'):\n",
        "            logging.warning(f\"No price data found for ticker {ticker}. Skipping backtest.\")\n",
        "            continue\n",
        "\n",
        "        # Select the price series for the specific ticker.\n",
        "        price_series = prices_df.loc[(slice(None), ticker), 'Close_Price_Adj'].droplevel('TICKER')\n",
        "\n",
        "        # Run the backtest simulation for this single strategy.\n",
        "        equity_curve, trade_log = _run_single_backtest(group_signals, price_series, risk_params)\n",
        "\n",
        "        # Store the results in the dictionaries.\n",
        "        strategy_key = (ticker, horizon)\n",
        "        all_equity_curves[strategy_key] = equity_curve\n",
        "        all_trade_logs[strategy_key] = trade_log\n",
        "\n",
        "        # --- Persistence (Step 3) ---\n",
        "        # Save the equity curve to a CSV file.\n",
        "        equity_curve.to_csv(equity_curve_dir / f\"{ticker}_H{horizon}.csv\", header=['Equity'])\n",
        "        # Save the trade log to a CSV file.\n",
        "        if trade_log:\n",
        "            pd.DataFrame(trade_log).to_csv(trade_log_dir / f\"{ticker}_H{horizon}.csv\", index=False)\n",
        "\n",
        "    logging.info(f\"Backtesting simulation complete. Results for {len(all_equity_curves)} strategies saved to '{output_dir}'.\")\n",
        "\n",
        "    return all_equity_curves, all_trade_logs\n"
      ],
      "metadata": {
        "id": "JZMxmxvQqmtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31: Compute and report performance metrics for each strategy\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 31: Compute and report performance metrics for each strategy\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Steps 1 & 2: Compute all performance metrics for a single strategy.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_strategy_metrics(\n",
        "    equity_curve: pd.Series,\n",
        "    trade_log: List[Dict[str, Any]],\n",
        "    risk_free_rate: float,\n",
        "    trading_days_per_year: int = 252\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculates a comprehensive set of performance metrics for a single strategy.\n",
        "\n",
        "    Args:\n",
        "        equity_curve (pd.Series): The daily equity curve of the strategy.\n",
        "        trade_log (List[Dict[str, Any]]): The log of executed trades.\n",
        "        risk_free_rate (float): The annualized risk-free rate.\n",
        "        trading_days_per_year (int): The number of trading days in a year.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all calculated performance metrics.\n",
        "    \"\"\"\n",
        "    # --- Annualized Return ---\n",
        "    # Equation: AR = (E_T / E_0)^(252 / N) - 1\n",
        "    num_days = len(equity_curve)\n",
        "    if num_days < 2: return {} # Not enough data to calculate metrics\n",
        "    total_return = equity_curve.iloc[-1] / equity_curve.iloc[0] - 1\n",
        "    annualized_return = (1 + total_return) ** (trading_days_per_year / num_days) - 1\n",
        "\n",
        "    # --- Sharpe Ratio ---\n",
        "    # Equation: Sharpe = mean(r_excess) / std(r_excess) * sqrt(252)\n",
        "    daily_returns = equity_curve.pct_change().dropna()\n",
        "    if len(daily_returns) > 1 and daily_returns.std() > 1e-8:\n",
        "        daily_risk_free = (1 + risk_free_rate)**(1/trading_days_per_year) - 1\n",
        "        excess_returns = daily_returns - daily_risk_free\n",
        "        sharpe_ratio = (excess_returns.mean() / excess_returns.std()) * np.sqrt(trading_days_per_year)\n",
        "    else:\n",
        "        sharpe_ratio = 0.0 # If no volatility, Sharpe is zero.\n",
        "\n",
        "    # --- Maximum Drawdown (MDD) ---\n",
        "    # Equation: MDD = max(1 - E_u / M_u) where M_u is running max equity.\n",
        "    running_max = equity_curve.expanding().max()\n",
        "    drawdowns = 1 - equity_curve / running_max\n",
        "    max_drawdown = drawdowns.max()\n",
        "\n",
        "    # --- Win Rate and Trade Count ---\n",
        "    num_trades = len(trade_log)\n",
        "    if num_trades > 0:\n",
        "        winning_trades = sum(1 for trade in trade_log if trade['return'] > 0)\n",
        "        win_rate = winning_trades / num_trades\n",
        "    else:\n",
        "        win_rate = np.nan # Undefined if no trades were made.\n",
        "\n",
        "    return {\n",
        "        'Annualized_Return': annualized_return,\n",
        "        'Sharpe_Ratio': sharpe_ratio,\n",
        "        'Max_Drawdown': max_drawdown,\n",
        "        'Win_Rate': win_rate,\n",
        "        'Num_Trades': num_trades\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 31, Step 3: Aggregate and report performance.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def report_backtest_performance(\n",
        "    all_equity_curves: Dict[Tuple[str, int], pd.Series],\n",
        "    all_trade_logs: Dict[Tuple[str, int], List[Dict[str, Any]]],\n",
        "    study_parameters: Dict[str, Any],\n",
        "    output_path: Union[str, Path] = \"reports/performance_summary.csv\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates performance metrics for all strategies and generates a summary report.\n",
        "\n",
        "    This function iterates through the results of all backtest simulations,\n",
        "    calculates performance metrics for each, and compiles them into a single\n",
        "    DataFrame. It also computes and logs high-level summary statistics as\n",
        "    described in the paper.\n",
        "\n",
        "    Args:\n",
        "        all_equity_curves (Dict): Dictionary mapping (Ticker, Horizon) to equity curves.\n",
        "        all_trade_logs (Dict): Dictionary mapping (Ticker, Horizon) to trade logs.\n",
        "        study_parameters (Dict[str, Any]): The main configuration dictionary.\n",
        "        output_path (Union[str, Path]): The path to save the summary CSV report.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the performance of all strategies.\n",
        "    \"\"\"\n",
        "    logging.info(\"Aggregating and reporting backtest performance metrics...\")\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # --- Parameter Extraction ---\n",
        "    risk_free_rate = study_parameters['backtesting']['market_assumptions']['risk_free_rate_annual']\n",
        "\n",
        "    # --- Metric Calculation Loop ---\n",
        "    # This list will store the dictionary of metrics for each strategy.\n",
        "    all_metrics = []\n",
        "    # Iterate through all simulated strategies.\n",
        "    for strategy_key, equity_curve in all_equity_curves.items():\n",
        "        # Retrieve the corresponding trade log.\n",
        "        trade_log = all_trade_logs.get(strategy_key, [])\n",
        "        # Calculate all performance metrics for this strategy.\n",
        "        metrics = _calculate_strategy_metrics(equity_curve, trade_log, risk_free_rate)\n",
        "\n",
        "        if metrics:\n",
        "            # Add the strategy identifiers (Ticker, Horizon) to the metrics dict.\n",
        "            metrics['Ticker'] = strategy_key[0]\n",
        "            metrics['Horizon'] = strategy_key[1]\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "    if not all_metrics:\n",
        "        logging.warning(\"No performance metrics could be calculated. No valid strategies found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Create and Persist Summary DataFrame ---\n",
        "    # Convert the list of dictionaries into a DataFrame.\n",
        "    summary_df = pd.DataFrame(all_metrics)\n",
        "    # Set a MultiIndex for easy lookup and analysis.\n",
        "    summary_df.set_index(['Ticker', 'Horizon'], inplace=True)\n",
        "\n",
        "    # Save the detailed summary report to a CSV file.\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    summary_df.to_csv(output_path)\n",
        "    logging.info(f\"Full performance summary saved to '{output_path}'.\")\n",
        "\n",
        "    # --- High-Level Analysis and Reporting ---\n",
        "    # Log the overall summary statistics across all strategies.\n",
        "    logging.info(\"--- Overall Strategy Performance Summary ---\")\n",
        "    logging.info(summary_df.describe().to_string())\n",
        "\n",
        "    # Identify and log the top 5 performing strategies by annualized return.\n",
        "    top_5 = summary_df.sort_values(by='Annualized_Return', ascending=False).head(5)\n",
        "    logging.info(\"\\n--- Top 5 Performing Strategies (by Annualized Return) ---\")\n",
        "    logging.info(top_5.to_string())\n",
        "\n",
        "    # Calculate and log the overall success rate (percentage of strategies with positive return).\n",
        "    success_rate = (summary_df['Annualized_Return'] > 0).mean()\n",
        "    logging.info(f\"\\nOverall Success Rate (Positive Ann. Return): {success_rate:.2%}\")\n",
        "\n",
        "    # Calculate and log the distribution of optimal horizons.\n",
        "    # For each ticker, find the horizon that yielded the highest annualized return.\n",
        "    optimal_horizons = summary_df.loc[summary_df.groupby('Ticker')['Annualized_Return'].idxmax()]\n",
        "    horizon_distribution = optimal_horizons.index.get_level_values('Horizon').value_counts(normalize=True).sort_index()\n",
        "    logging.info(\"\\n--- Distribution of Optimal Prediction Horizons ---\")\n",
        "    logging.info(horizon_distribution.to_string())\n",
        "\n",
        "    logging.info(\"Performance reporting pipeline finished successfully.\")\n",
        "\n",
        "    return summary_df\n"
      ],
      "metadata": {
        "id": "WC4RdwIftw1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32: Create an orchestrator function for the end-to-end research pipeline\n",
        "\n",
        "def run_hlppl_pipeline(\n",
        "    df_raw: pd.DataFrame,\n",
        "    study_parameters: Dict[str, Any],\n",
        "    intermediate_dir: Path,\n",
        "    model_dir: Path,\n",
        "    log_dir: Path,\n",
        "    report_dir: Path\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the HLPPL model.\n",
        "\n",
        "    This master orchestrator function serves as the engine for a single,\n",
        "    isolated experimental run. It manages the entire workflow, from raw data\n",
        "    validation to final performance reporting, by calling a sequence of modular,\n",
        "    task-specific functions. All outputs (intermediate data, models, logs,\n",
        "    and reports) are directed to the specified directories, ensuring that runs\n",
        "    from different experiments do not interfere with each other.\n",
        "\n",
        "    The pipeline is structured as follows:\n",
        "    -   Phase I: Setup and Validation (Tasks 1-2)\n",
        "    -   Phase II: Data Cleansing and Feature Engineering (Tasks 3-6)\n",
        "    -   Phase III: NLP Signal Generation (Tasks 7-12)\n",
        "    -   Phase IV: LPPL Signal Generation (Tasks 13-18)\n",
        "    -   Phase V: Deep Learning Data Preparation (Tasks 19-22)\n",
        "    -   Phase VI: Model Training and Evaluation (Tasks 23-26)\n",
        "    -   Phase VII: Persistence, Inference, and Backtesting (Tasks 27-31)\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame):\n",
        "            The raw input DataFrame containing market, fundamental, and news data.\n",
        "        study_parameters (Dict[str, Any]):\n",
        "            The configuration dictionary for this specific experimental run.\n",
        "        intermediate_dir (Path):\n",
        "            The directory for saving/loading intermediate data artifacts (e.g.,\n",
        "            embeddings, window definitions, signals).\n",
        "        model_dir (Path):\n",
        "            The directory for saving/loading model-related artifacts (e.g.,\n",
        "            checkpoints, metadata).\n",
        "        log_dir (Path):\n",
        "            The directory for saving logs and configuration snapshots.\n",
        "        report_dir (Path):\n",
        "            The directory for saving final output reports (e.g., performance summary).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            The final performance summary DataFrame, detailing the results of\n",
        "            all backtested strategies for this experimental run.\n",
        "\n",
        "    Raises:\n",
        "        Exception:\n",
        "            Propagates any exception from a failed pipeline step after logging\n",
        "            the error, causing the run for this experiment to halt.\n",
        "    \"\"\"\n",
        "    # Create a logger specific to this pipeline run, named after the experiment.\n",
        "    logger = logging.getLogger(f\"HLPPL_Pipeline_{model_dir.parent.name}\")\n",
        "    logger.info(f\"--- STARTING PIPELINE RUN. OUTPUTS DIRECTED TO: {model_dir.parent} ---\")\n",
        "\n",
        "    try:\n",
        "        # --- PHASE I: SETUP AND VALIDATION ---\n",
        "        logger.info(\"\\n--- Phase I: Setup and Validation ---\")\n",
        "\n",
        "        # Task 1: Validate the configuration and create a snapshot in the experiment's log directory.\n",
        "        config = validate_and_parse_config(study_parameters, log_dir=log_dir)\n",
        "\n",
        "        # Task 2: Validate the raw DataFrame's schema and structure.\n",
        "        df_validated = validate_input_dataframe(df_raw)\n",
        "\n",
        "        # --- PHASE II: DATA CLEANSING AND FEATURE ENGINEERING ---\n",
        "        logger.info(\"\\n--- Phase II: Data Cleansing and Feature Engineering ---\")\n",
        "\n",
        "        # Tasks 3-6: Sequentially clean, adjust, and engineer the base features.\n",
        "        df_cleansed = cleanse_raw_data(df_validated)\n",
        "        df_adjusted = adjust_for_corporate_actions(df_cleansed)\n",
        "        df_features = derive_engineered_features(df_adjusted)\n",
        "        df_aligned = align_and_validate_calendar(df_features)\n",
        "        master_calendar = df_aligned.index.get_level_values('Date').unique().sort_values()\n",
        "\n",
        "        # --- PHASE III: NLP SIGNAL GENERATION ---\n",
        "        logger.info(\"\\n--- Phase III: NLP Signal Generation ---\")\n",
        "\n",
        "        # Tasks 7-12: Process all text data to generate sentiment and hype signals.\n",
        "        corpus, _, topic_model = setup_topic_model(df_aligned, config, intermediate_data_dir=intermediate_dir, model_dir=model_dir)\n",
        "        df_filtered, corpus_retained = apply_topic_filter(df_aligned, corpus, topic_model, log_dir=log_dir)\n",
        "        sentiment_results = classify_article_sentiment(corpus_retained, config, output_path=intermediate_dir / \"finbert_sentiment_results.pkl\")\n",
        "        df_stock_sentiment = aggregate_stock_day_sentiment(df_filtered, sentiment_results)\n",
        "        df_market_sentiment = aggregate_market_level_sentiment(df_stock_sentiment, sentiment_results, master_calendar)\n",
        "        df_hype = construct_hype_index(df_market_sentiment)\n",
        "\n",
        "        # --- PHASE IV: LPPL SIGNAL GENERATION ---\n",
        "        logger.info(\"\\n--- Phase IV: LPPL Signal Generation ---\")\n",
        "\n",
        "        # Tasks 13-18: Perform the LPPL analysis to generate the final BubbleScore.\n",
        "        windows = define_lppl_calibration_windows(df_hype, config, output_path=intermediate_dir / \"lppl_windows.pkl\")\n",
        "        bounds, _ = initialize_lppl_fitter(config, log_dir=log_dir)\n",
        "        lppl_fits = fit_lppl_model_to_windows(windows, bounds, config, output_path=intermediate_dir / \"lppl_fit_parameters.csv\")\n",
        "        df_residuals = compute_and_merge_lppl_residuals(df_hype, lppl_fits, config)\n",
        "        df_bubblescore = construct_bubblescore(df_residuals, config, output_dir=intermediate_dir)\n",
        "        df_labeled = label_bubble_episodes(df_bubblescore, config, output_dir=intermediate_dir)\n",
        "\n",
        "        # --- PHASE V: DEEP LEARNING DATA PREPARATION ---\n",
        "        logger.info(\"\\n--- Phase V: Deep Learning Data Preparation ---\")\n",
        "\n",
        "        # Tasks 19-22: Engineer and align all features into sequences and split the data.\n",
        "        stock_sequences, anchor_indices, _ = engineer_stock_level_sequences(df_labeled, config)\n",
        "        market_sequence_map, _ = engineer_market_level_sequences(df_labeled, config)\n",
        "        final_anchor_indices, target_matrix = construct_and_align_targets(df_labeled, anchor_indices, config)\n",
        "\n",
        "        # Align the stock sequences with the final valid anchor indices.\n",
        "        valid_indices_set = set(final_anchor_indices)\n",
        "        stock_sequences = [seq for idx, seq in zip(anchor_indices, stock_sequences) if idx in valid_indices_set]\n",
        "\n",
        "        partitioned_datasets = split_dataset_chronologically(stock_sequences, market_sequence_map, target_matrix, final_anchor_indices, config)\n",
        "\n",
        "        # --- PHASE VI: MODEL TRAINING AND EVALUATION ---\n",
        "        logger.info(\"\\n--- Phase VI: Model Training and Evaluation ---\")\n",
        "\n",
        "        # Determine the device for training.\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Tasks 23-26: Define, train, and validate the Transformer model.\n",
        "        num_stock_features = partitioned_datasets['train'].stock_sequences.shape[2]\n",
        "        num_market_features = partitioned_datasets['train'].market_sequences.shape[2]\n",
        "        model = DualStreamTransformer(config, num_stock_features, num_market_features)\n",
        "        loss_function = CompositeLoss(config)\n",
        "        model, training_history = train_and_validate_model(model, partitioned_datasets, loss_function, config, device, output_dir=model_dir)\n",
        "\n",
        "        # Task 27: Persist the final trained model and all associated metadata.\n",
        "        persist_model_and_metadata(config, training_history, model_dir=model_dir, log_dir=log_dir)\n",
        "\n",
        "        # --- PHASE VII: INFERENCE AND BACKTESTING ---\n",
        "        logger.info(\"\\n--- Phase VII: Inference and Backtesting ---\")\n",
        "\n",
        "        # Task 28: Run inference on the held-out test set.\n",
        "        _, val_end_date = _determine_chronological_split_dates(final_anchor_indices, config['predictive_model']['data_preparation']['dataset_split_ratio'])\n",
        "        predictions_df, _ = run_inference_pipeline(partitioned_datasets, config, final_anchor_indices, val_end_date, model_dir=model_dir, log_dir=log_dir)\n",
        "\n",
        "        # Task 29: Convert model predictions into discrete trading signals.\n",
        "        signals_df = generate_trading_signals(predictions_df, config, output_path=intermediate_dir / \"trading_signals.csv\")\n",
        "\n",
        "        # Task 30: Simulate the trading strategy based on the signals.\n",
        "        test_prices = df_aligned.loc[df_aligned.index.get_level_values('Date') > val_end_date]\n",
        "        all_equity_curves, all_trade_logs = simulate_all_strategies(signals_df, test_prices, config, output_dir=intermediate_dir)\n",
        "\n",
        "        # Task 31: Compute and report the final performance metrics.\n",
        "        performance_summary = report_backtest_performance(all_equity_curves, all_trade_logs, config, output_path=report_dir / \"performance_summary.csv\")\n",
        "\n",
        "        # Log the successful completion of this experimental run.\n",
        "        logger.info(f\"--- PIPELINE RUN SUCCEEDED FOR: {model_dir.parent.name} ---\")\n",
        "\n",
        "        # Return the final performance summary.\n",
        "        return performance_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception from any step in the pipeline.\n",
        "        logger.critical(f\"!!! PIPELINE FAILED FOR: {model_dir.parent.name} !!!\", exc_info=True)\n",
        "        # Re-raise the exception to halt the execution of the ablation study for this run.\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "ZwS3d3-A61cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 33: Conduct robustness and ablation studies\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 33: Conduct robustness and ablation studies\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 33, Step 1: Define ablation test configurations.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _define_ablation_configurations(\n",
        "    base_config: Dict[str, Any]\n",
        ") -> List[Tuple[str, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Generates a list of modified configuration dictionaries for ablation studies.\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]): The original, validated study configuration.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, Dict[str, Any]]]: A list of tuples, where each tuple is\n",
        "            an (experiment_name, modified_config_dict).\n",
        "    \"\"\"\n",
        "    experiments = []\n",
        "\n",
        "    # --- Baseline Experiment ---\n",
        "    # The first experiment is the full model with the base configuration.\n",
        "    experiments.append((\"baseline_full_model\", copy.deepcopy(base_config)))\n",
        "\n",
        "    # --- Ablation: Residual-only (No Behavioral Features) ---\n",
        "    config_res_only = copy.deepcopy(base_config)\n",
        "    config_res_only['descriptive_model']['bubble_score_synthesis']['alpha_1_hype_weight'] = 0.0\n",
        "    config_res_only['descriptive_model']['bubble_score_synthesis']['alpha_2_sentiment_weight'] = 0.0\n",
        "    experiments.append((\"ablation_residual_only\", config_res_only))\n",
        "\n",
        "    # --- Ablation: No Hype ---\n",
        "    config_no_hype = copy.deepcopy(base_config)\n",
        "    config_no_hype['descriptive_model']['bubble_score_synthesis']['alpha_1_hype_weight'] = 0.0\n",
        "    experiments.append((\"ablation_no_hype\", config_no_hype))\n",
        "\n",
        "    # --- Ablation: No Sentiment ---\n",
        "    config_no_sentiment = copy.deepcopy(base_config)\n",
        "    config_no_sentiment['descriptive_model']['bubble_score_synthesis']['alpha_2_sentiment_weight'] = 0.0\n",
        "    experiments.append((\"ablation_no_sentiment\", config_no_sentiment))\n",
        "\n",
        "    # --- Sensitivity: Vary LPPL Window Size ---\n",
        "    for window_size in [150, 200, 250, 300]:\n",
        "        config_ws = copy.deepcopy(base_config)\n",
        "        config_ws['descriptive_model']['lppl_fitting']['rolling_window_size'] = window_size\n",
        "        experiments.append((f\"sensitivity_lppl_window_{window_size}\", config_ws))\n",
        "\n",
        "    logging.info(f\"Defined {len(experiments)} experiments for ablation and sensitivity analysis.\")\n",
        "    return experiments\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 33, Step 3: Compare ablation results and report sensitivity.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _analyze_and_report_ablation_results(\n",
        "    all_results: Dict[str, pd.DataFrame],\n",
        "    output_dir: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Analyzes results from all experiments and generates a summary report and plots.\n",
        "\n",
        "    Args:\n",
        "        all_results (Dict[str, pd.DataFrame]): A dictionary mapping experiment\n",
        "            names to their performance summary DataFrames.\n",
        "        output_dir (Path): The directory to save the final report and plots.\n",
        "    \"\"\"\n",
        "    summary_metrics = []\n",
        "    # Calculate aggregate metrics for each experiment's result.\n",
        "    for name, result_df in all_results.items():\n",
        "        if not result_df.empty:\n",
        "            summary_metrics.append({\n",
        "                'Experiment': name,\n",
        "                'Mean_Annualized_Return': result_df['Annualized_Return'].mean(),\n",
        "                'Mean_Sharpe_Ratio': result_df['Sharpe_Ratio'].mean(),\n",
        "                'Mean_Max_Drawdown': result_df['Max_Drawdown'].mean(),\n",
        "                'Mean_Win_Rate': result_df['Win_Rate'].mean()\n",
        "            })\n",
        "\n",
        "    if not summary_metrics:\n",
        "        logging.warning(\"No results to analyze for ablation study.\")\n",
        "        return\n",
        "\n",
        "    # Create a comparison DataFrame.\n",
        "    comparison_df = pd.DataFrame(summary_metrics).set_index('Experiment')\n",
        "\n",
        "    # Save the final comparison table.\n",
        "    report_path = output_dir / \"ablation_comparison_summary.csv\"\n",
        "    comparison_df.to_csv(report_path)\n",
        "    logging.info(f\"Ablation comparison summary saved to '{report_path}'.\")\n",
        "    logging.info(\"\\n--- Ablation Study Summary ---\")\n",
        "    logging.info(comparison_df.to_string())\n",
        "\n",
        "    # --- Generate Plots ---\n",
        "    # Set plot style.\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # Plot 1: Bar chart for core ablation study.\n",
        "    ablation_names = ['baseline_full_model', 'ablation_residual_only', 'ablation_no_hype', 'ablation_no_sentiment']\n",
        "    plot_data = comparison_df.loc[ablation_names]\n",
        "\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
        "    sns.barplot(x=plot_data.index, y='Mean_Annualized_Return', data=plot_data, ax=ax[0])\n",
        "    ax[0].set_title('Ablation Study: Mean Annualized Return')\n",
        "    ax[0].set_ylabel('Mean Annualized Return')\n",
        "    ax[0].tick_params(axis='x', rotation=15)\n",
        "\n",
        "    sns.barplot(x=plot_data.index, y='Mean_Sharpe_Ratio', data=plot_data, ax=ax[1])\n",
        "    ax[1].set_title('Ablation Study: Mean Sharpe Ratio')\n",
        "    ax[1].set_ylabel('Mean Sharpe Ratio')\n",
        "    ax[1].tick_params(axis='x', rotation=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = output_dir / \"ablation_core_performance.png\"\n",
        "    fig.savefig(plot_path)\n",
        "    logging.info(f\"Core ablation performance plot saved to '{plot_path}'.\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Plot 2: Line plot for LPPL window size sensitivity.\n",
        "    sensitivity_data = comparison_df[comparison_df.index.str.startswith('sensitivity_lppl_window_')]\n",
        "    sensitivity_data['Window_Size'] = sensitivity_data.index.str.split('_').str[-1].astype(int)\n",
        "    sensitivity_data.sort_values('Window_Size', inplace=True)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    sns.lineplot(x='Window_Size', y='Mean_Sharpe_Ratio', data=sensitivity_data, marker='o', ax=ax)\n",
        "    ax.set_title('Sensitivity Analysis: Mean Sharpe Ratio vs. LPPL Window Size')\n",
        "    ax.set_xlabel('LPPL Rolling Window Size (days)')\n",
        "    ax.set_ylabel('Mean Sharpe Ratio')\n",
        "\n",
        "    plot_path = output_dir / \"sensitivity_lppl_window.png\"\n",
        "    fig.savefig(plot_path)\n",
        "    logging.info(f\"LPPL window sensitivity plot saved to '{plot_path}'.\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 33, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_ablation_studies(\n",
        "    df_raw: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    root_output_dir: Union[str, Path] = \"results\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all ablation and sensitivity analysis experiments.\n",
        "\n",
        "    This function serves as the master controller for the entire study. It\n",
        "    systematically modifies the base configuration to create different\n",
        "    experimental setups (e.g., removing behavioral features, varying\n",
        "    hyperparameters). For each setup, it runs the entire end-to-end pipeline\n",
        "    in an isolated directory. Finally, it aggregates and analyzes the results\n",
        "    from all experiments to produce a final comparison report and visualizations,\n",
        "    quantifying the contribution of each model component.\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame):\n",
        "            The raw input DataFrame, which will be used for all experimental runs.\n",
        "        base_config (Dict[str, Any]):\n",
        "            The baseline configuration for the study, which will be modified for\n",
        "            each experiment.\n",
        "        root_output_dir (Union[str, Path]):\n",
        "            The root directory where results for all experiments will be saved\n",
        "            in separate, named subdirectories.\n",
        "    \"\"\"\n",
        "    # Get a logger specific to this master orchestrator.\n",
        "    logger = logging.getLogger(\"AblationOrchestrator\")\n",
        "    logger.info(\"==========================================================\")\n",
        "    logger.info(\"=== STARTING ABLATION AND SENSITIVITY ANALYSIS ===\")\n",
        "    logger.info(\"==========================================================\")\n",
        "\n",
        "    # Define the root path for all experimental outputs.\n",
        "    root_path = Path(root_output_dir)\n",
        "\n",
        "    # --- Step 1: Define all experimental configurations. ---\n",
        "    # This helper function generates a list of (name, config_dict) tuples.\n",
        "    experiments = _define_ablation_configurations(base_config)\n",
        "\n",
        "    # This dictionary will store the final performance summary DataFrame from each successful run.\n",
        "    all_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # --- Step 2: Run the full pipeline for each configuration. ---\n",
        "    # This loop executes the entire, computationally intensive research pipeline for each defined experiment.\n",
        "    for name, config in experiments:\n",
        "        # Log the start of a new experimental run.\n",
        "        logger.info(f\"\\n{'='*80}\\n--- Running Experiment: {name} ---\\n{'='*80}\")\n",
        "\n",
        "        # Define a unique, isolated output directory for this specific experiment\n",
        "        # to prevent interference between runs.\n",
        "        experiment_root_dir = root_path / name\n",
        "        # Define subdirectories for organized output within the experiment's folder.\n",
        "        intermediate_dir = experiment_root_dir / \"data_intermediate\"\n",
        "        model_dir = experiment_root_dir / \"models\"\n",
        "        log_dir = experiment_root_dir / \"logs\"\n",
        "        report_dir = experiment_root_dir / \"reports\"\n",
        "\n",
        "        # Create all necessary directories for the current experiment.\n",
        "        for d in [intermediate_dir, model_dir, log_dir, report_dir]:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # --- Execute the full end-to-end pipeline ---\n",
        "            # Call the master orchestrator, passing the specific config\n",
        "            # and isolated directories for this experimental run.\n",
        "            performance_summary = run_hlppl_pipeline(\n",
        "                df_raw=df_raw,\n",
        "                study_parameters=config,\n",
        "                intermediate_dir=intermediate_dir,\n",
        "                model_dir=model_dir,\n",
        "                log_dir=log_dir,\n",
        "                report_dir=report_dir\n",
        "            )\n",
        "            # If the pipeline succeeds, store the final performance summary for later analysis.\n",
        "            all_results[name] = performance_summary\n",
        "\n",
        "        except Exception:\n",
        "            # If any part of the pipeline fails for one experiment, log the error\n",
        "            # and continue to the next one. This makes the overall study robust.\n",
        "            logger.error(f\"!!! Experiment '{name}' FAILED and will be excluded from the final analysis. !!!\", exc_info=False)\n",
        "            # Store an empty DataFrame to signify a failed run.\n",
        "            all_results[name] = pd.DataFrame()\n",
        "\n",
        "    # --- Step 3: Compare results and generate final report. ---\n",
        "    logger.info(f\"\\n{'='*80}\\n--- All experiments complete. Analyzing and reporting results. ---\\n{'='*80}\")\n",
        "    # This function takes the dictionary of all results and produces the\n",
        "    # final comparison tables and plots.\n",
        "    _analyze_and_report_ablation_results(all_results, root_path)\n",
        "\n",
        "    logger.info(\"==========================================================\")\n",
        "    logger.info(\"=== ABLATION AND SENSITIVITY ANALYSIS COMPLETE ===\")\n",
        "    logger.info(\"==========================================================\")\n"
      ],
      "metadata": {
        "id": "YyYM4LpAA_bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# Ensure the logger is configured for the entire run.\n",
        "# This top-level configuration will be inherited by all modules.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        # Log to a file for a persistent record of the entire study.\n",
        "        logging.FileHandler(\"main_orchestrator.log\", mode='w'),\n",
        "        # Also log to the console for real-time feedback.\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "def execute_full_study(\n",
        "    df_raw: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    run_ablation: bool = True,\n",
        "    root_output_dir: Union[str, Path] = \"study_results\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Serves as the master orchestrator for the entire HLPPL research study.\n",
        "\n",
        "    This top-level function provides a single entry point to execute the complete\n",
        "    research pipeline. It performs two major operations:\n",
        "    1.  Executes a full end-to-end run of the baseline HLPPL model using the\n",
        "        provided base configuration. The results of this run are stored in a\n",
        "        dedicated 'baseline' subdirectory.\n",
        "    2.  If requested, it then proceeds to run the comprehensive ablation and\n",
        "        sensitivity analysis, which involves re-running the entire pipeline for\n",
        "        several variations of the configuration.\n",
        "\n",
        "    This function is designed for reproducibility and manages all I/O to ensure\n",
        "    that results from different experimental runs are kept isolated and organized.\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame):\n",
        "            The raw input DataFrame containing all necessary market, fundamental,\n",
        "            and news data for the study.\n",
        "        base_config (Dict[str, Any]):\n",
        "            The baseline configuration dictionary for the study. This will be\n",
        "            used for the main run and as a template for the ablation studies.\n",
        "        run_ablation (bool):\n",
        "            A flag to control whether the computationally expensive ablation\n",
        "            studies should be executed after the baseline run. Defaults to True.\n",
        "        root_output_dir (Union[str, Path]):\n",
        "            The root directory where all outputs from the entire study (baseline\n",
        "            and ablation runs) will be saved.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the key results of the study, including the\n",
        "            performance summary of the baseline model and paths to the detailed\n",
        "            output directories.\n",
        "    \"\"\"\n",
        "    # Get a logger for this top-level orchestrator.\n",
        "    logger = logging.getLogger(\"MainOrchestrator\")\n",
        "    logger.info(\"==========================================================\")\n",
        "    logger.info(\"====== EXECUTING FULL HLPPL REPLICATION STUDY ======\")\n",
        "    logger.info(\"==========================================================\")\n",
        "\n",
        "    # Define the root path for all outputs of this study.\n",
        "    root_path = Path(root_output_dir)\n",
        "    root_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # This dictionary will store the final results to be returned.\n",
        "    results_summary: Dict[str, Any] = {}\n",
        "\n",
        "    # --- 1. Execute the Baseline Pipeline Run ---\n",
        "    # Define the isolated output directories for the baseline experiment.\n",
        "    baseline_dir = root_path / \"baseline\"\n",
        "    logger.info(f\"\\n--- Starting Baseline Pipeline Run (Outputs in: {baseline_dir}) ---\")\n",
        "\n",
        "    try:\n",
        "        # Call the main pipeline orchestrator for the baseline configuration.\n",
        "        baseline_performance = run_hlppl_pipeline(\n",
        "            df_raw=df_raw,\n",
        "            study_parameters=base_config,\n",
        "            intermediate_dir=baseline_dir / \"data_intermediate\",\n",
        "            model_dir=baseline_dir / \"models\",\n",
        "            log_dir=baseline_dir / \"logs\",\n",
        "            report_dir=baseline_dir / \"reports\"\n",
        "        )\n",
        "        # Store the key result from the baseline run.\n",
        "        results_summary['baseline_performance'] = baseline_performance\n",
        "        results_summary['baseline_output_directory'] = str(baseline_dir.resolve())\n",
        "        logger.info(\"--- Baseline Pipeline Run Completed Successfully ---\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If the baseline run fails, it's a critical error. Log and terminate.\n",
        "        logger.critical(\"!!! BASELINE PIPELINE FAILED. ABORTING STUDY. !!!\", exc_info=True)\n",
        "        results_summary['status'] = 'FAILED'\n",
        "        results_summary['error'] = str(e)\n",
        "        return results_summary\n",
        "\n",
        "    # --- 2. Conditionally Execute Ablation Studies ---\n",
        "    # Check the flag to see if the user requested the ablation studies.\n",
        "    if run_ablation:\n",
        "        # The ablation orchestrator will manage its own subdirectories within the root path.\n",
        "        ablation_dir = root_path / \"ablation_studies\"\n",
        "        logger.info(f\"\\n--- Starting Ablation Studies (Outputs in: {ablation_dir}) ---\")\n",
        "\n",
        "        # Call the ablation study orchestrator. It handles all sub-runs internally.\n",
        "        # Note: This function has its own internal error handling for individual\n",
        "        # experiment failures and does not need to be wrapped in a try/except here.\n",
        "        run_ablation_studies(\n",
        "            df_raw=df_raw,\n",
        "            base_config=base_config,\n",
        "            root_output_dir=ablation_dir\n",
        "        )\n",
        "        # Store the path to the ablation results for easy access.\n",
        "        results_summary['ablation_output_directory'] = str(ablation_dir.resolve())\n",
        "        logger.info(\"--- Ablation Studies Completed ---\")\n",
        "    else:\n",
        "        # If ablation studies are skipped, log this information.\n",
        "        logger.info(\"\\n--- Skipping Ablation Studies as per configuration (run_ablation=False) ---\")\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Log the successful completion of the entire study.\n",
        "    logger.info(\"==========================================================\")\n",
        "    logger.info(\"========= FULL HLPPL REPLICATION STUDY COMPLETE =========\")\n",
        "    logger.info(\"==========================================================\")\n",
        "\n",
        "    # Add a final success status to the results summary.\n",
        "    results_summary['status'] = 'SUCCESS'\n",
        "\n",
        "    # Return the dictionary containing the key results and output paths.\n",
        "    return results_summary\n"
      ],
      "metadata": {
        "id": "U5gK8dE-KtrH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}